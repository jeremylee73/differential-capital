{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anjalichauhan/opt/anaconda3/lib/python3.8/site-packages/IPython/core/magics/pylab.py:159: UserWarning: pylab import has clobbered these variables: ['test']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  warn(\"pylab import has clobbered these variables: %s\"  % clobbered +\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Conv1D, MaxPooling1D, UpSampling1D, BatchNormalization, LSTM, RepeatVector\n",
    "from keras.models import Model\n",
    "from keras.models import model_from_json\n",
    "from keras import regularizers\n",
    "import datetime\n",
    "import time\n",
    "import requests as req\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## USE: full log(returns)/returns dataframe\n",
    "## Risk Adjusted Returns\n",
    "\n",
    "df = pd.read_pickle(\"../Data/risk_adj_returns.pkl\").iloc[1:]\n",
    "\n",
    "drop_columns = []\n",
    "for col in df.columns:\n",
    "    if df[col].isnull().all() == True:\n",
    "        drop_columns.append(col)\n",
    "        \n",
    "df.drop(columns=drop_columns, inplace=True)\n",
    "\n",
    "# df['pct_change'] = df.close.pct_change()\n",
    "# df['log_ret'] = np.log(df.close) - np.log(df.close.shift(1))\n",
    "# df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.dropna(how='any',axis=0) #All rows have NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>905270</th>\n",
       "      <th>921795</th>\n",
       "      <th>904261</th>\n",
       "      <th>905261</th>\n",
       "      <th>916328</th>\n",
       "      <th>923024</th>\n",
       "      <th>936365</th>\n",
       "      <th>902355</th>\n",
       "      <th>912215</th>\n",
       "      <th>929813</th>\n",
       "      <th>...</th>\n",
       "      <th>9660J1</th>\n",
       "      <th>69568X</th>\n",
       "      <th>543755</th>\n",
       "      <th>77463M</th>\n",
       "      <th>29235J</th>\n",
       "      <th>131745</th>\n",
       "      <th>69487D</th>\n",
       "      <th>68157P</th>\n",
       "      <th>9110RA</th>\n",
       "      <th>292703</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-05-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.073424</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.026594</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042002</td>\n",
       "      <td>0.042352</td>\n",
       "      <td>-0.007893</td>\n",
       "      <td>-0.034946</td>\n",
       "      <td>-0.006981</td>\n",
       "      <td>0.025437</td>\n",
       "      <td>0.006211</td>\n",
       "      <td>-0.034570</td>\n",
       "      <td>0.045914</td>\n",
       "      <td>-0.023507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.041516</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.023492</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045774</td>\n",
       "      <td>-0.040381</td>\n",
       "      <td>-0.029872</td>\n",
       "      <td>-0.042776</td>\n",
       "      <td>-0.034353</td>\n",
       "      <td>-0.047507</td>\n",
       "      <td>-0.041449</td>\n",
       "      <td>-0.032225</td>\n",
       "      <td>-0.001858</td>\n",
       "      <td>-0.073776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.133718</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.022297</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018214</td>\n",
       "      <td>0.006846</td>\n",
       "      <td>-0.041720</td>\n",
       "      <td>0.014670</td>\n",
       "      <td>0.006996</td>\n",
       "      <td>0.050378</td>\n",
       "      <td>-0.030298</td>\n",
       "      <td>0.008515</td>\n",
       "      <td>-0.039008</td>\n",
       "      <td>-0.103963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.114582</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.030586</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050278</td>\n",
       "      <td>-0.001888</td>\n",
       "      <td>-0.039754</td>\n",
       "      <td>-0.029330</td>\n",
       "      <td>-0.021043</td>\n",
       "      <td>-0.042413</td>\n",
       "      <td>-0.000474</td>\n",
       "      <td>-0.003118</td>\n",
       "      <td>-0.012079</td>\n",
       "      <td>0.010919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.046110</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001882</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059837</td>\n",
       "      <td>-0.051037</td>\n",
       "      <td>-0.046412</td>\n",
       "      <td>-0.017292</td>\n",
       "      <td>-0.040496</td>\n",
       "      <td>-0.066158</td>\n",
       "      <td>-0.032654</td>\n",
       "      <td>-0.031242</td>\n",
       "      <td>-0.039250</td>\n",
       "      <td>-0.007592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1237 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            905270  921795  904261  905261    916328  923024    936365  \\\n",
       "date                                                                     \n",
       "2021-05-24     NaN     NaN     NaN     NaN -0.073424     NaN -0.026594   \n",
       "2021-05-25     NaN     NaN     NaN     NaN -0.041516     NaN -0.023492   \n",
       "2021-05-26     NaN     NaN     NaN     NaN -0.133718     NaN -0.022297   \n",
       "2021-05-27     NaN     NaN     NaN     NaN -0.114582     NaN -0.030586   \n",
       "2021-05-28     NaN     NaN     NaN     NaN -0.046110     NaN  0.001882   \n",
       "\n",
       "            902355  912215  929813  ...    9660J1    69568X    543755  \\\n",
       "date                                ...                                 \n",
       "2021-05-24     NaN     NaN     NaN  ... -0.042002  0.042352 -0.007893   \n",
       "2021-05-25     NaN     NaN     NaN  ... -0.045774 -0.040381 -0.029872   \n",
       "2021-05-26     NaN     NaN     NaN  ... -0.018214  0.006846 -0.041720   \n",
       "2021-05-27     NaN     NaN     NaN  ... -0.050278 -0.001888 -0.039754   \n",
       "2021-05-28     NaN     NaN     NaN  ... -0.059837 -0.051037 -0.046412   \n",
       "\n",
       "              77463M    29235J    131745    69487D    68157P    9110RA  \\\n",
       "date                                                                     \n",
       "2021-05-24 -0.034946 -0.006981  0.025437  0.006211 -0.034570  0.045914   \n",
       "2021-05-25 -0.042776 -0.034353 -0.047507 -0.041449 -0.032225 -0.001858   \n",
       "2021-05-26  0.014670  0.006996  0.050378 -0.030298  0.008515 -0.039008   \n",
       "2021-05-27 -0.029330 -0.021043 -0.042413 -0.000474 -0.003118 -0.012079   \n",
       "2021-05-28 -0.017292 -0.040496 -0.066158 -0.032654 -0.031242 -0.039250   \n",
       "\n",
       "              292703  \n",
       "date                  \n",
       "2021-05-24 -0.023507  \n",
       "2021-05-25 -0.073776  \n",
       "2021-05-26 -0.103963  \n",
       "2021-05-27  0.010919  \n",
       "2021-05-28 -0.007592  \n",
       "\n",
       "[5 rows x 1237 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_investable(t, n_rows):\n",
    "    \"Find stocks in investable universe at time t\\\n",
    "    (stocks in the S&P500 that have prices recorded for the last n_rows days)\"\n",
    "    \n",
    "    df_investable = df.copy(deep = True).sort_index(ascending = False)\n",
    "    \n",
    "    #add 1 date to get the test features in investable\n",
    "    t = t + pd.DateOffset(1)\n",
    "    \n",
    "    #if t is now a non-trading day, advance until we reach a valid trading day\n",
    "    while t not in df_investable.index:\n",
    "        t = t + pd.DateOffset(1)\n",
    "    \n",
    "    t_index = df_investable.index.get_loc(t)\n",
    "    \n",
    "    #take n_rows worth of data upto time specified\n",
    "    df_investable = df_investable.iloc[t_index + 1:t_index + n_rows + 1]\n",
    "    \n",
    "    #find all stocks that exist in the S&P at this time period\n",
    "    investable_universe = []\n",
    "    for col in df_investable.columns:\n",
    "        if ~df_investable[col].iloc[:n_rows].isna().any():\n",
    "            investable_universe.append(col)\n",
    "        \n",
    "    df_investable = df_investable[investable_universe]\n",
    "    \n",
    "    return df_investable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>916328</th>\n",
       "      <th>936365</th>\n",
       "      <th>905271</th>\n",
       "      <th>905113</th>\n",
       "      <th>905802</th>\n",
       "      <th>905425</th>\n",
       "      <th>906156</th>\n",
       "      <th>916305</th>\n",
       "      <th>992816</th>\n",
       "      <th>921093</th>\n",
       "      <th>...</th>\n",
       "      <th>311917</th>\n",
       "      <th>69568X</th>\n",
       "      <th>543755</th>\n",
       "      <th>77463M</th>\n",
       "      <th>29235J</th>\n",
       "      <th>131745</th>\n",
       "      <th>69487D</th>\n",
       "      <th>68157P</th>\n",
       "      <th>9110RA</th>\n",
       "      <th>292703</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-05-11</th>\n",
       "      <td>-0.033261</td>\n",
       "      <td>-0.056147</td>\n",
       "      <td>-0.056673</td>\n",
       "      <td>-0.039225</td>\n",
       "      <td>-0.078155</td>\n",
       "      <td>-0.079176</td>\n",
       "      <td>-0.055281</td>\n",
       "      <td>-0.036198</td>\n",
       "      <td>-0.062879</td>\n",
       "      <td>-0.060282</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030098</td>\n",
       "      <td>-0.058198</td>\n",
       "      <td>-0.028429</td>\n",
       "      <td>-0.050278</td>\n",
       "      <td>-0.080133</td>\n",
       "      <td>-0.024472</td>\n",
       "      <td>-0.052140</td>\n",
       "      <td>-0.019944</td>\n",
       "      <td>-7.289672e-02</td>\n",
       "      <td>-0.031952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-10</th>\n",
       "      <td>-0.001857</td>\n",
       "      <td>-0.007957</td>\n",
       "      <td>-0.056608</td>\n",
       "      <td>-0.033577</td>\n",
       "      <td>-0.012766</td>\n",
       "      <td>-0.038382</td>\n",
       "      <td>-0.031704</td>\n",
       "      <td>-0.042638</td>\n",
       "      <td>-0.012963</td>\n",
       "      <td>-0.051148</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056529</td>\n",
       "      <td>-0.045813</td>\n",
       "      <td>-0.015254</td>\n",
       "      <td>0.108949</td>\n",
       "      <td>-0.000408</td>\n",
       "      <td>-0.018463</td>\n",
       "      <td>-0.030641</td>\n",
       "      <td>-0.039673</td>\n",
       "      <td>7.790037e-03</td>\n",
       "      <td>-0.075911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-09</th>\n",
       "      <td>-0.032497</td>\n",
       "      <td>0.014964</td>\n",
       "      <td>-0.041537</td>\n",
       "      <td>-0.019209</td>\n",
       "      <td>0.009759</td>\n",
       "      <td>-0.152829</td>\n",
       "      <td>-0.034908</td>\n",
       "      <td>-0.049782</td>\n",
       "      <td>-0.029491</td>\n",
       "      <td>-0.085189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030915</td>\n",
       "      <td>-0.006775</td>\n",
       "      <td>-0.098720</td>\n",
       "      <td>0.026074</td>\n",
       "      <td>-0.007489</td>\n",
       "      <td>-0.041385</td>\n",
       "      <td>-0.049166</td>\n",
       "      <td>-0.000544</td>\n",
       "      <td>-3.590120e-02</td>\n",
       "      <td>-0.052591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-08</th>\n",
       "      <td>-0.119326</td>\n",
       "      <td>-0.029451</td>\n",
       "      <td>-0.070543</td>\n",
       "      <td>-0.015943</td>\n",
       "      <td>0.022881</td>\n",
       "      <td>-0.186745</td>\n",
       "      <td>-0.023214</td>\n",
       "      <td>-0.047493</td>\n",
       "      <td>-0.034577</td>\n",
       "      <td>-0.046590</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051192</td>\n",
       "      <td>-0.039952</td>\n",
       "      <td>-0.071749</td>\n",
       "      <td>-0.048822</td>\n",
       "      <td>-0.039231</td>\n",
       "      <td>0.017063</td>\n",
       "      <td>-0.038039</td>\n",
       "      <td>-0.024803</td>\n",
       "      <td>-2.449209e-02</td>\n",
       "      <td>-0.070835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-07</th>\n",
       "      <td>-0.039900</td>\n",
       "      <td>0.012059</td>\n",
       "      <td>-0.056630</td>\n",
       "      <td>-0.000645</td>\n",
       "      <td>-0.030319</td>\n",
       "      <td>-0.127271</td>\n",
       "      <td>-0.049799</td>\n",
       "      <td>-0.023339</td>\n",
       "      <td>-0.029050</td>\n",
       "      <td>-0.079179</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046167</td>\n",
       "      <td>0.016789</td>\n",
       "      <td>0.003434</td>\n",
       "      <td>0.055430</td>\n",
       "      <td>-0.044564</td>\n",
       "      <td>-0.025124</td>\n",
       "      <td>-0.087165</td>\n",
       "      <td>0.004368</td>\n",
       "      <td>-4.533198e-02</td>\n",
       "      <td>-0.074954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-02</th>\n",
       "      <td>-0.072393</td>\n",
       "      <td>-0.072199</td>\n",
       "      <td>-0.091406</td>\n",
       "      <td>-0.072376</td>\n",
       "      <td>-0.001441</td>\n",
       "      <td>-0.069006</td>\n",
       "      <td>-0.054409</td>\n",
       "      <td>-0.027164</td>\n",
       "      <td>0.062017</td>\n",
       "      <td>-0.128361</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.066717</td>\n",
       "      <td>-0.000397</td>\n",
       "      <td>-0.034413</td>\n",
       "      <td>-0.047709</td>\n",
       "      <td>-0.072616</td>\n",
       "      <td>-0.041856</td>\n",
       "      <td>-0.050222</td>\n",
       "      <td>-0.016288</td>\n",
       "      <td>-8.383439e-03</td>\n",
       "      <td>-0.039350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-01</th>\n",
       "      <td>-0.081210</td>\n",
       "      <td>-0.020391</td>\n",
       "      <td>0.099262</td>\n",
       "      <td>-0.018858</td>\n",
       "      <td>-0.136634</td>\n",
       "      <td>-0.083397</td>\n",
       "      <td>-0.056063</td>\n",
       "      <td>-0.039072</td>\n",
       "      <td>-0.025318</td>\n",
       "      <td>0.021731</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054461</td>\n",
       "      <td>-0.056564</td>\n",
       "      <td>-0.034920</td>\n",
       "      <td>0.007488</td>\n",
       "      <td>-0.025096</td>\n",
       "      <td>-0.018787</td>\n",
       "      <td>-0.029751</td>\n",
       "      <td>0.011307</td>\n",
       "      <td>-3.998029e-02</td>\n",
       "      <td>-0.038914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-31</th>\n",
       "      <td>-0.112960</td>\n",
       "      <td>-0.071512</td>\n",
       "      <td>-0.105417</td>\n",
       "      <td>-0.066444</td>\n",
       "      <td>-0.069153</td>\n",
       "      <td>-0.081507</td>\n",
       "      <td>-0.042095</td>\n",
       "      <td>-0.034474</td>\n",
       "      <td>-0.057237</td>\n",
       "      <td>-0.034575</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058288</td>\n",
       "      <td>-0.096375</td>\n",
       "      <td>-0.038522</td>\n",
       "      <td>-0.056268</td>\n",
       "      <td>-0.050467</td>\n",
       "      <td>-0.035428</td>\n",
       "      <td>-0.043417</td>\n",
       "      <td>-0.056971</td>\n",
       "      <td>2.785112e-17</td>\n",
       "      <td>-0.038776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-28</th>\n",
       "      <td>-0.118123</td>\n",
       "      <td>-0.051635</td>\n",
       "      <td>-0.075864</td>\n",
       "      <td>-0.067341</td>\n",
       "      <td>-0.003632</td>\n",
       "      <td>-0.103423</td>\n",
       "      <td>-0.024447</td>\n",
       "      <td>-0.036920</td>\n",
       "      <td>-0.061538</td>\n",
       "      <td>-0.066325</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081006</td>\n",
       "      <td>-0.032072</td>\n",
       "      <td>-0.057375</td>\n",
       "      <td>-0.008117</td>\n",
       "      <td>-0.085291</td>\n",
       "      <td>-0.091201</td>\n",
       "      <td>-0.048182</td>\n",
       "      <td>-0.059747</td>\n",
       "      <td>-5.193123e-02</td>\n",
       "      <td>-0.086363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-27</th>\n",
       "      <td>-0.101479</td>\n",
       "      <td>-0.102112</td>\n",
       "      <td>-0.078104</td>\n",
       "      <td>-0.096516</td>\n",
       "      <td>-0.022157</td>\n",
       "      <td>-0.010757</td>\n",
       "      <td>-0.104324</td>\n",
       "      <td>-0.040602</td>\n",
       "      <td>-0.088521</td>\n",
       "      <td>-0.060760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.047624</td>\n",
       "      <td>-0.083475</td>\n",
       "      <td>-0.066640</td>\n",
       "      <td>-0.092776</td>\n",
       "      <td>-0.089591</td>\n",
       "      <td>-0.087135</td>\n",
       "      <td>-0.037175</td>\n",
       "      <td>-0.049250</td>\n",
       "      <td>-7.456348e-02</td>\n",
       "      <td>-0.088761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 650 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              916328    936365    905271    905113    905802    905425  \\\n",
       "date                                                                     \n",
       "2018-05-11 -0.033261 -0.056147 -0.056673 -0.039225 -0.078155 -0.079176   \n",
       "2018-05-10 -0.001857 -0.007957 -0.056608 -0.033577 -0.012766 -0.038382   \n",
       "2018-05-09 -0.032497  0.014964 -0.041537 -0.019209  0.009759 -0.152829   \n",
       "2018-05-08 -0.119326 -0.029451 -0.070543 -0.015943  0.022881 -0.186745   \n",
       "2018-05-07 -0.039900  0.012059 -0.056630 -0.000645 -0.030319 -0.127271   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2017-08-02 -0.072393 -0.072199 -0.091406 -0.072376 -0.001441 -0.069006   \n",
       "2017-08-01 -0.081210 -0.020391  0.099262 -0.018858 -0.136634 -0.083397   \n",
       "2017-07-31 -0.112960 -0.071512 -0.105417 -0.066444 -0.069153 -0.081507   \n",
       "2017-07-28 -0.118123 -0.051635 -0.075864 -0.067341 -0.003632 -0.103423   \n",
       "2017-07-27 -0.101479 -0.102112 -0.078104 -0.096516 -0.022157 -0.010757   \n",
       "\n",
       "              906156    916305    992816    921093  ...    311917    69568X  \\\n",
       "date                                                ...                       \n",
       "2018-05-11 -0.055281 -0.036198 -0.062879 -0.060282  ... -0.030098 -0.058198   \n",
       "2018-05-10 -0.031704 -0.042638 -0.012963 -0.051148  ... -0.056529 -0.045813   \n",
       "2018-05-09 -0.034908 -0.049782 -0.029491 -0.085189  ...  0.030915 -0.006775   \n",
       "2018-05-08 -0.023214 -0.047493 -0.034577 -0.046590  ... -0.051192 -0.039952   \n",
       "2018-05-07 -0.049799 -0.023339 -0.029050 -0.079179  ... -0.046167  0.016789   \n",
       "...              ...       ...       ...       ...  ...       ...       ...   \n",
       "2017-08-02 -0.054409 -0.027164  0.062017 -0.128361  ... -0.066717 -0.000397   \n",
       "2017-08-01 -0.056063 -0.039072 -0.025318  0.021731  ... -0.054461 -0.056564   \n",
       "2017-07-31 -0.042095 -0.034474 -0.057237 -0.034575  ... -0.058288 -0.096375   \n",
       "2017-07-28 -0.024447 -0.036920 -0.061538 -0.066325  ... -0.081006 -0.032072   \n",
       "2017-07-27 -0.104324 -0.040602 -0.088521 -0.060760  ... -0.047624 -0.083475   \n",
       "\n",
       "              543755    77463M    29235J    131745    69487D    68157P  \\\n",
       "date                                                                     \n",
       "2018-05-11 -0.028429 -0.050278 -0.080133 -0.024472 -0.052140 -0.019944   \n",
       "2018-05-10 -0.015254  0.108949 -0.000408 -0.018463 -0.030641 -0.039673   \n",
       "2018-05-09 -0.098720  0.026074 -0.007489 -0.041385 -0.049166 -0.000544   \n",
       "2018-05-08 -0.071749 -0.048822 -0.039231  0.017063 -0.038039 -0.024803   \n",
       "2018-05-07  0.003434  0.055430 -0.044564 -0.025124 -0.087165  0.004368   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2017-08-02 -0.034413 -0.047709 -0.072616 -0.041856 -0.050222 -0.016288   \n",
       "2017-08-01 -0.034920  0.007488 -0.025096 -0.018787 -0.029751  0.011307   \n",
       "2017-07-31 -0.038522 -0.056268 -0.050467 -0.035428 -0.043417 -0.056971   \n",
       "2017-07-28 -0.057375 -0.008117 -0.085291 -0.091201 -0.048182 -0.059747   \n",
       "2017-07-27 -0.066640 -0.092776 -0.089591 -0.087135 -0.037175 -0.049250   \n",
       "\n",
       "                  9110RA    292703  \n",
       "date                                \n",
       "2018-05-11 -7.289672e-02 -0.031952  \n",
       "2018-05-10  7.790037e-03 -0.075911  \n",
       "2018-05-09 -3.590120e-02 -0.052591  \n",
       "2018-05-08 -2.449209e-02 -0.070835  \n",
       "2018-05-07 -4.533198e-02 -0.074954  \n",
       "...                  ...       ...  \n",
       "2017-08-02 -8.383439e-03 -0.039350  \n",
       "2017-08-01 -3.998029e-02 -0.038914  \n",
       "2017-07-31  2.785112e-17 -0.038776  \n",
       "2017-07-28 -5.193123e-02 -0.086363  \n",
       "2017-07-27 -7.456348e-02 -0.088761  \n",
       "\n",
       "[200 rows x 650 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_investable(pd.to_datetime('2018-05-11'), 200)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tts = train_test_split(df, test_size=0.2, shuffle=False)\n",
    "train = tts[0]\n",
    "test = tts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>916328</th>\n",
       "      <th>936365</th>\n",
       "      <th>905271</th>\n",
       "      <th>905113</th>\n",
       "      <th>905802</th>\n",
       "      <th>905425</th>\n",
       "      <th>906156</th>\n",
       "      <th>916305</th>\n",
       "      <th>992816</th>\n",
       "      <th>921093</th>\n",
       "      <th>...</th>\n",
       "      <th>311917</th>\n",
       "      <th>69568X</th>\n",
       "      <th>543755</th>\n",
       "      <th>77463M</th>\n",
       "      <th>29235J</th>\n",
       "      <th>131745</th>\n",
       "      <th>69487D</th>\n",
       "      <th>68157P</th>\n",
       "      <th>9110RA</th>\n",
       "      <th>292703</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-09-28</th>\n",
       "      <td>0.033774</td>\n",
       "      <td>-0.032232</td>\n",
       "      <td>-0.071387</td>\n",
       "      <td>-0.084644</td>\n",
       "      <td>-0.076437</td>\n",
       "      <td>-0.076465</td>\n",
       "      <td>-0.035913</td>\n",
       "      <td>-0.049128</td>\n",
       "      <td>-0.059533</td>\n",
       "      <td>-0.089435</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.053948</td>\n",
       "      <td>-0.042377</td>\n",
       "      <td>-0.044853</td>\n",
       "      <td>0.024979</td>\n",
       "      <td>-0.039263</td>\n",
       "      <td>-0.017803</td>\n",
       "      <td>-0.042860</td>\n",
       "      <td>-0.043971</td>\n",
       "      <td>-0.016182</td>\n",
       "      <td>-0.061822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-27</th>\n",
       "      <td>-0.130996</td>\n",
       "      <td>0.005307</td>\n",
       "      <td>-0.063771</td>\n",
       "      <td>-0.039784</td>\n",
       "      <td>-0.017728</td>\n",
       "      <td>-0.143054</td>\n",
       "      <td>-0.030959</td>\n",
       "      <td>-0.014198</td>\n",
       "      <td>-0.029317</td>\n",
       "      <td>-0.102140</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050615</td>\n",
       "      <td>-0.057166</td>\n",
       "      <td>-0.010357</td>\n",
       "      <td>0.044859</td>\n",
       "      <td>0.029447</td>\n",
       "      <td>-0.029128</td>\n",
       "      <td>-0.038377</td>\n",
       "      <td>-0.028755</td>\n",
       "      <td>0.010398</td>\n",
       "      <td>-0.014246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-26</th>\n",
       "      <td>-0.060831</td>\n",
       "      <td>-0.052681</td>\n",
       "      <td>-0.084904</td>\n",
       "      <td>-0.036838</td>\n",
       "      <td>-0.040810</td>\n",
       "      <td>-0.079077</td>\n",
       "      <td>-0.041050</td>\n",
       "      <td>-0.057685</td>\n",
       "      <td>-0.006364</td>\n",
       "      <td>-0.070131</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.062638</td>\n",
       "      <td>-0.033960</td>\n",
       "      <td>-0.045905</td>\n",
       "      <td>-0.014777</td>\n",
       "      <td>-0.020430</td>\n",
       "      <td>-0.003253</td>\n",
       "      <td>-0.046713</td>\n",
       "      <td>-0.067935</td>\n",
       "      <td>-0.022240</td>\n",
       "      <td>-0.070503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-25</th>\n",
       "      <td>-0.071303</td>\n",
       "      <td>-0.115842</td>\n",
       "      <td>-0.041493</td>\n",
       "      <td>-0.057992</td>\n",
       "      <td>0.042759</td>\n",
       "      <td>-0.058970</td>\n",
       "      <td>-0.062454</td>\n",
       "      <td>-0.030108</td>\n",
       "      <td>-0.065593</td>\n",
       "      <td>-0.051800</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045482</td>\n",
       "      <td>-0.065947</td>\n",
       "      <td>-0.069041</td>\n",
       "      <td>-0.064799</td>\n",
       "      <td>-0.071611</td>\n",
       "      <td>-0.040592</td>\n",
       "      <td>-0.050160</td>\n",
       "      <td>-0.048418</td>\n",
       "      <td>-0.053214</td>\n",
       "      <td>-0.049827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-22</th>\n",
       "      <td>0.003938</td>\n",
       "      <td>-0.045452</td>\n",
       "      <td>-0.090654</td>\n",
       "      <td>-0.104774</td>\n",
       "      <td>-0.000160</td>\n",
       "      <td>-0.142855</td>\n",
       "      <td>-0.057031</td>\n",
       "      <td>-0.033048</td>\n",
       "      <td>-0.067770</td>\n",
       "      <td>-0.059431</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060346</td>\n",
       "      <td>-0.109388</td>\n",
       "      <td>-0.023386</td>\n",
       "      <td>-0.023368</td>\n",
       "      <td>-0.035876</td>\n",
       "      <td>-0.042236</td>\n",
       "      <td>-0.047319</td>\n",
       "      <td>-0.029264</td>\n",
       "      <td>-0.056097</td>\n",
       "      <td>-0.055420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 650 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              916328    936365    905271    905113    905802    905425  \\\n",
       "date                                                                     \n",
       "2017-09-28  0.033774 -0.032232 -0.071387 -0.084644 -0.076437 -0.076465   \n",
       "2017-09-27 -0.130996  0.005307 -0.063771 -0.039784 -0.017728 -0.143054   \n",
       "2017-09-26 -0.060831 -0.052681 -0.084904 -0.036838 -0.040810 -0.079077   \n",
       "2017-09-25 -0.071303 -0.115842 -0.041493 -0.057992  0.042759 -0.058970   \n",
       "2017-09-22  0.003938 -0.045452 -0.090654 -0.104774 -0.000160 -0.142855   \n",
       "\n",
       "              906156    916305    992816    921093  ...    311917    69568X  \\\n",
       "date                                                ...                       \n",
       "2017-09-28 -0.035913 -0.049128 -0.059533 -0.089435  ... -0.053948 -0.042377   \n",
       "2017-09-27 -0.030959 -0.014198 -0.029317 -0.102140  ... -0.050615 -0.057166   \n",
       "2017-09-26 -0.041050 -0.057685 -0.006364 -0.070131  ... -0.062638 -0.033960   \n",
       "2017-09-25 -0.062454 -0.030108 -0.065593 -0.051800  ... -0.045482 -0.065947   \n",
       "2017-09-22 -0.057031 -0.033048 -0.067770 -0.059431  ... -0.060346 -0.109388   \n",
       "\n",
       "              543755    77463M    29235J    131745    69487D    68157P  \\\n",
       "date                                                                     \n",
       "2017-09-28 -0.044853  0.024979 -0.039263 -0.017803 -0.042860 -0.043971   \n",
       "2017-09-27 -0.010357  0.044859  0.029447 -0.029128 -0.038377 -0.028755   \n",
       "2017-09-26 -0.045905 -0.014777 -0.020430 -0.003253 -0.046713 -0.067935   \n",
       "2017-09-25 -0.069041 -0.064799 -0.071611 -0.040592 -0.050160 -0.048418   \n",
       "2017-09-22 -0.023386 -0.023368 -0.035876 -0.042236 -0.047319 -0.029264   \n",
       "\n",
       "              9110RA    292703  \n",
       "date                            \n",
       "2017-09-28 -0.016182 -0.061822  \n",
       "2017-09-27  0.010398 -0.014246  \n",
       "2017-09-26 -0.022240 -0.070503  \n",
       "2017-09-25 -0.053214 -0.049827  \n",
       "2017-09-22 -0.056097 -0.055420  \n",
       "\n",
       "[5 rows x 650 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>916328</th>\n",
       "      <th>936365</th>\n",
       "      <th>905271</th>\n",
       "      <th>905113</th>\n",
       "      <th>905802</th>\n",
       "      <th>905425</th>\n",
       "      <th>906156</th>\n",
       "      <th>916305</th>\n",
       "      <th>992816</th>\n",
       "      <th>921093</th>\n",
       "      <th>...</th>\n",
       "      <th>311917</th>\n",
       "      <th>69568X</th>\n",
       "      <th>543755</th>\n",
       "      <th>77463M</th>\n",
       "      <th>29235J</th>\n",
       "      <th>131745</th>\n",
       "      <th>69487D</th>\n",
       "      <th>68157P</th>\n",
       "      <th>9110RA</th>\n",
       "      <th>292703</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-08-02</th>\n",
       "      <td>-0.072393</td>\n",
       "      <td>-0.072199</td>\n",
       "      <td>-0.091406</td>\n",
       "      <td>-0.072376</td>\n",
       "      <td>-0.001441</td>\n",
       "      <td>-0.069006</td>\n",
       "      <td>-0.054409</td>\n",
       "      <td>-0.027164</td>\n",
       "      <td>0.062017</td>\n",
       "      <td>-0.128361</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.066717</td>\n",
       "      <td>-0.000397</td>\n",
       "      <td>-0.034413</td>\n",
       "      <td>-0.047709</td>\n",
       "      <td>-0.072616</td>\n",
       "      <td>-0.041856</td>\n",
       "      <td>-0.050222</td>\n",
       "      <td>-0.016288</td>\n",
       "      <td>-8.383439e-03</td>\n",
       "      <td>-0.039350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-01</th>\n",
       "      <td>-0.081210</td>\n",
       "      <td>-0.020391</td>\n",
       "      <td>0.099262</td>\n",
       "      <td>-0.018858</td>\n",
       "      <td>-0.136634</td>\n",
       "      <td>-0.083397</td>\n",
       "      <td>-0.056063</td>\n",
       "      <td>-0.039072</td>\n",
       "      <td>-0.025318</td>\n",
       "      <td>0.021731</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054461</td>\n",
       "      <td>-0.056564</td>\n",
       "      <td>-0.034920</td>\n",
       "      <td>0.007488</td>\n",
       "      <td>-0.025096</td>\n",
       "      <td>-0.018787</td>\n",
       "      <td>-0.029751</td>\n",
       "      <td>0.011307</td>\n",
       "      <td>-3.998029e-02</td>\n",
       "      <td>-0.038914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-31</th>\n",
       "      <td>-0.112960</td>\n",
       "      <td>-0.071512</td>\n",
       "      <td>-0.105417</td>\n",
       "      <td>-0.066444</td>\n",
       "      <td>-0.069153</td>\n",
       "      <td>-0.081507</td>\n",
       "      <td>-0.042095</td>\n",
       "      <td>-0.034474</td>\n",
       "      <td>-0.057237</td>\n",
       "      <td>-0.034575</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058288</td>\n",
       "      <td>-0.096375</td>\n",
       "      <td>-0.038522</td>\n",
       "      <td>-0.056268</td>\n",
       "      <td>-0.050467</td>\n",
       "      <td>-0.035428</td>\n",
       "      <td>-0.043417</td>\n",
       "      <td>-0.056971</td>\n",
       "      <td>2.785112e-17</td>\n",
       "      <td>-0.038776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-28</th>\n",
       "      <td>-0.118123</td>\n",
       "      <td>-0.051635</td>\n",
       "      <td>-0.075864</td>\n",
       "      <td>-0.067341</td>\n",
       "      <td>-0.003632</td>\n",
       "      <td>-0.103423</td>\n",
       "      <td>-0.024447</td>\n",
       "      <td>-0.036920</td>\n",
       "      <td>-0.061538</td>\n",
       "      <td>-0.066325</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081006</td>\n",
       "      <td>-0.032072</td>\n",
       "      <td>-0.057375</td>\n",
       "      <td>-0.008117</td>\n",
       "      <td>-0.085291</td>\n",
       "      <td>-0.091201</td>\n",
       "      <td>-0.048182</td>\n",
       "      <td>-0.059747</td>\n",
       "      <td>-5.193123e-02</td>\n",
       "      <td>-0.086363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-27</th>\n",
       "      <td>-0.101479</td>\n",
       "      <td>-0.102112</td>\n",
       "      <td>-0.078104</td>\n",
       "      <td>-0.096516</td>\n",
       "      <td>-0.022157</td>\n",
       "      <td>-0.010757</td>\n",
       "      <td>-0.104324</td>\n",
       "      <td>-0.040602</td>\n",
       "      <td>-0.088521</td>\n",
       "      <td>-0.060760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.047624</td>\n",
       "      <td>-0.083475</td>\n",
       "      <td>-0.066640</td>\n",
       "      <td>-0.092776</td>\n",
       "      <td>-0.089591</td>\n",
       "      <td>-0.087135</td>\n",
       "      <td>-0.037175</td>\n",
       "      <td>-0.049250</td>\n",
       "      <td>-7.456348e-02</td>\n",
       "      <td>-0.088761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 650 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              916328    936365    905271    905113    905802    905425  \\\n",
       "date                                                                     \n",
       "2017-08-02 -0.072393 -0.072199 -0.091406 -0.072376 -0.001441 -0.069006   \n",
       "2017-08-01 -0.081210 -0.020391  0.099262 -0.018858 -0.136634 -0.083397   \n",
       "2017-07-31 -0.112960 -0.071512 -0.105417 -0.066444 -0.069153 -0.081507   \n",
       "2017-07-28 -0.118123 -0.051635 -0.075864 -0.067341 -0.003632 -0.103423   \n",
       "2017-07-27 -0.101479 -0.102112 -0.078104 -0.096516 -0.022157 -0.010757   \n",
       "\n",
       "              906156    916305    992816    921093  ...    311917    69568X  \\\n",
       "date                                                ...                       \n",
       "2017-08-02 -0.054409 -0.027164  0.062017 -0.128361  ... -0.066717 -0.000397   \n",
       "2017-08-01 -0.056063 -0.039072 -0.025318  0.021731  ... -0.054461 -0.056564   \n",
       "2017-07-31 -0.042095 -0.034474 -0.057237 -0.034575  ... -0.058288 -0.096375   \n",
       "2017-07-28 -0.024447 -0.036920 -0.061538 -0.066325  ... -0.081006 -0.032072   \n",
       "2017-07-27 -0.104324 -0.040602 -0.088521 -0.060760  ... -0.047624 -0.083475   \n",
       "\n",
       "              543755    77463M    29235J    131745    69487D    68157P  \\\n",
       "date                                                                     \n",
       "2017-08-02 -0.034413 -0.047709 -0.072616 -0.041856 -0.050222 -0.016288   \n",
       "2017-08-01 -0.034920  0.007488 -0.025096 -0.018787 -0.029751  0.011307   \n",
       "2017-07-31 -0.038522 -0.056268 -0.050467 -0.035428 -0.043417 -0.056971   \n",
       "2017-07-28 -0.057375 -0.008117 -0.085291 -0.091201 -0.048182 -0.059747   \n",
       "2017-07-27 -0.066640 -0.092776 -0.089591 -0.087135 -0.037175 -0.049250   \n",
       "\n",
       "                  9110RA    292703  \n",
       "date                                \n",
       "2017-08-02 -8.383439e-03 -0.039350  \n",
       "2017-08-01 -3.998029e-02 -0.038914  \n",
       "2017-07-31  2.785112e-17 -0.038776  \n",
       "2017-07-28 -5.193123e-02 -0.086363  \n",
       "2017-07-27 -7.456348e-02 -0.088761  \n",
       "\n",
       "[5 rows x 650 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>640</th>\n",
       "      <th>641</th>\n",
       "      <th>642</th>\n",
       "      <th>643</th>\n",
       "      <th>644</th>\n",
       "      <th>645</th>\n",
       "      <th>646</th>\n",
       "      <th>647</th>\n",
       "      <th>648</th>\n",
       "      <th>649</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.661406</td>\n",
       "      <td>0.441272</td>\n",
       "      <td>0.526470</td>\n",
       "      <td>0.828958</td>\n",
       "      <td>0.460741</td>\n",
       "      <td>0.520905</td>\n",
       "      <td>0.428907</td>\n",
       "      <td>0.635693</td>\n",
       "      <td>0.393566</td>\n",
       "      <td>0.529545</td>\n",
       "      <td>...</td>\n",
       "      <td>0.343729</td>\n",
       "      <td>0.461508</td>\n",
       "      <td>0.571224</td>\n",
       "      <td>0.259734</td>\n",
       "      <td>0.385493</td>\n",
       "      <td>0.413940</td>\n",
       "      <td>0.525361</td>\n",
       "      <td>0.613774</td>\n",
       "      <td>0.145999</td>\n",
       "      <td>0.745686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.749938</td>\n",
       "      <td>0.551324</td>\n",
       "      <td>0.526684</td>\n",
       "      <td>0.837788</td>\n",
       "      <td>0.648450</td>\n",
       "      <td>0.707915</td>\n",
       "      <td>0.492168</td>\n",
       "      <td>0.598313</td>\n",
       "      <td>0.634965</td>\n",
       "      <td>0.560486</td>\n",
       "      <td>...</td>\n",
       "      <td>0.270439</td>\n",
       "      <td>0.505696</td>\n",
       "      <td>0.629955</td>\n",
       "      <td>0.714857</td>\n",
       "      <td>0.675644</td>\n",
       "      <td>0.429399</td>\n",
       "      <td>0.575786</td>\n",
       "      <td>0.548986</td>\n",
       "      <td>0.374450</td>\n",
       "      <td>0.664123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.663560</td>\n",
       "      <td>0.603668</td>\n",
       "      <td>0.576641</td>\n",
       "      <td>0.860254</td>\n",
       "      <td>0.713111</td>\n",
       "      <td>0.183266</td>\n",
       "      <td>0.483571</td>\n",
       "      <td>0.556848</td>\n",
       "      <td>0.555032</td>\n",
       "      <td>0.445173</td>\n",
       "      <td>...</td>\n",
       "      <td>0.512915</td>\n",
       "      <td>0.644981</td>\n",
       "      <td>0.257864</td>\n",
       "      <td>0.477973</td>\n",
       "      <td>0.649873</td>\n",
       "      <td>0.370430</td>\n",
       "      <td>0.532337</td>\n",
       "      <td>0.677480</td>\n",
       "      <td>0.250746</td>\n",
       "      <td>0.707392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.418774</td>\n",
       "      <td>0.502238</td>\n",
       "      <td>0.480496</td>\n",
       "      <td>0.865360</td>\n",
       "      <td>0.750778</td>\n",
       "      <td>0.027789</td>\n",
       "      <td>0.514947</td>\n",
       "      <td>0.570130</td>\n",
       "      <td>0.530440</td>\n",
       "      <td>0.575925</td>\n",
       "      <td>...</td>\n",
       "      <td>0.285238</td>\n",
       "      <td>0.526607</td>\n",
       "      <td>0.378104</td>\n",
       "      <td>0.263895</td>\n",
       "      <td>0.534352</td>\n",
       "      <td>0.520791</td>\n",
       "      <td>0.558434</td>\n",
       "      <td>0.597818</td>\n",
       "      <td>0.283049</td>\n",
       "      <td>0.673541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.642690</td>\n",
       "      <td>0.597032</td>\n",
       "      <td>0.526614</td>\n",
       "      <td>0.889280</td>\n",
       "      <td>0.598061</td>\n",
       "      <td>0.300429</td>\n",
       "      <td>0.443615</td>\n",
       "      <td>0.710329</td>\n",
       "      <td>0.557165</td>\n",
       "      <td>0.465532</td>\n",
       "      <td>...</td>\n",
       "      <td>0.299173</td>\n",
       "      <td>0.729053</td>\n",
       "      <td>0.713267</td>\n",
       "      <td>0.561883</td>\n",
       "      <td>0.514943</td>\n",
       "      <td>0.412264</td>\n",
       "      <td>0.443215</td>\n",
       "      <td>0.693610</td>\n",
       "      <td>0.224044</td>\n",
       "      <td>0.665899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 650 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.661406  0.441272  0.526470  0.828958  0.460741  0.520905  0.428907   \n",
       "1  0.749938  0.551324  0.526684  0.837788  0.648450  0.707915  0.492168   \n",
       "2  0.663560  0.603668  0.576641  0.860254  0.713111  0.183266  0.483571   \n",
       "3  0.418774  0.502238  0.480496  0.865360  0.750778  0.027789  0.514947   \n",
       "4  0.642690  0.597032  0.526614  0.889280  0.598061  0.300429  0.443615   \n",
       "\n",
       "        7         8         9    ...       640       641       642       643  \\\n",
       "0  0.635693  0.393566  0.529545  ...  0.343729  0.461508  0.571224  0.259734   \n",
       "1  0.598313  0.634965  0.560486  ...  0.270439  0.505696  0.629955  0.714857   \n",
       "2  0.556848  0.555032  0.445173  ...  0.512915  0.644981  0.257864  0.477973   \n",
       "3  0.570130  0.530440  0.575925  ...  0.285238  0.526607  0.378104  0.263895   \n",
       "4  0.710329  0.557165  0.465532  ...  0.299173  0.729053  0.713267  0.561883   \n",
       "\n",
       "        644       645       646       647       648       649  \n",
       "0  0.385493  0.413940  0.525361  0.613774  0.145999  0.745686  \n",
       "1  0.675644  0.429399  0.575786  0.548986  0.374450  0.664123  \n",
       "2  0.649873  0.370430  0.532337  0.677480  0.250746  0.707392  \n",
       "3  0.534352  0.520791  0.558434  0.597818  0.283049  0.673541  \n",
       "4  0.514943  0.412264  0.443215  0.693610  0.224044  0.665899  \n",
       "\n",
       "[5 rows x 650 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Scaling\n",
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "training_set_scaled = sc.fit_transform(train)\n",
    "test_set_scaled = sc.fit_transform(test)\n",
    "pd.DataFrame(training_set_scaled).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple multi-layer percepetron (MLP) autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 650)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 1953      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 650)               2600      \n",
      "=================================================================\n",
      "Total params: 4,553\n",
      "Trainable params: 4,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "1/1 [==============================] - 10s 10s/step - loss: 0.3038 - val_loss: 0.3284\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.2989 - val_loss: 0.3252\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.2955 - val_loss: 0.3225\n",
      "Epoch 4/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2927 - val_loss: 0.3194\n",
      "Epoch 5/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.2898 - val_loss: 0.3161\n",
      "Epoch 6/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2865 - val_loss: 0.3126\n",
      "Epoch 7/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.2831 - val_loss: 0.3092\n",
      "Epoch 8/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.2798 - val_loss: 0.3058\n",
      "Epoch 9/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2765 - val_loss: 0.3025\n",
      "Epoch 10/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2733 - val_loss: 0.2992\n",
      "Epoch 11/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.2701 - val_loss: 0.2958\n",
      "Epoch 12/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.2668 - val_loss: 0.2922\n",
      "Epoch 13/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2633 - val_loss: 0.2886\n",
      "Epoch 14/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2597 - val_loss: 0.2849\n",
      "Epoch 15/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2560 - val_loss: 0.2811\n",
      "Epoch 16/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2522 - val_loss: 0.2773\n",
      "Epoch 17/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.2485 - val_loss: 0.2736\n",
      "Epoch 18/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.2447 - val_loss: 0.2698\n",
      "Epoch 19/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.2410 - val_loss: 0.2661\n",
      "Epoch 20/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.2373 - val_loss: 0.2624\n",
      "Epoch 21/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2336 - val_loss: 0.2587\n",
      "Epoch 22/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.2300 - val_loss: 0.2551\n",
      "Epoch 23/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.2264 - val_loss: 0.2515\n",
      "Epoch 24/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2229 - val_loss: 0.2480\n",
      "Epoch 25/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.2194 - val_loss: 0.2445\n",
      "Epoch 26/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2159 - val_loss: 0.2410\n",
      "Epoch 27/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.2125 - val_loss: 0.2376\n",
      "Epoch 28/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.2091 - val_loss: 0.2342\n",
      "Epoch 29/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.2058 - val_loss: 0.2309\n",
      "Epoch 30/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2025 - val_loss: 0.2277\n",
      "Epoch 31/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1993 - val_loss: 0.2244\n",
      "Epoch 32/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1961 - val_loss: 0.2213\n",
      "Epoch 33/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1929 - val_loss: 0.2181\n",
      "Epoch 34/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.1898 - val_loss: 0.2150\n",
      "Epoch 35/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.1867 - val_loss: 0.2120\n",
      "Epoch 36/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1837 - val_loss: 0.2090\n",
      "Epoch 37/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.1807 - val_loss: 0.2061\n",
      "Epoch 38/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1778 - val_loss: 0.2032\n",
      "Epoch 39/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1749 - val_loss: 0.2003\n",
      "Epoch 40/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.1721 - val_loss: 0.1975\n",
      "Epoch 41/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1693 - val_loss: 0.1948\n",
      "Epoch 42/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.1665 - val_loss: 0.1921\n",
      "Epoch 43/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1638 - val_loss: 0.1894\n",
      "Epoch 44/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1611 - val_loss: 0.1868\n",
      "Epoch 45/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.1585 - val_loss: 0.1843\n",
      "Epoch 46/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1559 - val_loss: 0.1817\n",
      "Epoch 47/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1534 - val_loss: 0.1793\n",
      "Epoch 48/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1509 - val_loss: 0.1768\n",
      "Epoch 49/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.1484 - val_loss: 0.1744\n",
      "Epoch 50/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1460 - val_loss: 0.1721\n",
      "Epoch 51/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1436 - val_loss: 0.1698\n",
      "Epoch 52/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1413 - val_loss: 0.1675\n",
      "Epoch 53/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.1390 - val_loss: 0.1653\n",
      "Epoch 54/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.1367 - val_loss: 0.1631\n",
      "Epoch 55/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1345 - val_loss: 0.1610\n",
      "Epoch 56/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1323 - val_loss: 0.1589\n",
      "Epoch 57/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.1302 - val_loss: 0.1568\n",
      "Epoch 58/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.1281 - val_loss: 0.1548\n",
      "Epoch 59/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1260 - val_loss: 0.1528\n",
      "Epoch 60/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1240 - val_loss: 0.1508\n",
      "Epoch 61/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1220 - val_loss: 0.1489\n",
      "Epoch 62/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1200 - val_loss: 0.1471\n",
      "Epoch 63/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1181 - val_loss: 0.1452\n",
      "Epoch 64/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1162 - val_loss: 0.1434\n",
      "Epoch 65/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1143 - val_loss: 0.1416\n",
      "Epoch 66/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.1125 - val_loss: 0.1399\n",
      "Epoch 67/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.1107 - val_loss: 0.1382\n",
      "Epoch 68/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.1089 - val_loss: 0.1365\n",
      "Epoch 69/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.1071 - val_loss: 0.1349\n",
      "Epoch 70/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.1054 - val_loss: 0.1333\n",
      "Epoch 71/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.1038 - val_loss: 0.1317\n",
      "Epoch 72/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.1021 - val_loss: 0.1302\n",
      "Epoch 73/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1005 - val_loss: 0.1286\n",
      "Epoch 74/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0989 - val_loss: 0.1272\n",
      "Epoch 75/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0973 - val_loss: 0.1257\n",
      "Epoch 76/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0958 - val_loss: 0.1243\n",
      "Epoch 77/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0943 - val_loss: 0.1229\n",
      "Epoch 78/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0928 - val_loss: 0.1215\n",
      "Epoch 79/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0914 - val_loss: 0.1202\n",
      "Epoch 80/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0899 - val_loss: 0.1189\n",
      "Epoch 81/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0885 - val_loss: 0.1176\n",
      "Epoch 82/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0872 - val_loss: 0.1163\n",
      "Epoch 83/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0858 - val_loss: 0.1151\n",
      "Epoch 84/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0845 - val_loss: 0.1139\n",
      "Epoch 85/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0832 - val_loss: 0.1127\n",
      "Epoch 86/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0819 - val_loss: 0.1116\n",
      "Epoch 87/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0806 - val_loss: 0.1104\n",
      "Epoch 88/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0794 - val_loss: 0.1093\n",
      "Epoch 89/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0782 - val_loss: 0.1082\n",
      "Epoch 90/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0770 - val_loss: 0.1072\n",
      "Epoch 91/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0758 - val_loss: 0.1061\n",
      "Epoch 92/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0747 - val_loss: 0.1051\n",
      "Epoch 93/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0736 - val_loss: 0.1041\n",
      "Epoch 94/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0725 - val_loss: 0.1031\n",
      "Epoch 95/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0714 - val_loss: 0.1022\n",
      "Epoch 96/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0703 - val_loss: 0.1013\n",
      "Epoch 97/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0693 - val_loss: 0.1003\n",
      "Epoch 98/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0683 - val_loss: 0.0995\n",
      "Epoch 99/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0673 - val_loss: 0.0986\n",
      "Epoch 100/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0663 - val_loss: 0.0977\n",
      "Epoch 101/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0653 - val_loss: 0.0969\n",
      "Epoch 102/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0644 - val_loss: 0.0961\n",
      "Epoch 103/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0634 - val_loss: 0.0953\n",
      "Epoch 104/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0625 - val_loss: 0.0945\n",
      "Epoch 105/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0616 - val_loss: 0.0937\n",
      "Epoch 106/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0607 - val_loss: 0.0930\n",
      "Epoch 107/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0599 - val_loss: 0.0923\n",
      "Epoch 108/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0590 - val_loss: 0.0916\n",
      "Epoch 109/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0582 - val_loss: 0.0909\n",
      "Epoch 110/500\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0574 - val_loss: 0.0902\n",
      "Epoch 111/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0566 - val_loss: 0.0895\n",
      "Epoch 112/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0558 - val_loss: 0.0889\n",
      "Epoch 113/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0550 - val_loss: 0.0882\n",
      "Epoch 114/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0543 - val_loss: 0.0876\n",
      "Epoch 115/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0535 - val_loss: 0.0870\n",
      "Epoch 116/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0528 - val_loss: 0.0864\n",
      "Epoch 117/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0521 - val_loss: 0.0859\n",
      "Epoch 118/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0514 - val_loss: 0.0853\n",
      "Epoch 119/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0507 - val_loss: 0.0848\n",
      "Epoch 120/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0501 - val_loss: 0.0842\n",
      "Epoch 121/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0494 - val_loss: 0.0837\n",
      "Epoch 122/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0488 - val_loss: 0.0832\n",
      "Epoch 123/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0481 - val_loss: 0.0827\n",
      "Epoch 124/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0475 - val_loss: 0.0822\n",
      "Epoch 125/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0469 - val_loss: 0.0817\n",
      "Epoch 126/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0463 - val_loss: 0.0813\n",
      "Epoch 127/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0457 - val_loss: 0.0808\n",
      "Epoch 128/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0452 - val_loss: 0.0804\n",
      "Epoch 129/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0446 - val_loss: 0.0800\n",
      "Epoch 130/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0440 - val_loss: 0.0795\n",
      "Epoch 131/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0435 - val_loss: 0.0791\n",
      "Epoch 132/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0430 - val_loss: 0.0787\n",
      "Epoch 133/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0425 - val_loss: 0.0783\n",
      "Epoch 134/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0419 - val_loss: 0.0780\n",
      "Epoch 135/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0415 - val_loss: 0.0776\n",
      "Epoch 136/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0410 - val_loss: 0.0772\n",
      "Epoch 137/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0405 - val_loss: 0.0769\n",
      "Epoch 138/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0400 - val_loss: 0.0766\n",
      "Epoch 139/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0396 - val_loss: 0.0762\n",
      "Epoch 140/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0391 - val_loss: 0.0759\n",
      "Epoch 141/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0387 - val_loss: 0.0756\n",
      "Epoch 142/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0382 - val_loss: 0.0753\n",
      "Epoch 143/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0378 - val_loss: 0.0750\n",
      "Epoch 144/500\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0374 - val_loss: 0.0747\n",
      "Epoch 145/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0370 - val_loss: 0.0744\n",
      "Epoch 146/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0366 - val_loss: 0.0742\n",
      "Epoch 147/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0362 - val_loss: 0.0739\n",
      "Epoch 148/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0358 - val_loss: 0.0736\n",
      "Epoch 149/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0354 - val_loss: 0.0734\n",
      "Epoch 150/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0351 - val_loss: 0.0731\n",
      "Epoch 151/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0347 - val_loss: 0.0729\n",
      "Epoch 152/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0344 - val_loss: 0.0727\n",
      "Epoch 153/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0340 - val_loss: 0.0724\n",
      "Epoch 154/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0337 - val_loss: 0.0722\n",
      "Epoch 155/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0333 - val_loss: 0.0720\n",
      "Epoch 156/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0330 - val_loss: 0.0718\n",
      "Epoch 157/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0327 - val_loss: 0.0716\n",
      "Epoch 158/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0324 - val_loss: 0.0714\n",
      "Epoch 159/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0321 - val_loss: 0.0712\n",
      "Epoch 160/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0317 - val_loss: 0.0710\n",
      "Epoch 161/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0314 - val_loss: 0.0708\n",
      "Epoch 162/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0312 - val_loss: 0.0707\n",
      "Epoch 163/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0309 - val_loss: 0.0705\n",
      "Epoch 164/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0306 - val_loss: 0.0703\n",
      "Epoch 165/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0303 - val_loss: 0.0701\n",
      "Epoch 166/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0300 - val_loss: 0.0700\n",
      "Epoch 167/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0297 - val_loss: 0.0698\n",
      "Epoch 168/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0295 - val_loss: 0.0697\n",
      "Epoch 169/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0292 - val_loss: 0.0695\n",
      "Epoch 170/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0289 - val_loss: 0.0693\n",
      "Epoch 171/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0287 - val_loss: 0.0692\n",
      "Epoch 172/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0284 - val_loss: 0.0690\n",
      "Epoch 173/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0282 - val_loss: 0.0689\n",
      "Epoch 174/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0279 - val_loss: 0.0687\n",
      "Epoch 175/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0277 - val_loss: 0.0686\n",
      "Epoch 176/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0275 - val_loss: 0.0684\n",
      "Epoch 177/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0272 - val_loss: 0.0683\n",
      "Epoch 178/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0270 - val_loss: 0.0681\n",
      "Epoch 179/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0268 - val_loss: 0.0680\n",
      "Epoch 180/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0266 - val_loss: 0.0679\n",
      "Epoch 181/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0263 - val_loss: 0.0678\n",
      "Epoch 182/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0261 - val_loss: 0.0677\n",
      "Epoch 183/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0259 - val_loss: 0.0676\n",
      "Epoch 184/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0257 - val_loss: 0.0675\n",
      "Epoch 185/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0255 - val_loss: 0.0674\n",
      "Epoch 186/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0253 - val_loss: 0.0673\n",
      "Epoch 187/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0251 - val_loss: 0.0672\n",
      "Epoch 188/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0249 - val_loss: 0.0671\n",
      "Epoch 189/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0247 - val_loss: 0.0670\n",
      "Epoch 190/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0246 - val_loss: 0.0669\n",
      "Epoch 191/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0244 - val_loss: 0.0668\n",
      "Epoch 192/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0242 - val_loss: 0.0667\n",
      "Epoch 193/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0240 - val_loss: 0.0666\n",
      "Epoch 194/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0239 - val_loss: 0.0665\n",
      "Epoch 195/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0237 - val_loss: 0.0664\n",
      "Epoch 196/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0236 - val_loss: 0.0663\n",
      "Epoch 197/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0234 - val_loss: 0.0662\n",
      "Epoch 198/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0233 - val_loss: 0.0661\n",
      "Epoch 199/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0231 - val_loss: 0.0660\n",
      "Epoch 200/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0230 - val_loss: 0.0660\n",
      "Epoch 201/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0228 - val_loss: 0.0659\n",
      "Epoch 202/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.022 - 0s 33ms/step - loss: 0.0227 - val_loss: 0.0658\n",
      "Epoch 203/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0226 - val_loss: 0.0658\n",
      "Epoch 204/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0224 - val_loss: 0.0657\n",
      "Epoch 205/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0223 - val_loss: 0.0657\n",
      "Epoch 206/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0222 - val_loss: 0.0656\n",
      "Epoch 207/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0220 - val_loss: 0.0656\n",
      "Epoch 208/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0219 - val_loss: 0.0655\n",
      "Epoch 209/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0218 - val_loss: 0.0655\n",
      "Epoch 210/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0217 - val_loss: 0.0654\n",
      "Epoch 211/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0216 - val_loss: 0.0654\n",
      "Epoch 212/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0215 - val_loss: 0.0653\n",
      "Epoch 213/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0213 - val_loss: 0.0653\n",
      "Epoch 214/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0212 - val_loss: 0.0652\n",
      "Epoch 215/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0211 - val_loss: 0.0652\n",
      "Epoch 216/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0210 - val_loss: 0.0652\n",
      "Epoch 217/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0209 - val_loss: 0.0651\n",
      "Epoch 218/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0208 - val_loss: 0.0651\n",
      "Epoch 219/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0207 - val_loss: 0.0651\n",
      "Epoch 220/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0206 - val_loss: 0.0650\n",
      "Epoch 221/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0205 - val_loss: 0.0650\n",
      "Epoch 222/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0204 - val_loss: 0.0650\n",
      "Epoch 223/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0203 - val_loss: 0.0650\n",
      "Epoch 224/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0203 - val_loss: 0.0650\n",
      "Epoch 225/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0202 - val_loss: 0.0650\n",
      "Epoch 226/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0201 - val_loss: 0.0649\n",
      "Epoch 227/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0200 - val_loss: 0.0649\n",
      "Epoch 228/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0199 - val_loss: 0.0649\n",
      "Epoch 229/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0198 - val_loss: 0.0649\n",
      "Epoch 230/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0198 - val_loss: 0.0649\n",
      "Epoch 231/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0197 - val_loss: 0.0649\n",
      "Epoch 232/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0196 - val_loss: 0.0649\n",
      "Epoch 233/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0195 - val_loss: 0.0648\n",
      "Epoch 234/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0195 - val_loss: 0.0648\n",
      "Epoch 235/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0194 - val_loss: 0.0648\n",
      "Epoch 236/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0193 - val_loss: 0.0648\n",
      "Epoch 237/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0192 - val_loss: 0.0648\n",
      "Epoch 238/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0192 - val_loss: 0.0648\n",
      "Epoch 239/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0191 - val_loss: 0.0648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 240/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0190 - val_loss: 0.0648\n",
      "Epoch 241/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0190 - val_loss: 0.0648\n",
      "Epoch 242/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0189 - val_loss: 0.0649\n",
      "Epoch 243/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0189 - val_loss: 0.0649\n",
      "Epoch 244/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0188 - val_loss: 0.0649\n",
      "Epoch 245/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0187 - val_loss: 0.0649\n",
      "Epoch 246/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0187 - val_loss: 0.0649\n",
      "Epoch 247/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0186 - val_loss: 0.0649\n",
      "Epoch 248/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0186 - val_loss: 0.0649\n",
      "Epoch 249/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0185 - val_loss: 0.0649\n",
      "Epoch 250/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0185 - val_loss: 0.0649\n",
      "Epoch 251/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0184 - val_loss: 0.0649\n",
      "Epoch 252/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0184 - val_loss: 0.0649\n",
      "Epoch 253/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0183 - val_loss: 0.0649\n",
      "Epoch 254/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0183 - val_loss: 0.0650\n",
      "Epoch 255/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0182 - val_loss: 0.0650\n",
      "Epoch 256/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0182 - val_loss: 0.0650\n",
      "Epoch 257/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0181 - val_loss: 0.0650\n",
      "Epoch 258/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0181 - val_loss: 0.0650\n",
      "Epoch 259/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0180 - val_loss: 0.0650\n",
      "Epoch 260/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0180 - val_loss: 0.0650\n",
      "Epoch 261/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0180 - val_loss: 0.0650\n",
      "Epoch 262/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0179 - val_loss: 0.0651\n",
      "Epoch 263/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0179 - val_loss: 0.0651\n",
      "Epoch 264/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0178 - val_loss: 0.0651\n",
      "Epoch 265/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0178 - val_loss: 0.0651\n",
      "Epoch 266/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0178 - val_loss: 0.0651\n",
      "Epoch 267/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0177 - val_loss: 0.0651\n",
      "Epoch 268/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0177 - val_loss: 0.0652\n",
      "Epoch 269/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0176 - val_loss: 0.0652\n",
      "Epoch 270/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0176 - val_loss: 0.0652\n",
      "Epoch 271/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0176 - val_loss: 0.0652\n",
      "Epoch 272/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0175 - val_loss: 0.0652\n",
      "Epoch 273/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0175 - val_loss: 0.0653\n",
      "Epoch 274/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0175 - val_loss: 0.0653\n",
      "Epoch 275/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0174 - val_loss: 0.0653\n",
      "Epoch 276/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0174 - val_loss: 0.0653\n",
      "Epoch 277/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0174 - val_loss: 0.0653\n",
      "Epoch 278/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0173 - val_loss: 0.0654\n",
      "Epoch 279/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0173 - val_loss: 0.0654\n",
      "Epoch 280/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0173 - val_loss: 0.0654\n",
      "Epoch 281/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0172 - val_loss: 0.0654\n",
      "Epoch 282/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0172 - val_loss: 0.0654\n",
      "Epoch 283/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0172 - val_loss: 0.0655\n",
      "Epoch 284/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0172 - val_loss: 0.0655\n",
      "Epoch 285/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0171 - val_loss: 0.0655\n",
      "Epoch 286/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0171 - val_loss: 0.0655\n",
      "Epoch 287/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0171 - val_loss: 0.0656\n",
      "Epoch 288/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0170 - val_loss: 0.0656\n",
      "Epoch 289/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0170 - val_loss: 0.0656\n",
      "Epoch 290/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0170 - val_loss: 0.0656\n",
      "Epoch 291/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0170 - val_loss: 0.0657\n",
      "Epoch 292/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0169 - val_loss: 0.0657\n",
      "Epoch 293/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0169 - val_loss: 0.0657\n",
      "Epoch 294/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0169 - val_loss: 0.0657\n",
      "Epoch 295/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0169 - val_loss: 0.0658\n",
      "Epoch 296/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0168 - val_loss: 0.0658\n",
      "Epoch 297/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0168 - val_loss: 0.0658\n",
      "Epoch 298/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0168 - val_loss: 0.0658\n",
      "Epoch 299/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0168 - val_loss: 0.0658\n",
      "Epoch 300/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0168 - val_loss: 0.0659\n",
      "Epoch 301/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0167 - val_loss: 0.0659\n",
      "Epoch 302/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0167 - val_loss: 0.0659\n",
      "Epoch 303/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0167 - val_loss: 0.0659\n",
      "Epoch 304/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0167 - val_loss: 0.0660\n",
      "Epoch 305/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0167 - val_loss: 0.0660\n",
      "Epoch 306/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0166 - val_loss: 0.0660\n",
      "Epoch 307/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0166 - val_loss: 0.0660\n",
      "Epoch 308/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0166 - val_loss: 0.0661\n",
      "Epoch 309/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0166 - val_loss: 0.0661\n",
      "Epoch 310/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0166 - val_loss: 0.0661\n",
      "Epoch 311/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0165 - val_loss: 0.0661\n",
      "Epoch 312/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0165 - val_loss: 0.0661\n",
      "Epoch 313/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0165 - val_loss: 0.0662\n",
      "Epoch 314/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0165 - val_loss: 0.0662\n",
      "Epoch 315/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0165 - val_loss: 0.0662\n",
      "Epoch 316/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0164 - val_loss: 0.0662\n",
      "Epoch 317/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0164 - val_loss: 0.0663\n",
      "Epoch 318/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0164 - val_loss: 0.0663\n",
      "Epoch 319/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0164 - val_loss: 0.0663\n",
      "Epoch 320/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0164 - val_loss: 0.0663\n",
      "Epoch 321/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0164 - val_loss: 0.0663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 322/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0163 - val_loss: 0.0664\n",
      "Epoch 323/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0163 - val_loss: 0.0664\n",
      "Epoch 324/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0163 - val_loss: 0.0664\n",
      "Epoch 325/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0163 - val_loss: 0.0664\n",
      "Epoch 326/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0163 - val_loss: 0.0665\n",
      "Epoch 327/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0163 - val_loss: 0.0665\n",
      "Epoch 328/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0162 - val_loss: 0.0665\n",
      "Epoch 329/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0162 - val_loss: 0.0665\n",
      "Epoch 330/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0162 - val_loss: 0.0665\n",
      "Epoch 331/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0162 - val_loss: 0.0666\n",
      "Epoch 332/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0162 - val_loss: 0.0666\n",
      "Epoch 333/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0162 - val_loss: 0.0666\n",
      "Epoch 334/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0162 - val_loss: 0.0666\n",
      "Epoch 335/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0161 - val_loss: 0.0666\n",
      "Epoch 336/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0161 - val_loss: 0.0667\n",
      "Epoch 337/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0161 - val_loss: 0.0667\n",
      "Epoch 338/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0161 - val_loss: 0.0667\n",
      "Epoch 339/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0161 - val_loss: 0.0667\n",
      "Epoch 340/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0161 - val_loss: 0.0667\n",
      "Epoch 341/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0161 - val_loss: 0.0668\n",
      "Epoch 342/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0161 - val_loss: 0.0668\n",
      "Epoch 343/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0160 - val_loss: 0.0668\n",
      "Epoch 344/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0160 - val_loss: 0.0668\n",
      "Epoch 345/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0160 - val_loss: 0.0668\n",
      "Epoch 346/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0160 - val_loss: 0.0669\n",
      "Epoch 347/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0160 - val_loss: 0.0669\n",
      "Epoch 348/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0160 - val_loss: 0.0669\n",
      "Epoch 349/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0160 - val_loss: 0.0669\n",
      "Epoch 350/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0160 - val_loss: 0.0669\n",
      "Epoch 351/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0159 - val_loss: 0.0670\n",
      "Epoch 352/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0159 - val_loss: 0.0670\n",
      "Epoch 353/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0159 - val_loss: 0.0670\n",
      "Epoch 354/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0159 - val_loss: 0.0670\n",
      "Epoch 355/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0159 - val_loss: 0.0670\n",
      "Epoch 356/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0159 - val_loss: 0.0670\n",
      "Epoch 357/500\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0159 - val_loss: 0.0671\n",
      "Epoch 358/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0159 - val_loss: 0.0671\n",
      "Epoch 359/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0158 - val_loss: 0.0671\n",
      "Epoch 360/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0158 - val_loss: 0.0671\n",
      "Epoch 361/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0158 - val_loss: 0.0671\n",
      "Epoch 362/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0158 - val_loss: 0.0671\n",
      "Epoch 363/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0158 - val_loss: 0.0672\n",
      "Epoch 364/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0158 - val_loss: 0.0672\n",
      "Epoch 365/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0158 - val_loss: 0.0672\n",
      "Epoch 366/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0158 - val_loss: 0.0672\n",
      "Epoch 367/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0158 - val_loss: 0.0672\n",
      "Epoch 368/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0158 - val_loss: 0.0672\n",
      "Epoch 369/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0157 - val_loss: 0.0672\n",
      "Epoch 370/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0157 - val_loss: 0.0673\n",
      "Epoch 371/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0157 - val_loss: 0.0673\n",
      "Epoch 372/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0157 - val_loss: 0.0673\n",
      "Epoch 373/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0157 - val_loss: 0.0673\n",
      "Epoch 374/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0157 - val_loss: 0.0673\n",
      "Epoch 375/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0157 - val_loss: 0.0673\n",
      "Epoch 376/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0157 - val_loss: 0.0673\n",
      "Epoch 377/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0157 - val_loss: 0.0674\n",
      "Epoch 378/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0157 - val_loss: 0.0674\n",
      "Epoch 379/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0156 - val_loss: 0.0674\n",
      "Epoch 380/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0156 - val_loss: 0.0674\n",
      "Epoch 381/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0156 - val_loss: 0.0674\n",
      "Epoch 382/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0156 - val_loss: 0.0674\n",
      "Epoch 383/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0156 - val_loss: 0.0674\n",
      "Epoch 384/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0156 - val_loss: 0.0674\n",
      "Epoch 385/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0156 - val_loss: 0.0675\n",
      "Epoch 386/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0156 - val_loss: 0.0675\n",
      "Epoch 387/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0156 - val_loss: 0.0675\n",
      "Epoch 388/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0156 - val_loss: 0.0675\n",
      "Epoch 389/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0156 - val_loss: 0.0675\n",
      "Epoch 390/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0156 - val_loss: 0.0675\n",
      "Epoch 391/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0155 - val_loss: 0.0675\n",
      "Epoch 392/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0155 - val_loss: 0.0675\n",
      "Epoch 393/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0155 - val_loss: 0.0676\n",
      "Epoch 394/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0155 - val_loss: 0.0676\n",
      "Epoch 395/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0155 - val_loss: 0.0676\n",
      "Epoch 396/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0155 - val_loss: 0.0676\n",
      "Epoch 397/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0155 - val_loss: 0.0676\n",
      "Epoch 398/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0155 - val_loss: 0.0676\n",
      "Epoch 399/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0155 - val_loss: 0.0676\n",
      "Epoch 400/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0155 - val_loss: 0.0676\n",
      "Epoch 401/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0155 - val_loss: 0.0676\n",
      "Epoch 402/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0155 - val_loss: 0.0677\n",
      "Epoch 403/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0154 - val_loss: 0.0677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 404/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0154 - val_loss: 0.0677\n",
      "Epoch 405/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0154 - val_loss: 0.0677\n",
      "Epoch 406/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0154 - val_loss: 0.0677\n",
      "Epoch 407/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0154 - val_loss: 0.0677\n",
      "Epoch 408/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0154 - val_loss: 0.0677\n",
      "Epoch 409/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0154 - val_loss: 0.0677\n",
      "Epoch 410/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0154 - val_loss: 0.0677\n",
      "Epoch 411/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0154 - val_loss: 0.0678\n",
      "Epoch 412/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0154 - val_loss: 0.0678\n",
      "Epoch 413/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0154 - val_loss: 0.0678\n",
      "Epoch 414/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0154 - val_loss: 0.0678\n",
      "Epoch 415/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0154 - val_loss: 0.0678\n",
      "Epoch 416/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0154 - val_loss: 0.0678\n",
      "Epoch 417/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0153 - val_loss: 0.0678\n",
      "Epoch 418/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0153 - val_loss: 0.0678\n",
      "Epoch 419/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0153 - val_loss: 0.0678\n",
      "Epoch 420/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0153 - val_loss: 0.0679\n",
      "Epoch 421/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0153 - val_loss: 0.0679\n",
      "Epoch 422/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0153 - val_loss: 0.0679\n",
      "Epoch 423/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0153 - val_loss: 0.0679\n",
      "Epoch 424/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0153 - val_loss: 0.0679\n",
      "Epoch 425/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0153 - val_loss: 0.0679\n",
      "Epoch 426/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0153 - val_loss: 0.0679\n",
      "Epoch 427/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0153 - val_loss: 0.0679\n",
      "Epoch 428/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0153 - val_loss: 0.0679\n",
      "Epoch 429/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0153 - val_loss: 0.0680\n",
      "Epoch 430/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0153 - val_loss: 0.0680\n",
      "Epoch 431/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0153 - val_loss: 0.0680\n",
      "Epoch 432/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0153 - val_loss: 0.0680\n",
      "Epoch 433/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0152 - val_loss: 0.0680\n",
      "Epoch 434/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0152 - val_loss: 0.0680\n",
      "Epoch 435/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0152 - val_loss: 0.0680\n",
      "Epoch 436/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0152 - val_loss: 0.0680\n",
      "Epoch 437/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0152 - val_loss: 0.0680\n",
      "Epoch 438/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0152 - val_loss: 0.0681\n",
      "Epoch 439/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0152 - val_loss: 0.0681\n",
      "Epoch 440/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0152 - val_loss: 0.0681\n",
      "Epoch 441/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0152 - val_loss: 0.0681\n",
      "Epoch 442/500\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0152 - val_loss: 0.0681\n",
      "Epoch 443/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0152 - val_loss: 0.0681\n",
      "Epoch 444/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0152 - val_loss: 0.0681\n",
      "Epoch 445/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0152 - val_loss: 0.0681\n",
      "Epoch 446/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0152 - val_loss: 0.0681\n",
      "Epoch 447/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0152 - val_loss: 0.0681\n",
      "Epoch 448/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0152 - val_loss: 0.0682\n",
      "Epoch 449/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0151 - val_loss: 0.0682\n",
      "Epoch 450/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0151 - val_loss: 0.0682\n",
      "Epoch 451/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0151 - val_loss: 0.0682\n",
      "Epoch 452/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0151 - val_loss: 0.0682\n",
      "Epoch 453/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0151 - val_loss: 0.0682\n",
      "Epoch 454/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0151 - val_loss: 0.0682\n",
      "Epoch 455/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0151 - val_loss: 0.0682\n",
      "Epoch 456/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0151 - val_loss: 0.0682\n",
      "Epoch 457/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0151 - val_loss: 0.0683\n",
      "Epoch 458/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0151 - val_loss: 0.0683\n",
      "Epoch 459/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0151 - val_loss: 0.0683\n",
      "Epoch 460/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0151 - val_loss: 0.0683\n",
      "Epoch 461/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0151 - val_loss: 0.0683\n",
      "Epoch 462/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0151 - val_loss: 0.0683\n",
      "Epoch 463/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0151 - val_loss: 0.0683\n",
      "Epoch 464/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0151 - val_loss: 0.0683\n",
      "Epoch 465/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0151 - val_loss: 0.0683\n",
      "Epoch 466/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0151 - val_loss: 0.0684\n",
      "Epoch 467/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0151 - val_loss: 0.0684\n",
      "Epoch 468/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0150 - val_loss: 0.0684\n",
      "Epoch 469/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0150 - val_loss: 0.0684\n",
      "Epoch 470/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0150 - val_loss: 0.0684\n",
      "Epoch 471/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0150 - val_loss: 0.0684\n",
      "Epoch 472/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0150 - val_loss: 0.0684\n",
      "Epoch 473/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0150 - val_loss: 0.0684\n",
      "Epoch 474/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0150 - val_loss: 0.0685\n",
      "Epoch 475/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0150 - val_loss: 0.0685\n",
      "Epoch 476/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0150 - val_loss: 0.0685\n",
      "Epoch 477/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0150 - val_loss: 0.0685\n",
      "Epoch 478/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0150 - val_loss: 0.0685\n",
      "Epoch 479/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0150 - val_loss: 0.0685\n",
      "Epoch 480/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0150 - val_loss: 0.0685\n",
      "Epoch 481/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0150 - val_loss: 0.0685\n",
      "Epoch 482/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0150 - val_loss: 0.0685\n",
      "Epoch 483/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0150 - val_loss: 0.0686\n",
      "Epoch 484/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0150 - val_loss: 0.0686\n",
      "Epoch 485/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0150 - val_loss: 0.0686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 486/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0150 - val_loss: 0.0686\n",
      "Epoch 487/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0150 - val_loss: 0.0686\n",
      "Epoch 488/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0150 - val_loss: 0.0686\n",
      "Epoch 489/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0149 - val_loss: 0.0686\n",
      "Epoch 490/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0149 - val_loss: 0.0686\n",
      "Epoch 491/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0149 - val_loss: 0.0687\n",
      "Epoch 492/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0149 - val_loss: 0.0687\n",
      "Epoch 493/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0149 - val_loss: 0.0687\n",
      "Epoch 494/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0149 - val_loss: 0.0687\n",
      "Epoch 495/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0149 - val_loss: 0.0687\n",
      "Epoch 496/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0149 - val_loss: 0.0687\n",
      "Epoch 497/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0149 - val_loss: 0.0687\n",
      "Epoch 498/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0149 - val_loss: 0.0687\n",
      "Epoch 499/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0149 - val_loss: 0.0687\n",
      "Epoch 500/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0149 - val_loss: 0.0688\n"
     ]
    }
   ],
   "source": [
    "# calculated log returns (i.e. the log of the difference between the price x+1 and price x)\n",
    "# windows of train.shape[1] consecutive returns will be produced. \n",
    "# Can be normalized with a MinMaxScaler to the range [0,1]??\n",
    "\n",
    "window_length = training_set_scaled.shape[1]\n",
    "encoding_dim = 3\n",
    "epochs = 500\n",
    "\n",
    "# compress the input to a 3-dimensional latent space. \n",
    "\n",
    "# input placeholder\n",
    "input_window = Input(shape=(window_length,))\n",
    "# encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation='tanh')(input_window) #tanh, linear, leakyrelu\n",
    "# lossy reconstruction of the input\n",
    "decoded = Dense(window_length, activation='linear')(encoded) #linear\n",
    "\n",
    "# model mapping an input to its reconstruction\n",
    "autoencoder = Model(input_window, decoded)\n",
    "\n",
    "# model mapping an input to its encoded representation\n",
    "encoder = Model(input_window, encoded)\n",
    "\n",
    "\n",
    "autoencoder.summary()\n",
    "autoencoder.compile(optimizer='adam', loss='MeanSquaredError') #MSE\n",
    "history = autoencoder.fit(training_set_scaled, training_set_scaled,\n",
    "                epochs=epochs,\n",
    "                batch_size=1024,\n",
    "                shuffle=True,\n",
    "                validation_data=(test_set_scaled, test_set_scaled))\n",
    "\n",
    "decoded_stocks = autoencoder.predict(test_set_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.973396</td>\n",
       "      <td>-0.994917</td>\n",
       "      <td>0.954638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.973760</td>\n",
       "      <td>-0.995692</td>\n",
       "      <td>0.974987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.946491</td>\n",
       "      <td>-0.989434</td>\n",
       "      <td>0.991947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966054</td>\n",
       "      <td>-0.997201</td>\n",
       "      <td>0.991587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.988301</td>\n",
       "      <td>-0.994133</td>\n",
       "      <td>0.969510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.985771</td>\n",
       "      <td>-0.994370</td>\n",
       "      <td>0.933277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.969154</td>\n",
       "      <td>-0.995851</td>\n",
       "      <td>0.922576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.978213</td>\n",
       "      <td>-0.995519</td>\n",
       "      <td>0.984633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.993069</td>\n",
       "      <td>-0.998975</td>\n",
       "      <td>0.992521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.987174</td>\n",
       "      <td>-0.995643</td>\n",
       "      <td>0.966206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.975594</td>\n",
       "      <td>-0.983169</td>\n",
       "      <td>0.949010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.987871</td>\n",
       "      <td>-0.994822</td>\n",
       "      <td>0.994933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.970285</td>\n",
       "      <td>-0.957974</td>\n",
       "      <td>0.808244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.982606</td>\n",
       "      <td>-0.997239</td>\n",
       "      <td>0.982072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.975766</td>\n",
       "      <td>-0.997343</td>\n",
       "      <td>0.990176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.989285</td>\n",
       "      <td>-0.996678</td>\n",
       "      <td>0.988734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.950885</td>\n",
       "      <td>-0.988896</td>\n",
       "      <td>0.980361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.958609</td>\n",
       "      <td>-0.991722</td>\n",
       "      <td>0.976634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.967546</td>\n",
       "      <td>-0.994983</td>\n",
       "      <td>0.979159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.920514</td>\n",
       "      <td>-0.993573</td>\n",
       "      <td>0.957747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.971728</td>\n",
       "      <td>-0.990745</td>\n",
       "      <td>0.935784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.995491</td>\n",
       "      <td>-0.998671</td>\n",
       "      <td>0.995512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.966032</td>\n",
       "      <td>-0.994892</td>\n",
       "      <td>0.973307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.974627</td>\n",
       "      <td>-0.992072</td>\n",
       "      <td>0.968529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.637740</td>\n",
       "      <td>-0.873456</td>\n",
       "      <td>0.484308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.978671</td>\n",
       "      <td>-0.997876</td>\n",
       "      <td>0.986667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.969391</td>\n",
       "      <td>-0.985933</td>\n",
       "      <td>0.935367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.996404</td>\n",
       "      <td>-0.998640</td>\n",
       "      <td>0.997598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.962837</td>\n",
       "      <td>-0.995759</td>\n",
       "      <td>0.967801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.664027</td>\n",
       "      <td>-0.906772</td>\n",
       "      <td>0.560534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-0.972770</td>\n",
       "      <td>-0.987024</td>\n",
       "      <td>0.956580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-0.965999</td>\n",
       "      <td>-0.977438</td>\n",
       "      <td>0.893826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-0.964661</td>\n",
       "      <td>-0.995264</td>\n",
       "      <td>0.968593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>-0.990555</td>\n",
       "      <td>-0.995932</td>\n",
       "      <td>0.987965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-0.938194</td>\n",
       "      <td>-0.970288</td>\n",
       "      <td>0.980815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-0.985598</td>\n",
       "      <td>-0.991799</td>\n",
       "      <td>0.871359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-0.983888</td>\n",
       "      <td>-0.993606</td>\n",
       "      <td>0.987100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-0.942883</td>\n",
       "      <td>-0.995197</td>\n",
       "      <td>0.946110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-0.984299</td>\n",
       "      <td>-0.986890</td>\n",
       "      <td>0.957872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-0.864363</td>\n",
       "      <td>-0.992729</td>\n",
       "      <td>0.966260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2\n",
       "0  -0.973396 -0.994917  0.954638\n",
       "1  -0.973760 -0.995692  0.974987\n",
       "2  -0.946491 -0.989434  0.991947\n",
       "3  -0.966054 -0.997201  0.991587\n",
       "4  -0.988301 -0.994133  0.969510\n",
       "5  -0.985771 -0.994370  0.933277\n",
       "6  -0.969154 -0.995851  0.922576\n",
       "7  -0.978213 -0.995519  0.984633\n",
       "8  -0.993069 -0.998975  0.992521\n",
       "9  -0.987174 -0.995643  0.966206\n",
       "10 -0.975594 -0.983169  0.949010\n",
       "11 -0.987871 -0.994822  0.994933\n",
       "12 -0.970285 -0.957974  0.808244\n",
       "13 -0.982606 -0.997239  0.982072\n",
       "14 -0.975766 -0.997343  0.990176\n",
       "15 -0.989285 -0.996678  0.988734\n",
       "16 -0.950885 -0.988896  0.980361\n",
       "17 -0.958609 -0.991722  0.976634\n",
       "18 -0.967546 -0.994983  0.979159\n",
       "19 -0.920514 -0.993573  0.957747\n",
       "20 -0.971728 -0.990745  0.935784\n",
       "21 -0.995491 -0.998671  0.995512\n",
       "22 -0.966032 -0.994892  0.973307\n",
       "23 -0.974627 -0.992072  0.968529\n",
       "24 -0.637740 -0.873456  0.484308\n",
       "25 -0.978671 -0.997876  0.986667\n",
       "26 -0.969391 -0.985933  0.935367\n",
       "27 -0.996404 -0.998640  0.997598\n",
       "28 -0.962837 -0.995759  0.967801\n",
       "29 -0.664027 -0.906772  0.560534\n",
       "30 -0.972770 -0.987024  0.956580\n",
       "31 -0.965999 -0.977438  0.893826\n",
       "32 -0.964661 -0.995264  0.968593\n",
       "33 -0.990555 -0.995932  0.987965\n",
       "34 -0.938194 -0.970288  0.980815\n",
       "35 -0.985598 -0.991799  0.871359\n",
       "36 -0.983888 -0.993606  0.987100\n",
       "37 -0.942883 -0.995197  0.946110\n",
       "38 -0.984299 -0.986890  0.957872\n",
       "39 -0.864363 -0.992729  0.966260"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(encoder.predict(test_set_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>640</th>\n",
       "      <th>641</th>\n",
       "      <th>642</th>\n",
       "      <th>643</th>\n",
       "      <th>644</th>\n",
       "      <th>645</th>\n",
       "      <th>646</th>\n",
       "      <th>647</th>\n",
       "      <th>648</th>\n",
       "      <th>649</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.584387</td>\n",
       "      <td>0.519720</td>\n",
       "      <td>0.514185</td>\n",
       "      <td>0.852923</td>\n",
       "      <td>0.592050</td>\n",
       "      <td>0.505877</td>\n",
       "      <td>0.478304</td>\n",
       "      <td>0.648814</td>\n",
       "      <td>0.515782</td>\n",
       "      <td>0.552013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.325500</td>\n",
       "      <td>0.565488</td>\n",
       "      <td>0.551077</td>\n",
       "      <td>0.379516</td>\n",
       "      <td>0.573362</td>\n",
       "      <td>0.409396</td>\n",
       "      <td>0.568800</td>\n",
       "      <td>0.564909</td>\n",
       "      <td>0.301514</td>\n",
       "      <td>0.716271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.589020</td>\n",
       "      <td>0.523963</td>\n",
       "      <td>0.516835</td>\n",
       "      <td>0.855804</td>\n",
       "      <td>0.595128</td>\n",
       "      <td>0.507146</td>\n",
       "      <td>0.480875</td>\n",
       "      <td>0.652000</td>\n",
       "      <td>0.518556</td>\n",
       "      <td>0.554319</td>\n",
       "      <td>...</td>\n",
       "      <td>0.326663</td>\n",
       "      <td>0.569737</td>\n",
       "      <td>0.554971</td>\n",
       "      <td>0.380613</td>\n",
       "      <td>0.577312</td>\n",
       "      <td>0.412271</td>\n",
       "      <td>0.571684</td>\n",
       "      <td>0.567651</td>\n",
       "      <td>0.302680</td>\n",
       "      <td>0.720313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.588418</td>\n",
       "      <td>0.524209</td>\n",
       "      <td>0.515085</td>\n",
       "      <td>0.849054</td>\n",
       "      <td>0.592086</td>\n",
       "      <td>0.502879</td>\n",
       "      <td>0.477471</td>\n",
       "      <td>0.648279</td>\n",
       "      <td>0.517610</td>\n",
       "      <td>0.550443</td>\n",
       "      <td>...</td>\n",
       "      <td>0.323372</td>\n",
       "      <td>0.569594</td>\n",
       "      <td>0.554306</td>\n",
       "      <td>0.379000</td>\n",
       "      <td>0.575308</td>\n",
       "      <td>0.412399</td>\n",
       "      <td>0.568917</td>\n",
       "      <td>0.563756</td>\n",
       "      <td>0.300560</td>\n",
       "      <td>0.718492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.591835</td>\n",
       "      <td>0.526922</td>\n",
       "      <td>0.518407</td>\n",
       "      <td>0.856337</td>\n",
       "      <td>0.596377</td>\n",
       "      <td>0.507193</td>\n",
       "      <td>0.481689</td>\n",
       "      <td>0.653296</td>\n",
       "      <td>0.520348</td>\n",
       "      <td>0.554964</td>\n",
       "      <td>...</td>\n",
       "      <td>0.326584</td>\n",
       "      <td>0.572517</td>\n",
       "      <td>0.557279</td>\n",
       "      <td>0.381299</td>\n",
       "      <td>0.579456</td>\n",
       "      <td>0.414353</td>\n",
       "      <td>0.573057</td>\n",
       "      <td>0.568574</td>\n",
       "      <td>0.303023</td>\n",
       "      <td>0.722642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.589567</td>\n",
       "      <td>0.523781</td>\n",
       "      <td>0.517251</td>\n",
       "      <td>0.858426</td>\n",
       "      <td>0.596639</td>\n",
       "      <td>0.508664</td>\n",
       "      <td>0.482558</td>\n",
       "      <td>0.653573</td>\n",
       "      <td>0.518714</td>\n",
       "      <td>0.555997</td>\n",
       "      <td>...</td>\n",
       "      <td>0.328247</td>\n",
       "      <td>0.569884</td>\n",
       "      <td>0.555534</td>\n",
       "      <td>0.380746</td>\n",
       "      <td>0.578260</td>\n",
       "      <td>0.412013</td>\n",
       "      <td>0.572747</td>\n",
       "      <td>0.569360</td>\n",
       "      <td>0.303507</td>\n",
       "      <td>0.721045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.581252</td>\n",
       "      <td>0.516330</td>\n",
       "      <td>0.512667</td>\n",
       "      <td>0.853280</td>\n",
       "      <td>0.591042</td>\n",
       "      <td>0.506442</td>\n",
       "      <td>0.477839</td>\n",
       "      <td>0.647864</td>\n",
       "      <td>0.513896</td>\n",
       "      <td>0.551821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.326038</td>\n",
       "      <td>0.562343</td>\n",
       "      <td>0.548528</td>\n",
       "      <td>0.379000</td>\n",
       "      <td>0.571200</td>\n",
       "      <td>0.407026</td>\n",
       "      <td>0.567636</td>\n",
       "      <td>0.564403</td>\n",
       "      <td>0.301427</td>\n",
       "      <td>0.713883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.576845</td>\n",
       "      <td>0.513080</td>\n",
       "      <td>0.510078</td>\n",
       "      <td>0.848062</td>\n",
       "      <td>0.586846</td>\n",
       "      <td>0.503761</td>\n",
       "      <td>0.473865</td>\n",
       "      <td>0.643533</td>\n",
       "      <td>0.511484</td>\n",
       "      <td>0.548087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.323325</td>\n",
       "      <td>0.558710</td>\n",
       "      <td>0.544707</td>\n",
       "      <td>0.378011</td>\n",
       "      <td>0.566925</td>\n",
       "      <td>0.404987</td>\n",
       "      <td>0.564123</td>\n",
       "      <td>0.560278</td>\n",
       "      <td>0.299571</td>\n",
       "      <td>0.709785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.591724</td>\n",
       "      <td>0.526232</td>\n",
       "      <td>0.518394</td>\n",
       "      <td>0.858120</td>\n",
       "      <td>0.597252</td>\n",
       "      <td>0.508264</td>\n",
       "      <td>0.482771</td>\n",
       "      <td>0.654192</td>\n",
       "      <td>0.520112</td>\n",
       "      <td>0.556061</td>\n",
       "      <td>...</td>\n",
       "      <td>0.327758</td>\n",
       "      <td>0.572109</td>\n",
       "      <td>0.557274</td>\n",
       "      <td>0.381232</td>\n",
       "      <td>0.579749</td>\n",
       "      <td>0.413764</td>\n",
       "      <td>0.573563</td>\n",
       "      <td>0.569642</td>\n",
       "      <td>0.303552</td>\n",
       "      <td>0.722734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.595842</td>\n",
       "      <td>0.529550</td>\n",
       "      <td>0.521431</td>\n",
       "      <td>0.863967</td>\n",
       "      <td>0.601370</td>\n",
       "      <td>0.511496</td>\n",
       "      <td>0.486667</td>\n",
       "      <td>0.658729</td>\n",
       "      <td>0.522839</td>\n",
       "      <td>0.559966</td>\n",
       "      <td>...</td>\n",
       "      <td>0.330444</td>\n",
       "      <td>0.575653</td>\n",
       "      <td>0.560821</td>\n",
       "      <td>0.382909</td>\n",
       "      <td>0.584021</td>\n",
       "      <td>0.416016</td>\n",
       "      <td>0.577345</td>\n",
       "      <td>0.573916</td>\n",
       "      <td>0.305602</td>\n",
       "      <td>0.726972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.588861</td>\n",
       "      <td>0.523264</td>\n",
       "      <td>0.517037</td>\n",
       "      <td>0.858158</td>\n",
       "      <td>0.596164</td>\n",
       "      <td>0.508619</td>\n",
       "      <td>0.482142</td>\n",
       "      <td>0.653171</td>\n",
       "      <td>0.518451</td>\n",
       "      <td>0.555694</td>\n",
       "      <td>...</td>\n",
       "      <td>0.328032</td>\n",
       "      <td>0.569304</td>\n",
       "      <td>0.554927</td>\n",
       "      <td>0.380817</td>\n",
       "      <td>0.577717</td>\n",
       "      <td>0.411719</td>\n",
       "      <td>0.572423</td>\n",
       "      <td>0.568992</td>\n",
       "      <td>0.303388</td>\n",
       "      <td>0.720543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.582087</td>\n",
       "      <td>0.516920</td>\n",
       "      <td>0.511432</td>\n",
       "      <td>0.849361</td>\n",
       "      <td>0.590127</td>\n",
       "      <td>0.503631</td>\n",
       "      <td>0.476680</td>\n",
       "      <td>0.646160</td>\n",
       "      <td>0.513294</td>\n",
       "      <td>0.549978</td>\n",
       "      <td>...</td>\n",
       "      <td>0.324654</td>\n",
       "      <td>0.563012</td>\n",
       "      <td>0.549205</td>\n",
       "      <td>0.377217</td>\n",
       "      <td>0.570780</td>\n",
       "      <td>0.407129</td>\n",
       "      <td>0.566245</td>\n",
       "      <td>0.562649</td>\n",
       "      <td>0.300238</td>\n",
       "      <td>0.713326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.595206</td>\n",
       "      <td>0.528967</td>\n",
       "      <td>0.520424</td>\n",
       "      <td>0.861715</td>\n",
       "      <td>0.600296</td>\n",
       "      <td>0.510068</td>\n",
       "      <td>0.485584</td>\n",
       "      <td>0.657337</td>\n",
       "      <td>0.522066</td>\n",
       "      <td>0.558682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.329559</td>\n",
       "      <td>0.575067</td>\n",
       "      <td>0.560268</td>\n",
       "      <td>0.382025</td>\n",
       "      <td>0.583015</td>\n",
       "      <td>0.415523</td>\n",
       "      <td>0.576174</td>\n",
       "      <td>0.572578</td>\n",
       "      <td>0.304860</td>\n",
       "      <td>0.725917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.547385</td>\n",
       "      <td>0.484234</td>\n",
       "      <td>0.488984</td>\n",
       "      <td>0.822930</td>\n",
       "      <td>0.565839</td>\n",
       "      <td>0.490612</td>\n",
       "      <td>0.456163</td>\n",
       "      <td>0.619840</td>\n",
       "      <td>0.490627</td>\n",
       "      <td>0.530471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.314790</td>\n",
       "      <td>0.530702</td>\n",
       "      <td>0.520104</td>\n",
       "      <td>0.365885</td>\n",
       "      <td>0.539871</td>\n",
       "      <td>0.384389</td>\n",
       "      <td>0.542313</td>\n",
       "      <td>0.539868</td>\n",
       "      <td>0.289939</td>\n",
       "      <td>0.681268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.591941</td>\n",
       "      <td>0.526339</td>\n",
       "      <td>0.518834</td>\n",
       "      <td>0.859437</td>\n",
       "      <td>0.597857</td>\n",
       "      <td>0.509098</td>\n",
       "      <td>0.483420</td>\n",
       "      <td>0.654952</td>\n",
       "      <td>0.520403</td>\n",
       "      <td>0.556814</td>\n",
       "      <td>...</td>\n",
       "      <td>0.328349</td>\n",
       "      <td>0.572262</td>\n",
       "      <td>0.557479</td>\n",
       "      <td>0.381629</td>\n",
       "      <td>0.580217</td>\n",
       "      <td>0.413864</td>\n",
       "      <td>0.574160</td>\n",
       "      <td>0.570410</td>\n",
       "      <td>0.303974</td>\n",
       "      <td>0.723192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.592833</td>\n",
       "      <td>0.527437</td>\n",
       "      <td>0.519186</td>\n",
       "      <td>0.858716</td>\n",
       "      <td>0.597859</td>\n",
       "      <td>0.508552</td>\n",
       "      <td>0.483215</td>\n",
       "      <td>0.654900</td>\n",
       "      <td>0.520937</td>\n",
       "      <td>0.556503</td>\n",
       "      <td>...</td>\n",
       "      <td>0.327845</td>\n",
       "      <td>0.573226</td>\n",
       "      <td>0.558182</td>\n",
       "      <td>0.381707</td>\n",
       "      <td>0.580694</td>\n",
       "      <td>0.414647</td>\n",
       "      <td>0.574278</td>\n",
       "      <td>0.570193</td>\n",
       "      <td>0.303810</td>\n",
       "      <td>0.723776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.594238</td>\n",
       "      <td>0.528125</td>\n",
       "      <td>0.520167</td>\n",
       "      <td>0.861841</td>\n",
       "      <td>0.599886</td>\n",
       "      <td>0.510306</td>\n",
       "      <td>0.485302</td>\n",
       "      <td>0.657044</td>\n",
       "      <td>0.521682</td>\n",
       "      <td>0.558566</td>\n",
       "      <td>...</td>\n",
       "      <td>0.329565</td>\n",
       "      <td>0.574203</td>\n",
       "      <td>0.559456</td>\n",
       "      <td>0.382142</td>\n",
       "      <td>0.582377</td>\n",
       "      <td>0.415008</td>\n",
       "      <td>0.575890</td>\n",
       "      <td>0.572369</td>\n",
       "      <td>0.304846</td>\n",
       "      <td>0.725292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.586382</td>\n",
       "      <td>0.522146</td>\n",
       "      <td>0.513994</td>\n",
       "      <td>0.848577</td>\n",
       "      <td>0.591111</td>\n",
       "      <td>0.502803</td>\n",
       "      <td>0.476791</td>\n",
       "      <td>0.647291</td>\n",
       "      <td>0.516369</td>\n",
       "      <td>0.549904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.323330</td>\n",
       "      <td>0.567624</td>\n",
       "      <td>0.552626</td>\n",
       "      <td>0.378569</td>\n",
       "      <td>0.573744</td>\n",
       "      <td>0.410969</td>\n",
       "      <td>0.567912</td>\n",
       "      <td>0.563018</td>\n",
       "      <td>0.300287</td>\n",
       "      <td>0.716824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.586911</td>\n",
       "      <td>0.522459</td>\n",
       "      <td>0.514825</td>\n",
       "      <td>0.850931</td>\n",
       "      <td>0.592261</td>\n",
       "      <td>0.504272</td>\n",
       "      <td>0.478003</td>\n",
       "      <td>0.648703</td>\n",
       "      <td>0.516949</td>\n",
       "      <td>0.551278</td>\n",
       "      <td>...</td>\n",
       "      <td>0.324395</td>\n",
       "      <td>0.568022</td>\n",
       "      <td>0.553112</td>\n",
       "      <td>0.379268</td>\n",
       "      <td>0.574679</td>\n",
       "      <td>0.411223</td>\n",
       "      <td>0.569029</td>\n",
       "      <td>0.564432</td>\n",
       "      <td>0.301049</td>\n",
       "      <td>0.717740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.589033</td>\n",
       "      <td>0.524194</td>\n",
       "      <td>0.516613</td>\n",
       "      <td>0.854513</td>\n",
       "      <td>0.594570</td>\n",
       "      <td>0.506328</td>\n",
       "      <td>0.480217</td>\n",
       "      <td>0.651333</td>\n",
       "      <td>0.518498</td>\n",
       "      <td>0.553579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.325981</td>\n",
       "      <td>0.569862</td>\n",
       "      <td>0.554943</td>\n",
       "      <td>0.380390</td>\n",
       "      <td>0.577026</td>\n",
       "      <td>0.412440</td>\n",
       "      <td>0.571224</td>\n",
       "      <td>0.566921</td>\n",
       "      <td>0.302285</td>\n",
       "      <td>0.720090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.577878</td>\n",
       "      <td>0.515902</td>\n",
       "      <td>0.509276</td>\n",
       "      <td>0.839229</td>\n",
       "      <td>0.583255</td>\n",
       "      <td>0.498122</td>\n",
       "      <td>0.469389</td>\n",
       "      <td>0.639305</td>\n",
       "      <td>0.511892</td>\n",
       "      <td>0.543068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.318356</td>\n",
       "      <td>0.560628</td>\n",
       "      <td>0.545257</td>\n",
       "      <td>0.376979</td>\n",
       "      <td>0.565681</td>\n",
       "      <td>0.407100</td>\n",
       "      <td>0.561444</td>\n",
       "      <td>0.555425</td>\n",
       "      <td>0.296933</td>\n",
       "      <td>0.709126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.579520</td>\n",
       "      <td>0.515141</td>\n",
       "      <td>0.510932</td>\n",
       "      <td>0.848903</td>\n",
       "      <td>0.588532</td>\n",
       "      <td>0.503840</td>\n",
       "      <td>0.475300</td>\n",
       "      <td>0.644962</td>\n",
       "      <td>0.512541</td>\n",
       "      <td>0.549113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.323991</td>\n",
       "      <td>0.560958</td>\n",
       "      <td>0.546992</td>\n",
       "      <td>0.377809</td>\n",
       "      <td>0.568950</td>\n",
       "      <td>0.406195</td>\n",
       "      <td>0.565314</td>\n",
       "      <td>0.561551</td>\n",
       "      <td>0.299971</td>\n",
       "      <td>0.711693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.596791</td>\n",
       "      <td>0.530298</td>\n",
       "      <td>0.521965</td>\n",
       "      <td>0.864882</td>\n",
       "      <td>0.602175</td>\n",
       "      <td>0.511945</td>\n",
       "      <td>0.487406</td>\n",
       "      <td>0.659552</td>\n",
       "      <td>0.523360</td>\n",
       "      <td>0.560646</td>\n",
       "      <td>...</td>\n",
       "      <td>0.330907</td>\n",
       "      <td>0.576460</td>\n",
       "      <td>0.561635</td>\n",
       "      <td>0.383104</td>\n",
       "      <td>0.584895</td>\n",
       "      <td>0.416495</td>\n",
       "      <td>0.578030</td>\n",
       "      <td>0.574682</td>\n",
       "      <td>0.305938</td>\n",
       "      <td>0.727823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.587528</td>\n",
       "      <td>0.522881</td>\n",
       "      <td>0.515742</td>\n",
       "      <td>0.853358</td>\n",
       "      <td>0.593460</td>\n",
       "      <td>0.505785</td>\n",
       "      <td>0.479249</td>\n",
       "      <td>0.650184</td>\n",
       "      <td>0.517614</td>\n",
       "      <td>0.552695</td>\n",
       "      <td>...</td>\n",
       "      <td>0.325464</td>\n",
       "      <td>0.568516</td>\n",
       "      <td>0.553668</td>\n",
       "      <td>0.380034</td>\n",
       "      <td>0.575697</td>\n",
       "      <td>0.411564</td>\n",
       "      <td>0.570218</td>\n",
       "      <td>0.565898</td>\n",
       "      <td>0.301840</td>\n",
       "      <td>0.718753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.587293</td>\n",
       "      <td>0.522166</td>\n",
       "      <td>0.515433</td>\n",
       "      <td>0.854161</td>\n",
       "      <td>0.593891</td>\n",
       "      <td>0.506234</td>\n",
       "      <td>0.479845</td>\n",
       "      <td>0.650527</td>\n",
       "      <td>0.517196</td>\n",
       "      <td>0.553231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.326183</td>\n",
       "      <td>0.568039</td>\n",
       "      <td>0.553539</td>\n",
       "      <td>0.379695</td>\n",
       "      <td>0.575669</td>\n",
       "      <td>0.410942</td>\n",
       "      <td>0.570299</td>\n",
       "      <td>0.566398</td>\n",
       "      <td>0.302058</td>\n",
       "      <td>0.718540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.421832</td>\n",
       "      <td>0.379209</td>\n",
       "      <td>0.401739</td>\n",
       "      <td>0.671429</td>\n",
       "      <td>0.451632</td>\n",
       "      <td>0.409143</td>\n",
       "      <td>0.350615</td>\n",
       "      <td>0.495415</td>\n",
       "      <td>0.409446</td>\n",
       "      <td>0.426569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.246497</td>\n",
       "      <td>0.420664</td>\n",
       "      <td>0.412681</td>\n",
       "      <td>0.320223</td>\n",
       "      <td>0.415788</td>\n",
       "      <td>0.313079</td>\n",
       "      <td>0.436861</td>\n",
       "      <td>0.424568</td>\n",
       "      <td>0.235676</td>\n",
       "      <td>0.557220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.592506</td>\n",
       "      <td>0.527048</td>\n",
       "      <td>0.519141</td>\n",
       "      <td>0.859182</td>\n",
       "      <td>0.597922</td>\n",
       "      <td>0.508892</td>\n",
       "      <td>0.483360</td>\n",
       "      <td>0.655022</td>\n",
       "      <td>0.520797</td>\n",
       "      <td>0.556718</td>\n",
       "      <td>...</td>\n",
       "      <td>0.328098</td>\n",
       "      <td>0.572879</td>\n",
       "      <td>0.557924</td>\n",
       "      <td>0.381776</td>\n",
       "      <td>0.580571</td>\n",
       "      <td>0.414385</td>\n",
       "      <td>0.574323</td>\n",
       "      <td>0.570373</td>\n",
       "      <td>0.303934</td>\n",
       "      <td>0.723623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.578564</td>\n",
       "      <td>0.514114</td>\n",
       "      <td>0.509726</td>\n",
       "      <td>0.846843</td>\n",
       "      <td>0.587460</td>\n",
       "      <td>0.502551</td>\n",
       "      <td>0.474300</td>\n",
       "      <td>0.643556</td>\n",
       "      <td>0.511516</td>\n",
       "      <td>0.547921</td>\n",
       "      <td>...</td>\n",
       "      <td>0.323301</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.546192</td>\n",
       "      <td>0.376805</td>\n",
       "      <td>0.567749</td>\n",
       "      <td>0.405367</td>\n",
       "      <td>0.564057</td>\n",
       "      <td>0.560270</td>\n",
       "      <td>0.299264</td>\n",
       "      <td>0.710386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.597370</td>\n",
       "      <td>0.530786</td>\n",
       "      <td>0.522299</td>\n",
       "      <td>0.865372</td>\n",
       "      <td>0.602627</td>\n",
       "      <td>0.512181</td>\n",
       "      <td>0.487809</td>\n",
       "      <td>0.660019</td>\n",
       "      <td>0.523693</td>\n",
       "      <td>0.561015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.331138</td>\n",
       "      <td>0.576969</td>\n",
       "      <td>0.562128</td>\n",
       "      <td>0.383236</td>\n",
       "      <td>0.585416</td>\n",
       "      <td>0.416816</td>\n",
       "      <td>0.578431</td>\n",
       "      <td>0.575104</td>\n",
       "      <td>0.306123</td>\n",
       "      <td>0.728341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.585986</td>\n",
       "      <td>0.521655</td>\n",
       "      <td>0.514940</td>\n",
       "      <td>0.852091</td>\n",
       "      <td>0.592233</td>\n",
       "      <td>0.505201</td>\n",
       "      <td>0.478142</td>\n",
       "      <td>0.648958</td>\n",
       "      <td>0.516802</td>\n",
       "      <td>0.551708</td>\n",
       "      <td>...</td>\n",
       "      <td>0.324806</td>\n",
       "      <td>0.567198</td>\n",
       "      <td>0.552347</td>\n",
       "      <td>0.379791</td>\n",
       "      <td>0.574330</td>\n",
       "      <td>0.410786</td>\n",
       "      <td>0.569191</td>\n",
       "      <td>0.564770</td>\n",
       "      <td>0.301362</td>\n",
       "      <td>0.717422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.445999</td>\n",
       "      <td>0.401871</td>\n",
       "      <td>0.419937</td>\n",
       "      <td>0.697536</td>\n",
       "      <td>0.471301</td>\n",
       "      <td>0.423382</td>\n",
       "      <td>0.368003</td>\n",
       "      <td>0.517710</td>\n",
       "      <td>0.426759</td>\n",
       "      <td>0.444200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.256459</td>\n",
       "      <td>0.443123</td>\n",
       "      <td>0.433038</td>\n",
       "      <td>0.331022</td>\n",
       "      <td>0.439224</td>\n",
       "      <td>0.329214</td>\n",
       "      <td>0.456707</td>\n",
       "      <td>0.444454</td>\n",
       "      <td>0.245260</td>\n",
       "      <td>0.581564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.583826</td>\n",
       "      <td>0.518822</td>\n",
       "      <td>0.512829</td>\n",
       "      <td>0.850705</td>\n",
       "      <td>0.591218</td>\n",
       "      <td>0.504363</td>\n",
       "      <td>0.477530</td>\n",
       "      <td>0.647481</td>\n",
       "      <td>0.514685</td>\n",
       "      <td>0.550882</td>\n",
       "      <td>...</td>\n",
       "      <td>0.324942</td>\n",
       "      <td>0.564769</td>\n",
       "      <td>0.550633</td>\n",
       "      <td>0.378138</td>\n",
       "      <td>0.572368</td>\n",
       "      <td>0.408546</td>\n",
       "      <td>0.567541</td>\n",
       "      <td>0.563723</td>\n",
       "      <td>0.300771</td>\n",
       "      <td>0.715076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.567959</td>\n",
       "      <td>0.504156</td>\n",
       "      <td>0.502723</td>\n",
       "      <td>0.838262</td>\n",
       "      <td>0.579844</td>\n",
       "      <td>0.498240</td>\n",
       "      <td>0.467810</td>\n",
       "      <td>0.635252</td>\n",
       "      <td>0.504512</td>\n",
       "      <td>0.541687</td>\n",
       "      <td>...</td>\n",
       "      <td>0.320064</td>\n",
       "      <td>0.550141</td>\n",
       "      <td>0.537289</td>\n",
       "      <td>0.373184</td>\n",
       "      <td>0.558181</td>\n",
       "      <td>0.398423</td>\n",
       "      <td>0.556543</td>\n",
       "      <td>0.553038</td>\n",
       "      <td>0.295960</td>\n",
       "      <td>0.700469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.586347</td>\n",
       "      <td>0.521882</td>\n",
       "      <td>0.515113</td>\n",
       "      <td>0.852518</td>\n",
       "      <td>0.592596</td>\n",
       "      <td>0.505411</td>\n",
       "      <td>0.478494</td>\n",
       "      <td>0.649314</td>\n",
       "      <td>0.516963</td>\n",
       "      <td>0.552026</td>\n",
       "      <td>...</td>\n",
       "      <td>0.325059</td>\n",
       "      <td>0.567475</td>\n",
       "      <td>0.552664</td>\n",
       "      <td>0.379821</td>\n",
       "      <td>0.574675</td>\n",
       "      <td>0.410912</td>\n",
       "      <td>0.569466</td>\n",
       "      <td>0.565120</td>\n",
       "      <td>0.301513</td>\n",
       "      <td>0.717737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.594151</td>\n",
       "      <td>0.527955</td>\n",
       "      <td>0.520050</td>\n",
       "      <td>0.861856</td>\n",
       "      <td>0.599899</td>\n",
       "      <td>0.510305</td>\n",
       "      <td>0.485345</td>\n",
       "      <td>0.657023</td>\n",
       "      <td>0.521558</td>\n",
       "      <td>0.558587</td>\n",
       "      <td>...</td>\n",
       "      <td>0.329644</td>\n",
       "      <td>0.574076</td>\n",
       "      <td>0.559395</td>\n",
       "      <td>0.382029</td>\n",
       "      <td>0.582309</td>\n",
       "      <td>0.414862</td>\n",
       "      <td>0.575834</td>\n",
       "      <td>0.572380</td>\n",
       "      <td>0.304842</td>\n",
       "      <td>0.725187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.582656</td>\n",
       "      <td>0.518301</td>\n",
       "      <td>0.509233</td>\n",
       "      <td>0.839926</td>\n",
       "      <td>0.586634</td>\n",
       "      <td>0.497396</td>\n",
       "      <td>0.472539</td>\n",
       "      <td>0.641486</td>\n",
       "      <td>0.512389</td>\n",
       "      <td>0.544886</td>\n",
       "      <td>...</td>\n",
       "      <td>0.320271</td>\n",
       "      <td>0.563971</td>\n",
       "      <td>0.549482</td>\n",
       "      <td>0.374615</td>\n",
       "      <td>0.568929</td>\n",
       "      <td>0.407870</td>\n",
       "      <td>0.562803</td>\n",
       "      <td>0.557660</td>\n",
       "      <td>0.297344</td>\n",
       "      <td>0.711641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.567253</td>\n",
       "      <td>0.503461</td>\n",
       "      <td>0.504649</td>\n",
       "      <td>0.844694</td>\n",
       "      <td>0.581811</td>\n",
       "      <td>0.502673</td>\n",
       "      <td>0.470155</td>\n",
       "      <td>0.638303</td>\n",
       "      <td>0.505492</td>\n",
       "      <td>0.544933</td>\n",
       "      <td>...</td>\n",
       "      <td>0.322612</td>\n",
       "      <td>0.549478</td>\n",
       "      <td>0.536772</td>\n",
       "      <td>0.375666</td>\n",
       "      <td>0.559291</td>\n",
       "      <td>0.398295</td>\n",
       "      <td>0.558955</td>\n",
       "      <td>0.556194</td>\n",
       "      <td>0.297936</td>\n",
       "      <td>0.701676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.592806</td>\n",
       "      <td>0.526882</td>\n",
       "      <td>0.518860</td>\n",
       "      <td>0.859343</td>\n",
       "      <td>0.598336</td>\n",
       "      <td>0.508845</td>\n",
       "      <td>0.483827</td>\n",
       "      <td>0.655230</td>\n",
       "      <td>0.520551</td>\n",
       "      <td>0.556993</td>\n",
       "      <td>...</td>\n",
       "      <td>0.328521</td>\n",
       "      <td>0.572924</td>\n",
       "      <td>0.558228</td>\n",
       "      <td>0.381258</td>\n",
       "      <td>0.580764</td>\n",
       "      <td>0.414107</td>\n",
       "      <td>0.574351</td>\n",
       "      <td>0.570668</td>\n",
       "      <td>0.303984</td>\n",
       "      <td>0.723646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.578472</td>\n",
       "      <td>0.515607</td>\n",
       "      <td>0.510307</td>\n",
       "      <td>0.844030</td>\n",
       "      <td>0.585630</td>\n",
       "      <td>0.501065</td>\n",
       "      <td>0.472052</td>\n",
       "      <td>0.642021</td>\n",
       "      <td>0.512383</td>\n",
       "      <td>0.545939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.320918</td>\n",
       "      <td>0.560739</td>\n",
       "      <td>0.545900</td>\n",
       "      <td>0.377768</td>\n",
       "      <td>0.567186</td>\n",
       "      <td>0.406821</td>\n",
       "      <td>0.563381</td>\n",
       "      <td>0.558321</td>\n",
       "      <td>0.298440</td>\n",
       "      <td>0.710394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.585631</td>\n",
       "      <td>0.519983</td>\n",
       "      <td>0.514055</td>\n",
       "      <td>0.853814</td>\n",
       "      <td>0.593364</td>\n",
       "      <td>0.506075</td>\n",
       "      <td>0.479660</td>\n",
       "      <td>0.649760</td>\n",
       "      <td>0.515717</td>\n",
       "      <td>0.552972</td>\n",
       "      <td>...</td>\n",
       "      <td>0.326571</td>\n",
       "      <td>0.566170</td>\n",
       "      <td>0.552228</td>\n",
       "      <td>0.378704</td>\n",
       "      <td>0.574339</td>\n",
       "      <td>0.409254</td>\n",
       "      <td>0.569329</td>\n",
       "      <td>0.565967</td>\n",
       "      <td>0.301829</td>\n",
       "      <td>0.716927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.572186</td>\n",
       "      <td>0.512987</td>\n",
       "      <td>0.504808</td>\n",
       "      <td>0.825515</td>\n",
       "      <td>0.574737</td>\n",
       "      <td>0.490275</td>\n",
       "      <td>0.460608</td>\n",
       "      <td>0.630079</td>\n",
       "      <td>0.508527</td>\n",
       "      <td>0.534203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.311082</td>\n",
       "      <td>0.556598</td>\n",
       "      <td>0.540099</td>\n",
       "      <td>0.374633</td>\n",
       "      <td>0.558582</td>\n",
       "      <td>0.405446</td>\n",
       "      <td>0.554427</td>\n",
       "      <td>0.546103</td>\n",
       "      <td>0.292399</td>\n",
       "      <td>0.702632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40 rows × 650 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6    \\\n",
       "0   0.584387  0.519720  0.514185  0.852923  0.592050  0.505877  0.478304   \n",
       "1   0.589020  0.523963  0.516835  0.855804  0.595128  0.507146  0.480875   \n",
       "2   0.588418  0.524209  0.515085  0.849054  0.592086  0.502879  0.477471   \n",
       "3   0.591835  0.526922  0.518407  0.856337  0.596377  0.507193  0.481689   \n",
       "4   0.589567  0.523781  0.517251  0.858426  0.596639  0.508664  0.482558   \n",
       "5   0.581252  0.516330  0.512667  0.853280  0.591042  0.506442  0.477839   \n",
       "6   0.576845  0.513080  0.510078  0.848062  0.586846  0.503761  0.473865   \n",
       "7   0.591724  0.526232  0.518394  0.858120  0.597252  0.508264  0.482771   \n",
       "8   0.595842  0.529550  0.521431  0.863967  0.601370  0.511496  0.486667   \n",
       "9   0.588861  0.523264  0.517037  0.858158  0.596164  0.508619  0.482142   \n",
       "10  0.582087  0.516920  0.511432  0.849361  0.590127  0.503631  0.476680   \n",
       "11  0.595206  0.528967  0.520424  0.861715  0.600296  0.510068  0.485584   \n",
       "12  0.547385  0.484234  0.488984  0.822930  0.565839  0.490612  0.456163   \n",
       "13  0.591941  0.526339  0.518834  0.859437  0.597857  0.509098  0.483420   \n",
       "14  0.592833  0.527437  0.519186  0.858716  0.597859  0.508552  0.483215   \n",
       "15  0.594238  0.528125  0.520167  0.861841  0.599886  0.510306  0.485302   \n",
       "16  0.586382  0.522146  0.513994  0.848577  0.591111  0.502803  0.476791   \n",
       "17  0.586911  0.522459  0.514825  0.850931  0.592261  0.504272  0.478003   \n",
       "18  0.589033  0.524194  0.516613  0.854513  0.594570  0.506328  0.480217   \n",
       "19  0.577878  0.515902  0.509276  0.839229  0.583255  0.498122  0.469389   \n",
       "20  0.579520  0.515141  0.510932  0.848903  0.588532  0.503840  0.475300   \n",
       "21  0.596791  0.530298  0.521965  0.864882  0.602175  0.511945  0.487406   \n",
       "22  0.587528  0.522881  0.515742  0.853358  0.593460  0.505785  0.479249   \n",
       "23  0.587293  0.522166  0.515433  0.854161  0.593891  0.506234  0.479845   \n",
       "24  0.421832  0.379209  0.401739  0.671429  0.451632  0.409143  0.350615   \n",
       "25  0.592506  0.527048  0.519141  0.859182  0.597922  0.508892  0.483360   \n",
       "26  0.578564  0.514114  0.509726  0.846843  0.587460  0.502551  0.474300   \n",
       "27  0.597370  0.530786  0.522299  0.865372  0.602627  0.512181  0.487809   \n",
       "28  0.585986  0.521655  0.514940  0.852091  0.592233  0.505201  0.478142   \n",
       "29  0.445999  0.401871  0.419937  0.697536  0.471301  0.423382  0.368003   \n",
       "30  0.583826  0.518822  0.512829  0.850705  0.591218  0.504363  0.477530   \n",
       "31  0.567959  0.504156  0.502723  0.838262  0.579844  0.498240  0.467810   \n",
       "32  0.586347  0.521882  0.515113  0.852518  0.592596  0.505411  0.478494   \n",
       "33  0.594151  0.527955  0.520050  0.861856  0.599899  0.510305  0.485345   \n",
       "34  0.582656  0.518301  0.509233  0.839926  0.586634  0.497396  0.472539   \n",
       "35  0.567253  0.503461  0.504649  0.844694  0.581811  0.502673  0.470155   \n",
       "36  0.592806  0.526882  0.518860  0.859343  0.598336  0.508845  0.483827   \n",
       "37  0.578472  0.515607  0.510307  0.844030  0.585630  0.501065  0.472052   \n",
       "38  0.585631  0.519983  0.514055  0.853814  0.593364  0.506075  0.479660   \n",
       "39  0.572186  0.512987  0.504808  0.825515  0.574737  0.490275  0.460608   \n",
       "\n",
       "         7         8         9    ...       640       641       642       643  \\\n",
       "0   0.648814  0.515782  0.552013  ...  0.325500  0.565488  0.551077  0.379516   \n",
       "1   0.652000  0.518556  0.554319  ...  0.326663  0.569737  0.554971  0.380613   \n",
       "2   0.648279  0.517610  0.550443  ...  0.323372  0.569594  0.554306  0.379000   \n",
       "3   0.653296  0.520348  0.554964  ...  0.326584  0.572517  0.557279  0.381299   \n",
       "4   0.653573  0.518714  0.555997  ...  0.328247  0.569884  0.555534  0.380746   \n",
       "5   0.647864  0.513896  0.551821  ...  0.326038  0.562343  0.548528  0.379000   \n",
       "6   0.643533  0.511484  0.548087  ...  0.323325  0.558710  0.544707  0.378011   \n",
       "7   0.654192  0.520112  0.556061  ...  0.327758  0.572109  0.557274  0.381232   \n",
       "8   0.658729  0.522839  0.559966  ...  0.330444  0.575653  0.560821  0.382909   \n",
       "9   0.653171  0.518451  0.555694  ...  0.328032  0.569304  0.554927  0.380817   \n",
       "10  0.646160  0.513294  0.549978  ...  0.324654  0.563012  0.549205  0.377217   \n",
       "11  0.657337  0.522066  0.558682  ...  0.329559  0.575067  0.560268  0.382025   \n",
       "12  0.619840  0.490627  0.530471  ...  0.314790  0.530702  0.520104  0.365885   \n",
       "13  0.654952  0.520403  0.556814  ...  0.328349  0.572262  0.557479  0.381629   \n",
       "14  0.654900  0.520937  0.556503  ...  0.327845  0.573226  0.558182  0.381707   \n",
       "15  0.657044  0.521682  0.558566  ...  0.329565  0.574203  0.559456  0.382142   \n",
       "16  0.647291  0.516369  0.549904  ...  0.323330  0.567624  0.552626  0.378569   \n",
       "17  0.648703  0.516949  0.551278  ...  0.324395  0.568022  0.553112  0.379268   \n",
       "18  0.651333  0.518498  0.553579  ...  0.325981  0.569862  0.554943  0.380390   \n",
       "19  0.639305  0.511892  0.543068  ...  0.318356  0.560628  0.545257  0.376979   \n",
       "20  0.644962  0.512541  0.549113  ...  0.323991  0.560958  0.546992  0.377809   \n",
       "21  0.659552  0.523360  0.560646  ...  0.330907  0.576460  0.561635  0.383104   \n",
       "22  0.650184  0.517614  0.552695  ...  0.325464  0.568516  0.553668  0.380034   \n",
       "23  0.650527  0.517196  0.553231  ...  0.326183  0.568039  0.553539  0.379695   \n",
       "24  0.495415  0.409446  0.426569  ...  0.246497  0.420664  0.412681  0.320223   \n",
       "25  0.655022  0.520797  0.556718  ...  0.328098  0.572879  0.557924  0.381776   \n",
       "26  0.643556  0.511516  0.547921  ...  0.323301  0.560000  0.546192  0.376805   \n",
       "27  0.660019  0.523693  0.561015  ...  0.331138  0.576969  0.562128  0.383236   \n",
       "28  0.648958  0.516802  0.551708  ...  0.324806  0.567198  0.552347  0.379791   \n",
       "29  0.517710  0.426759  0.444200  ...  0.256459  0.443123  0.433038  0.331022   \n",
       "30  0.647481  0.514685  0.550882  ...  0.324942  0.564769  0.550633  0.378138   \n",
       "31  0.635252  0.504512  0.541687  ...  0.320064  0.550141  0.537289  0.373184   \n",
       "32  0.649314  0.516963  0.552026  ...  0.325059  0.567475  0.552664  0.379821   \n",
       "33  0.657023  0.521558  0.558587  ...  0.329644  0.574076  0.559395  0.382029   \n",
       "34  0.641486  0.512389  0.544886  ...  0.320271  0.563971  0.549482  0.374615   \n",
       "35  0.638303  0.505492  0.544933  ...  0.322612  0.549478  0.536772  0.375666   \n",
       "36  0.655230  0.520551  0.556993  ...  0.328521  0.572924  0.558228  0.381258   \n",
       "37  0.642021  0.512383  0.545939  ...  0.320918  0.560739  0.545900  0.377768   \n",
       "38  0.649760  0.515717  0.552972  ...  0.326571  0.566170  0.552228  0.378704   \n",
       "39  0.630079  0.508527  0.534203  ...  0.311082  0.556598  0.540099  0.374633   \n",
       "\n",
       "         644       645       646       647       648       649  \n",
       "0   0.573362  0.409396  0.568800  0.564909  0.301514  0.716271  \n",
       "1   0.577312  0.412271  0.571684  0.567651  0.302680  0.720313  \n",
       "2   0.575308  0.412399  0.568917  0.563756  0.300560  0.718492  \n",
       "3   0.579456  0.414353  0.573057  0.568574  0.303023  0.722642  \n",
       "4   0.578260  0.412013  0.572747  0.569360  0.303507  0.721045  \n",
       "5   0.571200  0.407026  0.567636  0.564403  0.301427  0.713883  \n",
       "6   0.566925  0.404987  0.564123  0.560278  0.299571  0.709785  \n",
       "7   0.579749  0.413764  0.573563  0.569642  0.303552  0.722734  \n",
       "8   0.584021  0.416016  0.577345  0.573916  0.305602  0.726972  \n",
       "9   0.577717  0.411719  0.572423  0.568992  0.303388  0.720543  \n",
       "10  0.570780  0.407129  0.566245  0.562649  0.300238  0.713326  \n",
       "11  0.583015  0.415523  0.576174  0.572578  0.304860  0.725917  \n",
       "12  0.539871  0.384389  0.542313  0.539868  0.289939  0.681268  \n",
       "13  0.580217  0.413864  0.574160  0.570410  0.303974  0.723192  \n",
       "14  0.580694  0.414647  0.574278  0.570193  0.303810  0.723776  \n",
       "15  0.582377  0.415008  0.575890  0.572369  0.304846  0.725292  \n",
       "16  0.573744  0.410969  0.567912  0.563018  0.300287  0.716824  \n",
       "17  0.574679  0.411223  0.569029  0.564432  0.301049  0.717740  \n",
       "18  0.577026  0.412440  0.571224  0.566921  0.302285  0.720090  \n",
       "19  0.565681  0.407100  0.561444  0.555425  0.296933  0.709126  \n",
       "20  0.568950  0.406195  0.565314  0.561551  0.299971  0.711693  \n",
       "21  0.584895  0.416495  0.578030  0.574682  0.305938  0.727823  \n",
       "22  0.575697  0.411564  0.570218  0.565898  0.301840  0.718753  \n",
       "23  0.575669  0.410942  0.570299  0.566398  0.302058  0.718540  \n",
       "24  0.415788  0.313079  0.436861  0.424568  0.235676  0.557220  \n",
       "25  0.580571  0.414385  0.574323  0.570373  0.303934  0.723623  \n",
       "26  0.567749  0.405367  0.564057  0.560270  0.299264  0.710386  \n",
       "27  0.585416  0.416816  0.578431  0.575104  0.306123  0.728341  \n",
       "28  0.574330  0.410786  0.569191  0.564770  0.301362  0.717422  \n",
       "29  0.439224  0.329214  0.456707  0.444454  0.245260  0.581564  \n",
       "30  0.572368  0.408546  0.567541  0.563723  0.300771  0.715076  \n",
       "31  0.558181  0.398423  0.556543  0.553038  0.295960  0.700469  \n",
       "32  0.574675  0.410912  0.569466  0.565120  0.301513  0.717737  \n",
       "33  0.582309  0.414862  0.575834  0.572380  0.304842  0.725187  \n",
       "34  0.568929  0.407870  0.562803  0.557660  0.297344  0.711641  \n",
       "35  0.559291  0.398295  0.558955  0.556194  0.297936  0.701676  \n",
       "36  0.580764  0.414107  0.574351  0.570668  0.303984  0.723646  \n",
       "37  0.567186  0.406821  0.563381  0.558321  0.298440  0.710394  \n",
       "38  0.574339  0.409254  0.569329  0.565967  0.301829  0.716927  \n",
       "39  0.558582  0.405446  0.554427  0.546103  0.292399  0.702632  \n",
       "\n",
       "[40 rows x 650 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(decoded_stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    ax = plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history[\"loss\"])\n",
    "    plt.title(\"Train loss\")\n",
    "    ax = plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history[\"val_loss\"])\n",
    "    plt.title(\"Test loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loss vs Epoch\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D convolutional autoencoder\n",
    "(Kernel size = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 650, 1)]          0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 650, 16)           64        \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 325, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 325, 1)            49        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 163, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 163, 1)            4         \n",
      "_________________________________________________________________\n",
      "up_sampling1d (UpSampling1D) (None, 326, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 325, 16)           48        \n",
      "_________________________________________________________________\n",
      "up_sampling1d_1 (UpSampling1 (None, 650, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 650, 1)            49        \n",
      "=================================================================\n",
      "Total params: 214\n",
      "Trainable params: 214\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "1/1 [==============================] - 13s 13s/step - loss: 0.3040 - val_loss: 0.3267\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.2979 - val_loss: 0.3206\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.2920 - val_loss: 0.3146\n",
      "Epoch 4/500\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.2861 - val_loss: 0.3085\n",
      "Epoch 5/500\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.2801 - val_loss: 0.3024\n",
      "Epoch 6/500\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.2741 - val_loss: 0.2963\n",
      "Epoch 7/500\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.2680 - val_loss: 0.2900\n",
      "Epoch 8/500\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.2618 - val_loss: 0.2837\n",
      "Epoch 9/500\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.2555 - val_loss: 0.2772\n",
      "Epoch 10/500\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.2492 - val_loss: 0.2707\n",
      "Epoch 11/500\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.2427 - val_loss: 0.2640\n",
      "Epoch 12/500\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.2362 - val_loss: 0.2573\n",
      "Epoch 13/500\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.2295 - val_loss: 0.2504\n",
      "Epoch 14/500\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.2227 - val_loss: 0.2435\n",
      "Epoch 15/500\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.2159 - val_loss: 0.2364\n",
      "Epoch 16/500\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.2089 - val_loss: 0.2293\n",
      "Epoch 17/500\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.2019 - val_loss: 0.2220\n",
      "Epoch 18/500\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.1947 - val_loss: 0.2147\n",
      "Epoch 19/500\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.1875 - val_loss: 0.2072\n",
      "Epoch 20/500\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.1802 - val_loss: 0.1997\n",
      "Epoch 21/500\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.1728 - val_loss: 0.1921\n",
      "Epoch 22/500\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.1654 - val_loss: 0.1845\n",
      "Epoch 23/500\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.1579 - val_loss: 0.1768\n",
      "Epoch 24/500\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.1504 - val_loss: 0.1691\n",
      "Epoch 25/500\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.1429 - val_loss: 0.1614\n",
      "Epoch 26/500\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.1353 - val_loss: 0.1537\n",
      "Epoch 27/500\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 0.1278 - val_loss: 0.1461\n",
      "Epoch 28/500\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.1204 - val_loss: 0.1385\n",
      "Epoch 29/500\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.1130 - val_loss: 0.1310\n",
      "Epoch 30/500\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.1058 - val_loss: 0.1237\n",
      "Epoch 31/500\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.0986 - val_loss: 0.1165\n",
      "Epoch 32/500\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0917 - val_loss: 0.1095\n",
      "Epoch 33/500\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0849 - val_loss: 0.1027\n",
      "Epoch 34/500\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0785 - val_loss: 0.0963\n",
      "Epoch 35/500\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.0723 - val_loss: 0.0901\n",
      "Epoch 36/500\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.0664 - val_loss: 0.0844\n",
      "Epoch 37/500\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0609 - val_loss: 0.0790\n",
      "Epoch 38/500\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0559 - val_loss: 0.0741\n",
      "Epoch 39/500\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0513 - val_loss: 0.0697\n",
      "Epoch 40/500\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0472 - val_loss: 0.0658\n",
      "Epoch 41/500\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0436 - val_loss: 0.0624\n",
      "Epoch 42/500\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0405 - val_loss: 0.0596\n",
      "Epoch 43/500\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0380 - val_loss: 0.0573\n",
      "Epoch 44/500\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0361 - val_loss: 0.0556\n",
      "Epoch 45/500\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0347 - val_loss: 0.0544\n",
      "Epoch 46/500\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0337 - val_loss: 0.0536\n",
      "Epoch 47/500\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0333 - val_loss: 0.0533\n",
      "Epoch 48/500\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0332 - val_loss: 0.0533\n",
      "Epoch 49/500\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0335 - val_loss: 0.0536\n",
      "Epoch 50/500\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0340 - val_loss: 0.0540\n",
      "Epoch 51/500\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0346 - val_loss: 0.0545\n",
      "Epoch 52/500\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0352 - val_loss: 0.0550\n",
      "Epoch 53/500\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0359 - val_loss: 0.0554\n",
      "Epoch 54/500\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0364 - val_loss: 0.0557\n",
      "Epoch 55/500\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.0368 - val_loss: 0.0559\n",
      "Epoch 56/500\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0370 - val_loss: 0.0559\n",
      "Epoch 57/500\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0371 - val_loss: 0.0559\n",
      "Epoch 58/500\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0370 - val_loss: 0.0557\n",
      "Epoch 59/500\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0367 - val_loss: 0.0554\n",
      "Epoch 60/500\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0364 - val_loss: 0.0550\n",
      "Epoch 61/500\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.0359 - val_loss: 0.0547\n",
      "Epoch 62/500\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.0355 - val_loss: 0.0543\n",
      "Epoch 63/500\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.0350 - val_loss: 0.0540\n",
      "Epoch 64/500\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.0346 - val_loss: 0.0537\n",
      "Epoch 65/500\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.0342 - val_loss: 0.0535\n",
      "Epoch 66/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0339 - val_loss: 0.0534\n",
      "Epoch 67/500\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0336 - val_loss: 0.0533\n",
      "Epoch 68/500\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0334 - val_loss: 0.0532\n",
      "Epoch 69/500\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.0333 - val_loss: 0.0533\n",
      "Epoch 70/500\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.0332 - val_loss: 0.0533\n",
      "Epoch 71/500\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0332 - val_loss: 0.0534\n",
      "Epoch 72/500\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0332 - val_loss: 0.0535\n",
      "Epoch 73/500\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0332 - val_loss: 0.0536\n",
      "Epoch 74/500\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0333 - val_loss: 0.0537\n",
      "Epoch 75/500\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0333 - val_loss: 0.0538\n",
      "Epoch 76/500\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0334 - val_loss: 0.0539\n",
      "Epoch 77/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 0.0334 - val_loss: 0.0539\n",
      "Epoch 78/500\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0334 - val_loss: 0.0540\n",
      "Epoch 79/500\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0335 - val_loss: 0.0540\n",
      "Epoch 80/500\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0335 - val_loss: 0.0540\n",
      "Epoch 81/500\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0335 - val_loss: 0.0540\n",
      "Epoch 82/500\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0335 - val_loss: 0.0539\n",
      "Epoch 83/500\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0334 - val_loss: 0.0539\n",
      "Epoch 84/500\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0334 - val_loss: 0.0538\n",
      "Epoch 85/500\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0334 - val_loss: 0.0537\n",
      "Epoch 86/500\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.0333 - val_loss: 0.0537\n",
      "Epoch 87/500\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0333 - val_loss: 0.0536\n",
      "Epoch 88/500\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0332 - val_loss: 0.0535\n",
      "Epoch 89/500\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0332 - val_loss: 0.0534\n",
      "Epoch 90/500\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.0331 - val_loss: 0.0534\n",
      "Epoch 91/500\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0331 - val_loss: 0.0533\n",
      "Epoch 92/500\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0331 - val_loss: 0.0532\n",
      "Epoch 93/500\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0331 - val_loss: 0.0532\n",
      "Epoch 94/500\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0331 - val_loss: 0.0532\n",
      "Epoch 95/500\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0331 - val_loss: 0.0531\n",
      "Epoch 96/500\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0330 - val_loss: 0.0531\n",
      "Epoch 97/500\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.0330 - val_loss: 0.0531\n",
      "Epoch 98/500\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.0330 - val_loss: 0.0530\n",
      "Epoch 99/500\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0330 - val_loss: 0.0530\n",
      "Epoch 100/500\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0330 - val_loss: 0.0530\n",
      "Epoch 101/500\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0330 - val_loss: 0.0530\n",
      "Epoch 102/500\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.0330 - val_loss: 0.0530\n",
      "Epoch 103/500\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 0.0330 - val_loss: 0.0530\n",
      "Epoch 104/500\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0330 - val_loss: 0.0530\n",
      "Epoch 105/500\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0330 - val_loss: 0.0530\n",
      "Epoch 106/500\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0330 - val_loss: 0.0529\n",
      "Epoch 107/500\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.0330 - val_loss: 0.0529\n",
      "Epoch 108/500\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0330 - val_loss: 0.0529\n",
      "Epoch 109/500\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0330 - val_loss: 0.0529\n",
      "Epoch 110/500\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0330 - val_loss: 0.0529\n",
      "Epoch 111/500\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0329 - val_loss: 0.0529\n",
      "Epoch 112/500\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0329 - val_loss: 0.0529\n",
      "Epoch 113/500\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0329 - val_loss: 0.0529\n",
      "Epoch 114/500\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0329 - val_loss: 0.0529\n",
      "Epoch 115/500\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0329 - val_loss: 0.0529\n",
      "Epoch 116/500\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0329 - val_loss: 0.0529\n",
      "Epoch 117/500\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0329 - val_loss: 0.0529\n",
      "Epoch 118/500\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0329 - val_loss: 0.0529\n",
      "Epoch 119/500\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0329 - val_loss: 0.0529\n",
      "Epoch 120/500\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0329 - val_loss: 0.0529\n",
      "Epoch 121/500\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0328 - val_loss: 0.0529\n",
      "Epoch 122/500\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0328 - val_loss: 0.0529\n",
      "Epoch 123/500\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0328 - val_loss: 0.0529\n",
      "Epoch 124/500\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.0328 - val_loss: 0.0528\n",
      "Epoch 125/500\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.0328 - val_loss: 0.0528\n",
      "Epoch 126/500\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.0328 - val_loss: 0.0528\n",
      "Epoch 127/500\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0328 - val_loss: 0.0528\n",
      "Epoch 128/500\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0328 - val_loss: 0.0528\n",
      "Epoch 129/500\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0328 - val_loss: 0.0528\n",
      "Epoch 130/500\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0328 - val_loss: 0.0527\n",
      "Epoch 131/500\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0328 - val_loss: 0.0527\n",
      "Epoch 132/500\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 0.0327 - val_loss: 0.0527\n",
      "Epoch 133/500\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0327 - val_loss: 0.0527\n",
      "Epoch 134/500\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0327 - val_loss: 0.0527\n",
      "Epoch 135/500\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.0327 - val_loss: 0.0527\n",
      "Epoch 136/500\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0327 - val_loss: 0.0526\n",
      "Epoch 137/500\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0327 - val_loss: 0.0526\n",
      "Epoch 138/500\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0327 - val_loss: 0.0526\n",
      "Epoch 139/500\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.0327 - val_loss: 0.0526\n",
      "Epoch 140/500\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0327 - val_loss: 0.0526\n",
      "Epoch 141/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.032 - 0s 148ms/step - loss: 0.0327 - val_loss: 0.0525\n",
      "Epoch 142/500\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0326 - val_loss: 0.0525\n",
      "Epoch 143/500\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0326 - val_loss: 0.0525\n",
      "Epoch 144/500\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0326 - val_loss: 0.0525\n",
      "Epoch 145/500\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.0326 - val_loss: 0.0525\n",
      "Epoch 146/500\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.0326 - val_loss: 0.0525\n",
      "Epoch 147/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0326 - val_loss: 0.0524\n",
      "Epoch 148/500\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0326 - val_loss: 0.0524\n",
      "Epoch 149/500\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.0326 - val_loss: 0.0524\n",
      "Epoch 150/500\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0325 - val_loss: 0.0524\n",
      "Epoch 151/500\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0325 - val_loss: 0.0524\n",
      "Epoch 152/500\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0325 - val_loss: 0.0524\n",
      "Epoch 153/500\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0325 - val_loss: 0.0524\n",
      "Epoch 154/500\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0325 - val_loss: 0.0523\n",
      "Epoch 155/500\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0325 - val_loss: 0.0523\n",
      "Epoch 156/500\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0325 - val_loss: 0.0523\n",
      "Epoch 157/500\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0325 - val_loss: 0.0523\n",
      "Epoch 158/500\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0325 - val_loss: 0.0523\n",
      "Epoch 159/500\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.0324 - val_loss: 0.0523\n",
      "Epoch 160/500\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.0324 - val_loss: 0.0522\n",
      "Epoch 161/500\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.0324 - val_loss: 0.0522\n",
      "Epoch 162/500\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0324 - val_loss: 0.0522\n",
      "Epoch 163/500\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0324 - val_loss: 0.0522\n",
      "Epoch 164/500\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0324 - val_loss: 0.0522\n",
      "Epoch 165/500\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 0.0324 - val_loss: 0.0521\n",
      "Epoch 166/500\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0323 - val_loss: 0.0521\n",
      "Epoch 167/500\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0323 - val_loss: 0.0521\n",
      "Epoch 168/500\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0323 - val_loss: 0.0521\n",
      "Epoch 169/500\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0323 - val_loss: 0.0521\n",
      "Epoch 170/500\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0323 - val_loss: 0.0520\n",
      "Epoch 171/500\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.0323 - val_loss: 0.0520\n",
      "Epoch 172/500\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0323 - val_loss: 0.0520\n",
      "Epoch 173/500\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0322 - val_loss: 0.0520\n",
      "Epoch 174/500\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0322 - val_loss: 0.0520\n",
      "Epoch 175/500\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.0322 - val_loss: 0.0519\n",
      "Epoch 176/500\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0322 - val_loss: 0.0519\n",
      "Epoch 177/500\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.0322 - val_loss: 0.0519\n",
      "Epoch 178/500\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0322 - val_loss: 0.0519\n",
      "Epoch 179/500\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0322 - val_loss: 0.0518\n",
      "Epoch 180/500\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.0321 - val_loss: 0.0518\n",
      "Epoch 181/500\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0321 - val_loss: 0.0518\n",
      "Epoch 182/500\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.0321 - val_loss: 0.0518\n",
      "Epoch 183/500\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0321 - val_loss: 0.0517\n",
      "Epoch 184/500\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0321 - val_loss: 0.0517\n",
      "Epoch 185/500\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0321 - val_loss: 0.0517\n",
      "Epoch 186/500\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0320 - val_loss: 0.0517\n",
      "Epoch 187/500\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0320 - val_loss: 0.0517\n",
      "Epoch 188/500\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0320 - val_loss: 0.0516\n",
      "Epoch 189/500\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0320 - val_loss: 0.0516\n",
      "Epoch 190/500\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0320 - val_loss: 0.0516\n",
      "Epoch 191/500\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0320 - val_loss: 0.0516\n",
      "Epoch 192/500\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0319 - val_loss: 0.0515\n",
      "Epoch 193/500\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0319 - val_loss: 0.0515\n",
      "Epoch 194/500\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0319 - val_loss: 0.0515\n",
      "Epoch 195/500\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0319 - val_loss: 0.0515\n",
      "Epoch 196/500\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0319 - val_loss: 0.0514\n",
      "Epoch 197/500\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0318 - val_loss: 0.0514\n",
      "Epoch 198/500\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0318 - val_loss: 0.0514\n",
      "Epoch 199/500\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.0318 - val_loss: 0.0513\n",
      "Epoch 200/500\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.0318 - val_loss: 0.0513\n",
      "Epoch 201/500\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0318 - val_loss: 0.0513\n",
      "Epoch 202/500\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0318 - val_loss: 0.0513\n",
      "Epoch 203/500\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.0317 - val_loss: 0.0512\n",
      "Epoch 204/500\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0317 - val_loss: 0.0512\n",
      "Epoch 205/500\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0317 - val_loss: 0.0512\n",
      "Epoch 206/500\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0317 - val_loss: 0.0511\n",
      "Epoch 207/500\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0316 - val_loss: 0.0511\n",
      "Epoch 208/500\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.0316 - val_loss: 0.0511\n",
      "Epoch 209/500\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0316 - val_loss: 0.0511\n",
      "Epoch 210/500\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0316 - val_loss: 0.0510\n",
      "Epoch 211/500\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0316 - val_loss: 0.0510\n",
      "Epoch 212/500\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0315 - val_loss: 0.0510\n",
      "Epoch 213/500\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0315 - val_loss: 0.0509\n",
      "Epoch 214/500\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0315 - val_loss: 0.0509\n",
      "Epoch 215/500\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0315 - val_loss: 0.0509\n",
      "Epoch 216/500\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.0314 - val_loss: 0.0508\n",
      "Epoch 217/500\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.0314 - val_loss: 0.0508\n",
      "Epoch 218/500\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.0314 - val_loss: 0.0508\n",
      "Epoch 219/500\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.0314 - val_loss: 0.0507\n",
      "Epoch 220/500\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0314 - val_loss: 0.0507\n",
      "Epoch 221/500\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0313 - val_loss: 0.0507\n",
      "Epoch 222/500\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0313 - val_loss: 0.0506\n",
      "Epoch 223/500\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.0313 - val_loss: 0.0506\n",
      "Epoch 224/500\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0313 - val_loss: 0.0506\n",
      "Epoch 225/500\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0312 - val_loss: 0.0505\n",
      "Epoch 226/500\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0312 - val_loss: 0.0505\n",
      "Epoch 227/500\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0312 - val_loss: 0.0504\n",
      "Epoch 228/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0311 - val_loss: 0.0504\n",
      "Epoch 229/500\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.0311 - val_loss: 0.0504\n",
      "Epoch 230/500\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0311 - val_loss: 0.0503\n",
      "Epoch 231/500\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0311 - val_loss: 0.0503\n",
      "Epoch 232/500\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0310 - val_loss: 0.0503\n",
      "Epoch 233/500\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0310 - val_loss: 0.0502\n",
      "Epoch 234/500\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.0310 - val_loss: 0.0502\n",
      "Epoch 235/500\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0310 - val_loss: 0.0501\n",
      "Epoch 236/500\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0309 - val_loss: 0.0501\n",
      "Epoch 237/500\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0309 - val_loss: 0.0500\n",
      "Epoch 238/500\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.0309 - val_loss: 0.0500\n",
      "Epoch 239/500\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0308 - val_loss: 0.0500\n",
      "Epoch 240/500\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.0308 - val_loss: 0.0499\n",
      "Epoch 241/500\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.0308 - val_loss: 0.0499\n",
      "Epoch 242/500\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0307 - val_loss: 0.0498\n",
      "Epoch 243/500\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0307 - val_loss: 0.0498\n",
      "Epoch 244/500\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0307 - val_loss: 0.0497\n",
      "Epoch 245/500\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.0306 - val_loss: 0.0497\n",
      "Epoch 246/500\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0306 - val_loss: 0.0497\n",
      "Epoch 247/500\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0306 - val_loss: 0.0496\n",
      "Epoch 248/500\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0305 - val_loss: 0.0496\n",
      "Epoch 249/500\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0305 - val_loss: 0.0495\n",
      "Epoch 250/500\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.0305 - val_loss: 0.0495\n",
      "Epoch 251/500\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.0304 - val_loss: 0.0494\n",
      "Epoch 252/500\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.0304 - val_loss: 0.0494\n",
      "Epoch 253/500\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0304 - val_loss: 0.0493\n",
      "Epoch 254/500\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0303 - val_loss: 0.0493\n",
      "Epoch 255/500\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.0303 - val_loss: 0.0492\n",
      "Epoch 256/500\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0303 - val_loss: 0.0492\n",
      "Epoch 257/500\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0302 - val_loss: 0.0491\n",
      "Epoch 258/500\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0302 - val_loss: 0.0491\n",
      "Epoch 259/500\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.0301 - val_loss: 0.0490\n",
      "Epoch 260/500\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0301 - val_loss: 0.0490\n",
      "Epoch 261/500\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0301 - val_loss: 0.0489\n",
      "Epoch 262/500\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.0300 - val_loss: 0.0488\n",
      "Epoch 263/500\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0300 - val_loss: 0.0488\n",
      "Epoch 264/500\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 0.0299 - val_loss: 0.0487\n",
      "Epoch 265/500\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0299 - val_loss: 0.0487\n",
      "Epoch 266/500\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.0299 - val_loss: 0.0486\n",
      "Epoch 267/500\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.0298 - val_loss: 0.0486\n",
      "Epoch 268/500\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0298 - val_loss: 0.0485\n",
      "Epoch 269/500\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0297 - val_loss: 0.0484\n",
      "Epoch 270/500\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.0297 - val_loss: 0.0484\n",
      "Epoch 271/500\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.0297 - val_loss: 0.0483\n",
      "Epoch 272/500\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.0296 - val_loss: 0.0483\n",
      "Epoch 273/500\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0296 - val_loss: 0.0482\n",
      "Epoch 274/500\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.0295 - val_loss: 0.0482\n",
      "Epoch 275/500\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0295 - val_loss: 0.0481\n",
      "Epoch 276/500\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0294 - val_loss: 0.0480\n",
      "Epoch 277/500\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.0294 - val_loss: 0.0480\n",
      "Epoch 278/500\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.0293 - val_loss: 0.0479\n",
      "Epoch 279/500\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0293 - val_loss: 0.0479\n",
      "Epoch 280/500\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.0293 - val_loss: 0.0478\n",
      "Epoch 281/500\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 0.0292 - val_loss: 0.0477\n",
      "Epoch 282/500\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 0.0292 - val_loss: 0.0477\n",
      "Epoch 283/500\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 0.0291 - val_loss: 0.0476\n",
      "Epoch 284/500\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 0.0291 - val_loss: 0.0475\n",
      "Epoch 285/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.0290 - val_loss: 0.0475\n",
      "Epoch 286/500\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.0290 - val_loss: 0.0474\n",
      "Epoch 287/500\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0289 - val_loss: 0.0474\n",
      "Epoch 288/500\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0289 - val_loss: 0.0473\n",
      "Epoch 289/500\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0288 - val_loss: 0.0472\n",
      "Epoch 290/500\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.0288 - val_loss: 0.0472\n",
      "Epoch 291/500\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.0288 - val_loss: 0.0471\n",
      "Epoch 292/500\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0287 - val_loss: 0.0470\n",
      "Epoch 293/500\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.0287 - val_loss: 0.0470\n",
      "Epoch 294/500\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.0286 - val_loss: 0.0469\n",
      "Epoch 295/500\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.0286 - val_loss: 0.0469\n",
      "Epoch 296/500\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0285 - val_loss: 0.0468\n",
      "Epoch 297/500\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0285 - val_loss: 0.0467\n",
      "Epoch 298/500\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0284 - val_loss: 0.0467\n",
      "Epoch 299/500\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0284 - val_loss: 0.0466\n",
      "Epoch 300/500\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0283 - val_loss: 0.0466\n",
      "Epoch 301/500\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 0.0283 - val_loss: 0.0465\n",
      "Epoch 302/500\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.0283 - val_loss: 0.0464\n",
      "Epoch 303/500\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0282 - val_loss: 0.0464\n",
      "Epoch 304/500\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0282 - val_loss: 0.0463\n",
      "Epoch 305/500\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0281 - val_loss: 0.0463\n",
      "Epoch 306/500\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.0281 - val_loss: 0.0462\n",
      "Epoch 307/500\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.0280 - val_loss: 0.0462\n",
      "Epoch 308/500\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.0280 - val_loss: 0.0461\n",
      "Epoch 309/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0280 - val_loss: 0.0460\n",
      "Epoch 310/500\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0279 - val_loss: 0.0460\n",
      "Epoch 311/500\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.0279 - val_loss: 0.0459\n",
      "Epoch 312/500\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0278 - val_loss: 0.0459\n",
      "Epoch 313/500\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0278 - val_loss: 0.0458\n",
      "Epoch 314/500\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.0278 - val_loss: 0.0458\n",
      "Epoch 315/500\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.0277 - val_loss: 0.0457\n",
      "Epoch 316/500\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.0277 - val_loss: 0.0457\n",
      "Epoch 317/500\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.0277 - val_loss: 0.0457\n",
      "Epoch 318/500\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0276 - val_loss: 0.0456\n",
      "Epoch 319/500\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0276 - val_loss: 0.0456\n",
      "Epoch 320/500\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0276 - val_loss: 0.0455\n",
      "Epoch 321/500\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0275 - val_loss: 0.0455\n",
      "Epoch 322/500\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0275 - val_loss: 0.0454\n",
      "Epoch 323/500\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0275 - val_loss: 0.0454\n",
      "Epoch 324/500\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.0274 - val_loss: 0.0454\n",
      "Epoch 325/500\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0274 - val_loss: 0.0453\n",
      "Epoch 326/500\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 0.0274 - val_loss: 0.0453\n",
      "Epoch 327/500\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.0274 - val_loss: 0.0453\n",
      "Epoch 328/500\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0273 - val_loss: 0.0452\n",
      "Epoch 329/500\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.0273 - val_loss: 0.0452\n",
      "Epoch 330/500\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.0273 - val_loss: 0.0452\n",
      "Epoch 331/500\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0273 - val_loss: 0.0451\n",
      "Epoch 332/500\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0272 - val_loss: 0.0451\n",
      "Epoch 333/500\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0272 - val_loss: 0.0451\n",
      "Epoch 334/500\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.0272 - val_loss: 0.0451\n",
      "Epoch 335/500\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.0272 - val_loss: 0.0450\n",
      "Epoch 336/500\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0272 - val_loss: 0.0450\n",
      "Epoch 337/500\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.0271 - val_loss: 0.0450\n",
      "Epoch 338/500\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.0271 - val_loss: 0.0450\n",
      "Epoch 339/500\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.0271 - val_loss: 0.0449\n",
      "Epoch 340/500\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0271 - val_loss: 0.0449\n",
      "Epoch 341/500\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.0271 - val_loss: 0.0449\n",
      "Epoch 342/500\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 0.0271 - val_loss: 0.0449\n",
      "Epoch 343/500\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.0271 - val_loss: 0.0449\n",
      "Epoch 344/500\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 0.0270 - val_loss: 0.0448\n",
      "Epoch 345/500\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.0270 - val_loss: 0.0448\n",
      "Epoch 346/500\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0270 - val_loss: 0.0448\n",
      "Epoch 347/500\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0270 - val_loss: 0.0448\n",
      "Epoch 348/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.027 - 0s 162ms/step - loss: 0.0270 - val_loss: 0.0448\n",
      "Epoch 349/500\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.0270 - val_loss: 0.0448\n",
      "Epoch 350/500\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0270 - val_loss: 0.0447\n",
      "Epoch 351/500\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0270 - val_loss: 0.0447\n",
      "Epoch 352/500\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0270 - val_loss: 0.0447\n",
      "Epoch 353/500\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0270 - val_loss: 0.0447\n",
      "Epoch 354/500\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0269 - val_loss: 0.0447\n",
      "Epoch 355/500\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.0269 - val_loss: 0.0447\n",
      "Epoch 356/500\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0269 - val_loss: 0.0447\n",
      "Epoch 357/500\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0269 - val_loss: 0.0446\n",
      "Epoch 358/500\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.0269 - val_loss: 0.0446\n",
      "Epoch 359/500\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 0.0269 - val_loss: 0.0446\n",
      "Epoch 360/500\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0269 - val_loss: 0.0446\n",
      "Epoch 361/500\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0269 - val_loss: 0.0446\n",
      "Epoch 362/500\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0269 - val_loss: 0.0446\n",
      "Epoch 363/500\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0269 - val_loss: 0.0446\n",
      "Epoch 364/500\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0269 - val_loss: 0.0446\n",
      "Epoch 365/500\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0269 - val_loss: 0.0445\n",
      "Epoch 366/500\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.0269 - val_loss: 0.0445\n",
      "Epoch 367/500\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.0268 - val_loss: 0.0445\n",
      "Epoch 368/500\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0268 - val_loss: 0.0445\n",
      "Epoch 369/500\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0268 - val_loss: 0.0445\n",
      "Epoch 370/500\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.0268 - val_loss: 0.0445\n",
      "Epoch 371/500\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.0268 - val_loss: 0.0445\n",
      "Epoch 372/500\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.0268 - val_loss: 0.0445\n",
      "Epoch 373/500\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.0268 - val_loss: 0.0445\n",
      "Epoch 374/500\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.0268 - val_loss: 0.0444\n",
      "Epoch 375/500\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.0268 - val_loss: 0.0444\n",
      "Epoch 376/500\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.0268 - val_loss: 0.0444\n",
      "Epoch 377/500\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0268 - val_loss: 0.0444\n",
      "Epoch 378/500\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.0268 - val_loss: 0.0444\n",
      "Epoch 379/500\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.0268 - val_loss: 0.0444\n",
      "Epoch 380/500\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.0268 - val_loss: 0.0444\n",
      "Epoch 381/500\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.0268 - val_loss: 0.0444\n",
      "Epoch 382/500\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.0267 - val_loss: 0.0444\n",
      "Epoch 383/500\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.0267 - val_loss: 0.0443\n",
      "Epoch 384/500\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 0.0267 - val_loss: 0.0443\n",
      "Epoch 385/500\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0267 - val_loss: 0.0443\n",
      "Epoch 386/500\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0267 - val_loss: 0.0443\n",
      "Epoch 387/500\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.0267 - val_loss: 0.0443\n",
      "Epoch 388/500\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 0.0267 - val_loss: 0.0443\n",
      "Epoch 389/500\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.0267 - val_loss: 0.0443\n",
      "Epoch 390/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0267 - val_loss: 0.0443\n",
      "Epoch 391/500\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0267 - val_loss: 0.0443\n",
      "Epoch 392/500\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0267 - val_loss: 0.0442\n",
      "Epoch 393/500\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0267 - val_loss: 0.0442\n",
      "Epoch 394/500\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0267 - val_loss: 0.0442\n",
      "Epoch 395/500\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0267 - val_loss: 0.0442\n",
      "Epoch 396/500\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.0267 - val_loss: 0.0442\n",
      "Epoch 397/500\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0267 - val_loss: 0.0442\n",
      "Epoch 398/500\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0267 - val_loss: 0.0442\n",
      "Epoch 399/500\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.0266 - val_loss: 0.0442\n",
      "Epoch 400/500\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.0266 - val_loss: 0.0442\n",
      "Epoch 401/500\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.0266 - val_loss: 0.0441\n",
      "Epoch 402/500\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 0.0266 - val_loss: 0.0441\n",
      "Epoch 403/500\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0266 - val_loss: 0.0441\n",
      "Epoch 404/500\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.0266 - val_loss: 0.0441\n",
      "Epoch 405/500\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 0.0266 - val_loss: 0.0441\n",
      "Epoch 406/500\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.0266 - val_loss: 0.0441\n",
      "Epoch 407/500\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.0266 - val_loss: 0.0441\n",
      "Epoch 408/500\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 0.0266 - val_loss: 0.0441\n",
      "Epoch 409/500\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0266 - val_loss: 0.0441\n",
      "Epoch 410/500\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.0266 - val_loss: 0.0440\n",
      "Epoch 411/500\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.0266 - val_loss: 0.0440\n",
      "Epoch 412/500\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0266 - val_loss: 0.0440\n",
      "Epoch 413/500\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 0.0266 - val_loss: 0.0440\n",
      "Epoch 414/500\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.0266 - val_loss: 0.0440\n",
      "Epoch 415/500\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0266 - val_loss: 0.0440\n",
      "Epoch 416/500\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0265 - val_loss: 0.0440\n",
      "Epoch 417/500\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0265 - val_loss: 0.0440\n",
      "Epoch 418/500\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.0265 - val_loss: 0.0440\n",
      "Epoch 419/500\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.0265 - val_loss: 0.0440\n",
      "Epoch 420/500\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.0265 - val_loss: 0.0439\n",
      "Epoch 421/500\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.0265 - val_loss: 0.0439\n",
      "Epoch 422/500\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0265 - val_loss: 0.0439\n",
      "Epoch 423/500\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0265 - val_loss: 0.0439\n",
      "Epoch 424/500\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.0265 - val_loss: 0.0439\n",
      "Epoch 425/500\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.0265 - val_loss: 0.0439\n",
      "Epoch 426/500\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0265 - val_loss: 0.0439\n",
      "Epoch 427/500\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.0265 - val_loss: 0.0439\n",
      "Epoch 428/500\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.0265 - val_loss: 0.0439\n",
      "Epoch 429/500\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.0265 - val_loss: 0.0438\n",
      "Epoch 430/500\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0265 - val_loss: 0.0438\n",
      "Epoch 431/500\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0265 - val_loss: 0.0438\n",
      "Epoch 432/500\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.0265 - val_loss: 0.0438\n",
      "Epoch 433/500\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.0264 - val_loss: 0.0438\n",
      "Epoch 434/500\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 0.0264 - val_loss: 0.0438\n",
      "Epoch 435/500\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.0264 - val_loss: 0.0438\n",
      "Epoch 436/500\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.0264 - val_loss: 0.0438\n",
      "Epoch 437/500\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0264 - val_loss: 0.0438\n",
      "Epoch 438/500\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0264 - val_loss: 0.0438\n",
      "Epoch 439/500\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0264 - val_loss: 0.0437\n",
      "Epoch 440/500\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.0264 - val_loss: 0.0437\n",
      "Epoch 441/500\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.0264 - val_loss: 0.0437\n",
      "Epoch 442/500\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.0264 - val_loss: 0.0437\n",
      "Epoch 443/500\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.0264 - val_loss: 0.0437\n",
      "Epoch 444/500\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 0.0264 - val_loss: 0.0437\n",
      "Epoch 445/500\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.0264 - val_loss: 0.0437\n",
      "Epoch 446/500\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.0264 - val_loss: 0.0437\n",
      "Epoch 447/500\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.0264 - val_loss: 0.0437\n",
      "Epoch 448/500\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.0264 - val_loss: 0.0437\n",
      "Epoch 449/500\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.0264 - val_loss: 0.0436\n",
      "Epoch 450/500\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.0264 - val_loss: 0.0436\n",
      "Epoch 451/500\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0263 - val_loss: 0.0436\n",
      "Epoch 452/500\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.0263 - val_loss: 0.0436\n",
      "Epoch 453/500\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.0263 - val_loss: 0.0436\n",
      "Epoch 454/500\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.0263 - val_loss: 0.0436\n",
      "Epoch 455/500\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.0263 - val_loss: 0.0436\n",
      "Epoch 456/500\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.0263 - val_loss: 0.0436\n",
      "Epoch 457/500\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.0263 - val_loss: 0.0436\n",
      "Epoch 458/500\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.0263 - val_loss: 0.0436\n",
      "Epoch 459/500\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.0263 - val_loss: 0.0435\n",
      "Epoch 460/500\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.0263 - val_loss: 0.0435\n",
      "Epoch 461/500\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.0263 - val_loss: 0.0435\n",
      "Epoch 462/500\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.0263 - val_loss: 0.0435\n",
      "Epoch 463/500\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0263 - val_loss: 0.0435\n",
      "Epoch 464/500\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.0263 - val_loss: 0.0435\n",
      "Epoch 465/500\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.0263 - val_loss: 0.0435\n",
      "Epoch 466/500\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 0.0263 - val_loss: 0.0435\n",
      "Epoch 467/500\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.0263 - val_loss: 0.0435\n",
      "Epoch 468/500\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 0.0263 - val_loss: 0.0435\n",
      "Epoch 469/500\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0263 - val_loss: 0.0434\n",
      "Epoch 470/500\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.0262 - val_loss: 0.0434\n",
      "Epoch 471/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 159ms/step - loss: 0.0262 - val_loss: 0.0434\n",
      "Epoch 472/500\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 0.0262 - val_loss: 0.0434\n",
      "Epoch 473/500\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 0.0262 - val_loss: 0.0434\n",
      "Epoch 474/500\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0262 - val_loss: 0.0434\n",
      "Epoch 475/500\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.0262 - val_loss: 0.0434\n",
      "Epoch 476/500\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0262 - val_loss: 0.0434\n",
      "Epoch 477/500\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0262 - val_loss: 0.0434\n",
      "Epoch 478/500\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.0262 - val_loss: 0.0434\n",
      "Epoch 479/500\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.0262 - val_loss: 0.0433\n",
      "Epoch 480/500\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0262 - val_loss: 0.0433\n",
      "Epoch 481/500\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0262 - val_loss: 0.0433\n",
      "Epoch 482/500\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0262 - val_loss: 0.0433\n",
      "Epoch 483/500\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.0262 - val_loss: 0.0433\n",
      "Epoch 484/500\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.0262 - val_loss: 0.0433\n",
      "Epoch 485/500\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0262 - val_loss: 0.0433\n",
      "Epoch 486/500\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.0262 - val_loss: 0.0433\n",
      "Epoch 487/500\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0261 - val_loss: 0.0433\n",
      "Epoch 488/500\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0261 - val_loss: 0.0433\n",
      "Epoch 489/500\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.0261 - val_loss: 0.0433\n",
      "Epoch 490/500\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0261 - val_loss: 0.0432\n",
      "Epoch 491/500\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0261 - val_loss: 0.0432\n",
      "Epoch 492/500\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.0261 - val_loss: 0.0432\n",
      "Epoch 493/500\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.0261 - val_loss: 0.0432\n",
      "Epoch 494/500\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.0261 - val_loss: 0.0432\n",
      "Epoch 495/500\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.0261 - val_loss: 0.0432\n",
      "Epoch 496/500\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 0.0261 - val_loss: 0.0432\n",
      "Epoch 497/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 0.0261 - val_loss: 0.0432\n",
      "Epoch 498/500\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 0.0261 - val_loss: 0.0432\n",
      "Epoch 499/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 0.0261 - val_loss: 0.0432\n",
      "Epoch 500/500\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.0261 - val_loss: 0.0431\n"
     ]
    }
   ],
   "source": [
    "# main “event” very well represented while the overall reconstruction is very smooth \n",
    "\n",
    "input_window = Input(shape=(window_length,1))\n",
    "x = Conv1D(16, 3, activation=\"tanh\", padding=\"same\")(input_window) # 10 dims\n",
    "#x = BatchNormalization()(x)\n",
    "x = MaxPooling1D(2, padding=\"same\")(x) # 5 dims\n",
    "x = Conv1D(1, 3, activation=\"tanh\", padding=\"same\")(x) # 5 dims\n",
    "#x = BatchNormalization()(x)\n",
    "encoded = MaxPooling1D(2, padding=\"same\")(x) # 3 dims\n",
    "encoder = Model(input_window, encoded)\n",
    "\n",
    "# 3 dimensions in the encoded layer\n",
    "\n",
    "x = Conv1D(1, 3, activation=\"tanh\", padding=\"same\")(encoded) # 3 dims\n",
    "#x = BatchNormalization()(x)\n",
    "x = UpSampling1D(2)(x) # 6 dims\n",
    "x = Conv1D(16, 2, activation='tanh')(x) # 5 dims\n",
    "#x = BatchNormalization()(x)\n",
    "x = UpSampling1D(2)(x) # 10 dims\n",
    "decoded = Conv1D(1, 3, activation='linear', padding='same')(x) # 10 dims\n",
    "autoencoder = Model(input_window, decoded)\n",
    "autoencoder.summary()\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='MeanSquaredError')\n",
    "history = autoencoder.fit(training_set_scaled, training_set_scaled,\n",
    "                epochs=epochs,\n",
    "                batch_size=1024,\n",
    "                shuffle=True,\n",
    "                validation_data=(test_set_scaled, test_set_scaled))\n",
    "\n",
    "decoded_stocks = autoencoder.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(encoder.predict(test_set_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
