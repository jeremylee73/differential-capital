{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Conv1D, MaxPooling1D, UpSampling1D, BatchNormalization, LSTM, RepeatVector\n",
    "from keras.models import Sequential, Model\n",
    "from keras.models import model_from_json\n",
    "from tensorflow import keras\n",
    "from keras import regularizers\n",
    "import datetime\n",
    "import time\n",
    "import requests as req\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from tqdm import tqdm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.compat.v1\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_pickle(\"../Data/returns.pkl\").iloc[1:]\n",
    "\n",
    "drop_columns = []\n",
    "for col in df1.columns:\n",
    "    if df1[col].isnull().all() == True:\n",
    "        drop_columns.append(col)\n",
    "        \n",
    "df1.drop(columns=drop_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_investable(t, n_rows):\n",
    "    \"Find stocks in investable universe at time t\\\n",
    "    (stocks in the S&P500 that have prices recorded for the last n_rows days)\"\n",
    "    \n",
    "    df_investable = df1.copy(deep = True).sort_index(ascending = False)\n",
    "    \n",
    "    #add 1 date to get the test features in investable\n",
    "    t = t + pd.DateOffset(1)\n",
    "    \n",
    "    #if t is now a non-trading day, advance until we reach a valid trading day\n",
    "    while t not in df_investable.index:\n",
    "        t = t + pd.DateOffset(1)\n",
    "    \n",
    "    t_index = df_investable.index.get_loc(t)\n",
    "    \n",
    "    #take n_rows worth of data upto time specified\n",
    "    df_investable = df_investable.iloc[t_index + 1:t_index + n_rows + 1]\n",
    "    \n",
    "    #find all stocks that exist in the S&P at this time period\n",
    "    investable_universe = []\n",
    "    for col in df_investable.columns:\n",
    "        if ~df_investable[col].iloc[:n_rows].isna().any():\n",
    "            investable_universe.append(col)\n",
    "        \n",
    "    df_investable = df_investable[investable_universe]\n",
    "    \n",
    "    return df_investable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = get_investable(pd.to_datetime('2018-05-11'),500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tts = train_test_split(df1, test_size=0.2, shuffle=False)\n",
    "train = tts[0]\n",
    "test = tts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "training_set_scaled = sc.fit_transform(train)\n",
    "test_set_scaled = sc.fit_transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 644)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 20)                12900     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 644)               13524     \n",
      "=================================================================\n",
      "Total params: 26,424\n",
      "Trainable params: 26,424\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/250\n",
      "1/1 [==============================] - 11s 11s/step - loss: 0.2875 - acc: 0.0000e+00 - mae: 0.5007 - val_loss: 0.2784 - val_acc: 0.0000e+00 - val_mae: 0.4940\n",
      "Epoch 2/250\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.2735 - acc: 0.0031 - mae: 0.4909 - val_loss: 0.2689 - val_acc: 0.0000e+00 - val_mae: 0.4841\n",
      "Epoch 3/250\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.2642 - acc: 0.0031 - mae: 0.4809 - val_loss: 0.2624 - val_acc: 0.0000e+00 - val_mae: 0.4754\n",
      "Epoch 4/250\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2577 - acc: 0.0000e+00 - mae: 0.4722 - val_loss: 0.2556 - val_acc: 0.0000e+00 - val_mae: 0.4655\n",
      "Epoch 5/250\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.2511 - acc: 0.0063 - mae: 0.4624 - val_loss: 0.2464 - val_acc: 0.0000e+00 - val_mae: 0.4529\n",
      "Epoch 6/250\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2419 - acc: 0.0063 - mae: 0.4498 - val_loss: 0.2354 - val_acc: 0.0000e+00 - val_mae: 0.4382\n",
      "Epoch 7/250\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2310 - acc: 0.0063 - mae: 0.4351 - val_loss: 0.2238 - val_acc: 0.0000e+00 - val_mae: 0.4223\n",
      "Epoch 8/250\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.2194 - acc: 0.0063 - mae: 0.4192 - val_loss: 0.2121 - val_acc: 0.0000e+00 - val_mae: 0.4061\n",
      "Epoch 9/250\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.2078 - acc: 0.0063 - mae: 0.4029 - val_loss: 0.2006 - val_acc: 0.0000e+00 - val_mae: 0.3902\n",
      "Epoch 10/250\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1964 - acc: 0.0063 - mae: 0.3869 - val_loss: 0.1891 - val_acc: 0.0000e+00 - val_mae: 0.3748\n",
      "Epoch 11/250\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.1849 - acc: 0.0063 - mae: 0.3713 - val_loss: 0.1775 - val_acc: 0.0000e+00 - val_mae: 0.3597\n",
      "Epoch 12/250\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1734 - acc: 0.0031 - mae: 0.3560 - val_loss: 0.1659 - val_acc: 0.0000e+00 - val_mae: 0.3445\n",
      "Epoch 13/250\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1619 - acc: 0.0000e+00 - mae: 0.3408 - val_loss: 0.1544 - val_acc: 0.0000e+00 - val_mae: 0.3293\n",
      "Epoch 14/250\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1505 - acc: 0.0000e+00 - mae: 0.3256 - val_loss: 0.1431 - val_acc: 0.0000e+00 - val_mae: 0.3141\n",
      "Epoch 15/250\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1394 - acc: 0.0000e+00 - mae: 0.3104 - val_loss: 0.1323 - val_acc: 0.0000e+00 - val_mae: 0.2991\n",
      "Epoch 16/250\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1286 - acc: 0.0000e+00 - mae: 0.2953 - val_loss: 0.1218 - val_acc: 0.0000e+00 - val_mae: 0.2843\n",
      "Epoch 17/250\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1183 - acc: 0.0000e+00 - mae: 0.2806 - val_loss: 0.1119 - val_acc: 0.0000e+00 - val_mae: 0.2699\n",
      "Epoch 18/250\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1085 - acc: 0.0000e+00 - mae: 0.2661 - val_loss: 0.1025 - val_acc: 0.0000e+00 - val_mae: 0.2559\n",
      "Epoch 19/250\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0991 - acc: 0.0000e+00 - mae: 0.2520 - val_loss: 0.0936 - val_acc: 0.0000e+00 - val_mae: 0.2423\n",
      "Epoch 20/250\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0903 - acc: 0.0000e+00 - mae: 0.2383 - val_loss: 0.0852 - val_acc: 0.0000e+00 - val_mae: 0.2292\n",
      "Epoch 21/250\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0821 - acc: 0.0000e+00 - mae: 0.2251 - val_loss: 0.0775 - val_acc: 0.0000e+00 - val_mae: 0.2165\n",
      "Epoch 22/250\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0745 - acc: 0.0000e+00 - mae: 0.2124 - val_loss: 0.0704 - val_acc: 0.0000e+00 - val_mae: 0.2044\n",
      "Epoch 23/250\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0674 - acc: 0.0000e+00 - mae: 0.2003 - val_loss: 0.0638 - val_acc: 0.0000e+00 - val_mae: 0.1930\n",
      "Epoch 24/250\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0610 - acc: 0.0000e+00 - mae: 0.1888 - val_loss: 0.0579 - val_acc: 0.0000e+00 - val_mae: 0.1822\n",
      "Epoch 25/250\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0551 - acc: 0.0000e+00 - mae: 0.1780 - val_loss: 0.0525 - val_acc: 0.0000e+00 - val_mae: 0.1722\n",
      "Epoch 26/250\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0498 - acc: 0.0000e+00 - mae: 0.1679 - val_loss: 0.0476 - val_acc: 0.0000e+00 - val_mae: 0.1629\n",
      "Epoch 27/250\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0449 - acc: 0.0000e+00 - mae: 0.1585 - val_loss: 0.0432 - val_acc: 0.0000e+00 - val_mae: 0.1543\n",
      "Epoch 28/250\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0406 - acc: 0.0000e+00 - mae: 0.1499 - val_loss: 0.0393 - val_acc: 0.0000e+00 - val_mae: 0.1464\n",
      "Epoch 29/250\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0368 - acc: 0.0000e+00 - mae: 0.1420 - val_loss: 0.0359 - val_acc: 0.0000e+00 - val_mae: 0.1392\n",
      "Epoch 30/250\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0334 - acc: 0.0031 - mae: 0.1348 - val_loss: 0.0328 - val_acc: 0.0000e+00 - val_mae: 0.1327\n",
      "Epoch 31/250\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0304 - acc: 0.0094 - mae: 0.1282 - val_loss: 0.0301 - val_acc: 0.0000e+00 - val_mae: 0.1268\n",
      "Epoch 32/250\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0278 - acc: 0.0094 - mae: 0.1223 - val_loss: 0.0278 - val_acc: 0.0000e+00 - val_mae: 0.1215\n",
      "Epoch 33/250\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0254 - acc: 0.0094 - mae: 0.1169 - val_loss: 0.0257 - val_acc: 0.0000e+00 - val_mae: 0.1167\n",
      "Epoch 34/250\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0234 - acc: 0.0094 - mae: 0.1121 - val_loss: 0.0239 - val_acc: 0.0000e+00 - val_mae: 0.1124\n",
      "Epoch 35/250\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0217 - acc: 0.0094 - mae: 0.1078 - val_loss: 0.0224 - val_acc: 0.0000e+00 - val_mae: 0.1086\n",
      "Epoch 36/250\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0202 - acc: 0.0094 - mae: 0.1039 - val_loss: 0.0210 - val_acc: 0.0000e+00 - val_mae: 0.1052\n",
      "Epoch 37/250\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0189 - acc: 0.0094 - mae: 0.1004 - val_loss: 0.0199 - val_acc: 0.0000e+00 - val_mae: 0.1022\n",
      "Epoch 38/250\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0177 - acc: 0.0125 - mae: 0.0974 - val_loss: 0.0189 - val_acc: 0.0000e+00 - val_mae: 0.0996\n",
      "Epoch 39/250\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0168 - acc: 0.0125 - mae: 0.0947 - val_loss: 0.0180 - val_acc: 0.0000e+00 - val_mae: 0.0972\n",
      "Epoch 40/250\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0159 - acc: 0.0125 - mae: 0.0922 - val_loss: 0.0173 - val_acc: 0.0000e+00 - val_mae: 0.0951\n",
      "Epoch 41/250\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0152 - acc: 0.0125 - mae: 0.0901 - val_loss: 0.0167 - val_acc: 0.0000e+00 - val_mae: 0.0932\n",
      "Epoch 42/250\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0146 - acc: 0.0125 - mae: 0.0881 - val_loss: 0.0161 - val_acc: 0.0000e+00 - val_mae: 0.0915\n",
      "Epoch 43/250\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0140 - acc: 0.0125 - mae: 0.0864 - val_loss: 0.0156 - val_acc: 0.0000e+00 - val_mae: 0.0900\n",
      "Epoch 44/250\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0136 - acc: 0.0125 - mae: 0.0848 - val_loss: 0.0152 - val_acc: 0.0000e+00 - val_mae: 0.0886\n",
      "Epoch 45/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0131 - acc: 0.0125 - mae: 0.0833 - val_loss: 0.0148 - val_acc: 0.0000e+00 - val_mae: 0.0873\n",
      "Epoch 46/250\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0128 - acc: 0.0125 - mae: 0.0820 - val_loss: 0.0145 - val_acc: 0.0000e+00 - val_mae: 0.0862\n",
      "Epoch 47/250\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0125 - acc: 0.0125 - mae: 0.0808 - val_loss: 0.0143 - val_acc: 0.0000e+00 - val_mae: 0.0852\n",
      "Epoch 48/250\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0122 - acc: 0.0125 - mae: 0.0797 - val_loss: 0.0140 - val_acc: 0.1250 - val_mae: 0.0842\n",
      "Epoch 49/250\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0120 - acc: 0.1187 - mae: 0.0787 - val_loss: 0.0138 - val_acc: 0.1500 - val_mae: 0.0833\n",
      "Epoch 50/250\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0117 - acc: 0.1750 - mae: 0.0778 - val_loss: 0.0136 - val_acc: 0.1500 - val_mae: 0.0825\n",
      "Epoch 51/250\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0115 - acc: 0.1781 - mae: 0.0770 - val_loss: 0.0134 - val_acc: 0.1500 - val_mae: 0.0818\n",
      "Epoch 52/250\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0114 - acc: 0.1844 - mae: 0.0762 - val_loss: 0.0133 - val_acc: 0.1500 - val_mae: 0.0811\n",
      "Epoch 53/250\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0112 - acc: 0.1844 - mae: 0.0755 - val_loss: 0.0131 - val_acc: 0.1500 - val_mae: 0.0805\n",
      "Epoch 54/250\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0111 - acc: 0.1844 - mae: 0.0749 - val_loss: 0.0130 - val_acc: 0.1500 - val_mae: 0.0799\n",
      "Epoch 55/250\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0110 - acc: 0.1844 - mae: 0.0743 - val_loss: 0.0129 - val_acc: 0.1500 - val_mae: 0.0794\n",
      "Epoch 56/250\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0109 - acc: 0.1844 - mae: 0.0738 - val_loss: 0.0128 - val_acc: 0.1500 - val_mae: 0.0789\n",
      "Epoch 57/250\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0108 - acc: 0.1844 - mae: 0.0733 - val_loss: 0.0127 - val_acc: 0.1500 - val_mae: 0.0784\n",
      "Epoch 58/250\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0107 - acc: 0.1844 - mae: 0.0728 - val_loss: 0.0127 - val_acc: 0.1500 - val_mae: 0.0780\n",
      "Epoch 59/250\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0106 - acc: 0.1844 - mae: 0.0724 - val_loss: 0.0126 - val_acc: 0.1500 - val_mae: 0.0777\n",
      "Epoch 60/250\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0105 - acc: 0.1875 - mae: 0.0721 - val_loss: 0.0125 - val_acc: 0.1500 - val_mae: 0.0773\n",
      "Epoch 61/250\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0105 - acc: 0.1875 - mae: 0.0718 - val_loss: 0.0125 - val_acc: 0.1500 - val_mae: 0.0770\n",
      "Epoch 62/250\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0104 - acc: 0.1875 - mae: 0.0715 - val_loss: 0.0124 - val_acc: 0.1500 - val_mae: 0.0767\n",
      "Epoch 63/250\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0104 - acc: 0.1875 - mae: 0.0712 - val_loss: 0.0124 - val_acc: 0.1500 - val_mae: 0.0765\n",
      "Epoch 64/250\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0103 - acc: 0.1875 - mae: 0.0710 - val_loss: 0.0124 - val_acc: 0.1500 - val_mae: 0.0762\n",
      "Epoch 65/250\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0103 - acc: 0.1875 - mae: 0.0707 - val_loss: 0.0123 - val_acc: 0.1500 - val_mae: 0.0760\n",
      "Epoch 66/250\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0102 - acc: 0.1875 - mae: 0.0705 - val_loss: 0.0123 - val_acc: 0.1500 - val_mae: 0.0758\n",
      "Epoch 67/250\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0102 - acc: 0.1875 - mae: 0.0703 - val_loss: 0.0123 - val_acc: 0.1500 - val_mae: 0.0756\n",
      "Epoch 68/250\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0102 - acc: 0.1875 - mae: 0.0702 - val_loss: 0.0122 - val_acc: 0.1500 - val_mae: 0.0755\n",
      "Epoch 69/250\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0102 - acc: 0.1875 - mae: 0.0700 - val_loss: 0.0122 - val_acc: 0.1500 - val_mae: 0.0753\n",
      "Epoch 70/250\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0101 - acc: 0.1875 - mae: 0.0699 - val_loss: 0.0122 - val_acc: 0.1500 - val_mae: 0.0752\n",
      "Epoch 71/250\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0101 - acc: 0.1875 - mae: 0.0697 - val_loss: 0.0121 - val_acc: 0.1500 - val_mae: 0.0750\n",
      "Epoch 72/250\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0101 - acc: 0.1875 - mae: 0.0696 - val_loss: 0.0121 - val_acc: 0.1500 - val_mae: 0.0749\n",
      "Epoch 73/250\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0101 - acc: 0.1875 - mae: 0.0695 - val_loss: 0.0121 - val_acc: 0.1500 - val_mae: 0.0748\n",
      "Epoch 74/250\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0100 - acc: 0.1875 - mae: 0.0694 - val_loss: 0.0121 - val_acc: 0.1500 - val_mae: 0.0747\n",
      "Epoch 75/250\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0100 - acc: 0.1875 - mae: 0.0693 - val_loss: 0.0121 - val_acc: 0.1500 - val_mae: 0.0746\n",
      "Epoch 76/250\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0100 - acc: 0.1875 - mae: 0.0692 - val_loss: 0.0121 - val_acc: 0.1500 - val_mae: 0.0745\n",
      "Epoch 77/250\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0100 - acc: 0.1875 - mae: 0.0691 - val_loss: 0.0120 - val_acc: 0.1500 - val_mae: 0.0745\n",
      "Epoch 78/250\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0100 - acc: 0.1875 - mae: 0.0690 - val_loss: 0.0120 - val_acc: 0.1500 - val_mae: 0.0744\n",
      "Epoch 79/250\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0100 - acc: 0.1844 - mae: 0.0690 - val_loss: 0.0120 - val_acc: 0.1500 - val_mae: 0.0743\n",
      "Epoch 80/250\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0099 - acc: 0.1844 - mae: 0.0689 - val_loss: 0.0120 - val_acc: 0.1500 - val_mae: 0.0743\n",
      "Epoch 81/250\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0099 - acc: 0.1844 - mae: 0.0688 - val_loss: 0.0120 - val_acc: 0.1500 - val_mae: 0.0742\n",
      "Epoch 82/250\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0099 - acc: 0.1844 - mae: 0.0688 - val_loss: 0.0120 - val_acc: 0.1500 - val_mae: 0.0742\n",
      "Epoch 83/250\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0099 - acc: 0.1844 - mae: 0.0687 - val_loss: 0.0120 - val_acc: 0.1500 - val_mae: 0.0742\n",
      "Epoch 84/250\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0099 - acc: 0.1844 - mae: 0.0687 - val_loss: 0.0120 - val_acc: 0.1500 - val_mae: 0.0741\n",
      "Epoch 85/250\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0099 - acc: 0.1844 - mae: 0.0686 - val_loss: 0.0120 - val_acc: 0.1500 - val_mae: 0.0741\n",
      "Epoch 86/250\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0099 - acc: 0.1844 - mae: 0.0686 - val_loss: 0.0120 - val_acc: 0.1500 - val_mae: 0.0741\n",
      "Epoch 87/250\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0099 - acc: 0.1844 - mae: 0.0685 - val_loss: 0.0120 - val_acc: 0.1500 - val_mae: 0.0740\n",
      "Epoch 88/250\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0099 - acc: 0.1844 - mae: 0.0685 - val_loss: 0.0120 - val_acc: 0.1500 - val_mae: 0.0740\n",
      "Epoch 89/250\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0099 - acc: 0.1844 - mae: 0.0684 - val_loss: 0.0119 - val_acc: 0.1500 - val_mae: 0.0740\n",
      "Epoch 90/250\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0098 - acc: 0.1844 - mae: 0.0684 - val_loss: 0.0119 - val_acc: 0.1500 - val_mae: 0.0740\n",
      "Epoch 91/250\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0098 - acc: 0.1844 - mae: 0.0684 - val_loss: 0.0119 - val_acc: 0.1500 - val_mae: 0.0740\n",
      "Epoch 92/250\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0098 - acc: 0.1844 - mae: 0.0683 - val_loss: 0.0119 - val_acc: 0.1500 - val_mae: 0.0739\n",
      "Epoch 93/250\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0098 - acc: 0.1844 - mae: 0.0683 - val_loss: 0.0119 - val_acc: 0.1500 - val_mae: 0.0739\n",
      "Epoch 94/250\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0098 - acc: 0.1844 - mae: 0.0683 - val_loss: 0.0119 - val_acc: 0.1500 - val_mae: 0.0739\n",
      "Epoch 95/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0098 - acc: 0.1844 - mae: 0.0682 - val_loss: 0.0119 - val_acc: 0.1500 - val_mae: 0.0739\n",
      "Epoch 96/250\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0098 - acc: 0.1844 - mae: 0.0682 - val_loss: 0.0119 - val_acc: 0.1500 - val_mae: 0.0738\n",
      "Epoch 97/250\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0098 - acc: 0.1844 - mae: 0.0682 - val_loss: 0.0119 - val_acc: 0.1500 - val_mae: 0.0738\n",
      "Epoch 98/250\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0098 - acc: 0.1844 - mae: 0.0681 - val_loss: 0.0119 - val_acc: 0.1500 - val_mae: 0.0738\n",
      "Epoch 99/250\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0098 - acc: 0.1813 - mae: 0.0681 - val_loss: 0.0119 - val_acc: 0.1500 - val_mae: 0.0738\n",
      "Epoch 100/250\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0098 - acc: 0.1813 - mae: 0.0681 - val_loss: 0.0119 - val_acc: 0.1500 - val_mae: 0.0738\n",
      "Epoch 101/250\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0098 - acc: 0.1813 - mae: 0.0680 - val_loss: 0.0119 - val_acc: 0.1500 - val_mae: 0.0737\n",
      "Epoch 102/250\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0098 - acc: 0.1813 - mae: 0.0680 - val_loss: 0.0119 - val_acc: 0.1500 - val_mae: 0.0737\n",
      "Epoch 103/250\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0097 - acc: 0.1813 - mae: 0.0680 - val_loss: 0.0119 - val_acc: 0.1500 - val_mae: 0.0737\n",
      "Epoch 104/250\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0097 - acc: 0.1813 - mae: 0.0680 - val_loss: 0.0119 - val_acc: 0.1500 - val_mae: 0.0737\n",
      "Epoch 105/250\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0097 - acc: 0.1781 - mae: 0.0679 - val_loss: 0.0119 - val_acc: 0.1500 - val_mae: 0.0737\n",
      "Epoch 106/250\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0097 - acc: 0.1781 - mae: 0.0679 - val_loss: 0.0119 - val_acc: 0.1500 - val_mae: 0.0737\n",
      "Epoch 107/250\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0097 - acc: 0.1781 - mae: 0.0679 - val_loss: 0.0119 - val_acc: 0.1500 - val_mae: 0.0736\n",
      "Epoch 108/250\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0097 - acc: 0.1781 - mae: 0.0679 - val_loss: 0.0119 - val_acc: 0.1500 - val_mae: 0.0736\n",
      "Epoch 109/250\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0097 - acc: 0.1781 - mae: 0.0678 - val_loss: 0.0119 - val_acc: 0.1500 - val_mae: 0.0736\n",
      "Epoch 110/250\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0097 - acc: 0.1781 - mae: 0.0678 - val_loss: 0.0119 - val_acc: 0.1500 - val_mae: 0.0736\n",
      "Epoch 111/250\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0097 - acc: 0.1781 - mae: 0.0678 - val_loss: 0.0119 - val_acc: 0.1500 - val_mae: 0.0735\n",
      "Epoch 112/250\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0097 - acc: 0.1781 - mae: 0.0677 - val_loss: 0.0119 - val_acc: 0.1500 - val_mae: 0.0735\n",
      "Epoch 113/250\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0097 - acc: 0.1781 - mae: 0.0677 - val_loss: 0.0118 - val_acc: 0.1500 - val_mae: 0.0735\n",
      "Epoch 114/250\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0097 - acc: 0.1781 - mae: 0.0677 - val_loss: 0.0118 - val_acc: 0.1500 - val_mae: 0.0735\n",
      "Epoch 115/250\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0097 - acc: 0.1781 - mae: 0.0676 - val_loss: 0.0118 - val_acc: 0.1500 - val_mae: 0.0735\n",
      "Epoch 116/250\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0096 - acc: 0.1781 - mae: 0.0676 - val_loss: 0.0118 - val_acc: 0.1500 - val_mae: 0.0734\n",
      "Epoch 117/250\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0096 - acc: 0.1781 - mae: 0.0676 - val_loss: 0.0118 - val_acc: 0.1500 - val_mae: 0.0734\n",
      "Epoch 118/250\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0096 - acc: 0.1781 - mae: 0.0675 - val_loss: 0.0118 - val_acc: 0.1500 - val_mae: 0.0734\n",
      "Epoch 119/250\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0096 - acc: 0.1781 - mae: 0.0675 - val_loss: 0.0118 - val_acc: 0.1500 - val_mae: 0.0734\n",
      "Epoch 120/250\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0096 - acc: 0.1781 - mae: 0.0675 - val_loss: 0.0118 - val_acc: 0.1500 - val_mae: 0.0733\n",
      "Epoch 121/250\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0096 - acc: 0.1750 - mae: 0.0674 - val_loss: 0.0118 - val_acc: 0.1500 - val_mae: 0.0733\n",
      "Epoch 122/250\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0096 - acc: 0.1750 - mae: 0.0674 - val_loss: 0.0118 - val_acc: 0.1500 - val_mae: 0.0733\n",
      "Epoch 123/250\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0096 - acc: 0.1750 - mae: 0.0674 - val_loss: 0.0118 - val_acc: 0.1500 - val_mae: 0.0733\n",
      "Epoch 124/250\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0096 - acc: 0.1750 - mae: 0.0673 - val_loss: 0.0118 - val_acc: 0.1500 - val_mae: 0.0732\n",
      "Epoch 125/250\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0096 - acc: 0.1750 - mae: 0.0673 - val_loss: 0.0118 - val_acc: 0.1500 - val_mae: 0.0732\n",
      "Epoch 126/250\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0095 - acc: 0.1750 - mae: 0.0672 - val_loss: 0.0118 - val_acc: 0.1500 - val_mae: 0.0732\n",
      "Epoch 127/250\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0095 - acc: 0.1750 - mae: 0.0672 - val_loss: 0.0118 - val_acc: 0.1500 - val_mae: 0.0732\n",
      "Epoch 128/250\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0095 - acc: 0.1750 - mae: 0.0672 - val_loss: 0.0118 - val_acc: 0.1500 - val_mae: 0.0731\n",
      "Epoch 129/250\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0095 - acc: 0.1750 - mae: 0.0671 - val_loss: 0.0118 - val_acc: 0.1500 - val_mae: 0.0731\n",
      "Epoch 130/250\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0095 - acc: 0.1750 - mae: 0.0671 - val_loss: 0.0118 - val_acc: 0.1500 - val_mae: 0.0731\n",
      "Epoch 131/250\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0095 - acc: 0.1750 - mae: 0.0670 - val_loss: 0.0117 - val_acc: 0.1500 - val_mae: 0.0731\n",
      "Epoch 132/250\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0095 - acc: 0.1750 - mae: 0.0670 - val_loss: 0.0117 - val_acc: 0.1500 - val_mae: 0.0730\n",
      "Epoch 133/250\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0095 - acc: 0.1750 - mae: 0.0670 - val_loss: 0.0117 - val_acc: 0.1500 - val_mae: 0.0730\n",
      "Epoch 134/250\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0095 - acc: 0.1750 - mae: 0.0669 - val_loss: 0.0117 - val_acc: 0.1500 - val_mae: 0.0730\n",
      "Epoch 135/250\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0095 - acc: 0.1750 - mae: 0.0669 - val_loss: 0.0117 - val_acc: 0.1500 - val_mae: 0.0729\n",
      "Epoch 136/250\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0094 - acc: 0.1750 - mae: 0.0668 - val_loss: 0.0117 - val_acc: 0.1500 - val_mae: 0.0729\n",
      "Epoch 137/250\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0094 - acc: 0.1750 - mae: 0.0668 - val_loss: 0.0117 - val_acc: 0.1500 - val_mae: 0.0729\n",
      "Epoch 138/250\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0094 - acc: 0.1750 - mae: 0.0667 - val_loss: 0.0117 - val_acc: 0.1500 - val_mae: 0.0729\n",
      "Epoch 139/250\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0094 - acc: 0.1750 - mae: 0.0667 - val_loss: 0.0117 - val_acc: 0.1500 - val_mae: 0.0728\n",
      "Epoch 140/250\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0094 - acc: 0.1750 - mae: 0.0667 - val_loss: 0.0117 - val_acc: 0.1500 - val_mae: 0.0728\n",
      "Epoch 141/250\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0094 - acc: 0.1750 - mae: 0.0666 - val_loss: 0.0117 - val_acc: 0.1500 - val_mae: 0.0728\n",
      "Epoch 142/250\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0094 - acc: 0.1750 - mae: 0.0666 - val_loss: 0.0117 - val_acc: 0.1500 - val_mae: 0.0727\n",
      "Epoch 143/250\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0094 - acc: 0.1750 - mae: 0.0665 - val_loss: 0.0117 - val_acc: 0.1500 - val_mae: 0.0727\n",
      "Epoch 144/250\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0094 - acc: 0.1750 - mae: 0.0665 - val_loss: 0.0117 - val_acc: 0.1500 - val_mae: 0.0727\n",
      "Epoch 145/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0093 - acc: 0.1750 - mae: 0.0664 - val_loss: 0.0117 - val_acc: 0.1500 - val_mae: 0.0726\n",
      "Epoch 146/250\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0093 - acc: 0.1750 - mae: 0.0664 - val_loss: 0.0116 - val_acc: 0.1500 - val_mae: 0.0726\n",
      "Epoch 147/250\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0093 - acc: 0.1750 - mae: 0.0663 - val_loss: 0.0116 - val_acc: 0.1500 - val_mae: 0.0726\n",
      "Epoch 148/250\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0093 - acc: 0.1750 - mae: 0.0663 - val_loss: 0.0116 - val_acc: 0.1500 - val_mae: 0.0725\n",
      "Epoch 149/250\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0093 - acc: 0.1750 - mae: 0.0662 - val_loss: 0.0116 - val_acc: 0.1500 - val_mae: 0.0725\n",
      "Epoch 150/250\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0093 - acc: 0.1750 - mae: 0.0662 - val_loss: 0.0116 - val_acc: 0.1500 - val_mae: 0.0725\n",
      "Epoch 151/250\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0093 - acc: 0.1750 - mae: 0.0661 - val_loss: 0.0116 - val_acc: 0.1500 - val_mae: 0.0724\n",
      "Epoch 152/250\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0092 - acc: 0.1750 - mae: 0.0661 - val_loss: 0.0116 - val_acc: 0.1500 - val_mae: 0.0724\n",
      "Epoch 153/250\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0092 - acc: 0.1750 - mae: 0.0660 - val_loss: 0.0116 - val_acc: 0.1500 - val_mae: 0.0723\n",
      "Epoch 154/250\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0092 - acc: 0.1750 - mae: 0.0659 - val_loss: 0.0116 - val_acc: 0.1500 - val_mae: 0.0723\n",
      "Epoch 155/250\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0092 - acc: 0.1750 - mae: 0.0659 - val_loss: 0.0116 - val_acc: 0.1500 - val_mae: 0.0723\n",
      "Epoch 156/250\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0092 - acc: 0.1750 - mae: 0.0658 - val_loss: 0.0116 - val_acc: 0.1500 - val_mae: 0.0723\n",
      "Epoch 157/250\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0092 - acc: 0.1750 - mae: 0.0658 - val_loss: 0.0116 - val_acc: 0.1500 - val_mae: 0.0723\n",
      "Epoch 158/250\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0092 - acc: 0.1750 - mae: 0.0658 - val_loss: 0.0116 - val_acc: 0.1500 - val_mae: 0.0723\n",
      "Epoch 159/250\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0091 - acc: 0.1750 - mae: 0.0658 - val_loss: 0.0115 - val_acc: 0.1500 - val_mae: 0.0722\n",
      "Epoch 160/250\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0091 - acc: 0.1750 - mae: 0.0656 - val_loss: 0.0115 - val_acc: 0.1500 - val_mae: 0.0721\n",
      "Epoch 161/250\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0091 - acc: 0.1750 - mae: 0.0655 - val_loss: 0.0115 - val_acc: 0.1500 - val_mae: 0.0720\n",
      "Epoch 162/250\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0091 - acc: 0.1750 - mae: 0.0655 - val_loss: 0.0115 - val_acc: 0.1500 - val_mae: 0.0720\n",
      "Epoch 163/250\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0091 - acc: 0.1750 - mae: 0.0654 - val_loss: 0.0115 - val_acc: 0.1500 - val_mae: 0.0719\n",
      "Epoch 164/250\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0091 - acc: 0.1750 - mae: 0.0654 - val_loss: 0.0115 - val_acc: 0.1500 - val_mae: 0.0719\n",
      "Epoch 165/250\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0090 - acc: 0.1750 - mae: 0.0653 - val_loss: 0.0115 - val_acc: 0.1500 - val_mae: 0.0719\n",
      "Epoch 166/250\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0090 - acc: 0.1750 - mae: 0.0653 - val_loss: 0.0115 - val_acc: 0.1500 - val_mae: 0.0719\n",
      "Epoch 167/250\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0090 - acc: 0.1750 - mae: 0.0653 - val_loss: 0.0115 - val_acc: 0.1500 - val_mae: 0.0718\n",
      "Epoch 168/250\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0090 - acc: 0.1750 - mae: 0.0652 - val_loss: 0.0115 - val_acc: 0.1500 - val_mae: 0.0718\n",
      "Epoch 169/250\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0090 - acc: 0.1750 - mae: 0.0652 - val_loss: 0.0114 - val_acc: 0.1500 - val_mae: 0.0717\n",
      "Epoch 170/250\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0090 - acc: 0.1750 - mae: 0.0651 - val_loss: 0.0114 - val_acc: 0.1500 - val_mae: 0.0717\n",
      "Epoch 171/250\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0090 - acc: 0.1750 - mae: 0.0650 - val_loss: 0.0114 - val_acc: 0.1500 - val_mae: 0.0716\n",
      "Epoch 172/250\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0090 - acc: 0.1750 - mae: 0.0650 - val_loss: 0.0114 - val_acc: 0.1500 - val_mae: 0.0716\n",
      "Epoch 173/250\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0090 - acc: 0.1750 - mae: 0.0649 - val_loss: 0.0114 - val_acc: 0.1500 - val_mae: 0.0716\n",
      "Epoch 174/250\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0089 - acc: 0.1750 - mae: 0.0649 - val_loss: 0.0114 - val_acc: 0.1500 - val_mae: 0.0715\n",
      "Epoch 175/250\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0089 - acc: 0.1750 - mae: 0.0648 - val_loss: 0.0114 - val_acc: 0.1500 - val_mae: 0.0715\n",
      "Epoch 176/250\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0089 - acc: 0.1750 - mae: 0.0648 - val_loss: 0.0114 - val_acc: 0.1500 - val_mae: 0.0715\n",
      "Epoch 177/250\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0089 - acc: 0.1750 - mae: 0.0648 - val_loss: 0.0114 - val_acc: 0.1500 - val_mae: 0.0714\n",
      "Epoch 178/250\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0089 - acc: 0.1750 - mae: 0.0647 - val_loss: 0.0114 - val_acc: 0.1500 - val_mae: 0.0714\n",
      "Epoch 179/250\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0089 - acc: 0.1750 - mae: 0.0647 - val_loss: 0.0114 - val_acc: 0.1500 - val_mae: 0.0714\n",
      "Epoch 180/250\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0089 - acc: 0.1750 - mae: 0.0646 - val_loss: 0.0113 - val_acc: 0.1500 - val_mae: 0.0713\n",
      "Epoch 181/250\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0089 - acc: 0.1750 - mae: 0.0646 - val_loss: 0.0113 - val_acc: 0.1500 - val_mae: 0.0713\n",
      "Epoch 182/250\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0089 - acc: 0.1750 - mae: 0.0645 - val_loss: 0.0113 - val_acc: 0.1500 - val_mae: 0.0713\n",
      "Epoch 183/250\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0089 - acc: 0.1750 - mae: 0.0645 - val_loss: 0.0113 - val_acc: 0.1500 - val_mae: 0.0712\n",
      "Epoch 184/250\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0088 - acc: 0.1750 - mae: 0.0645 - val_loss: 0.0113 - val_acc: 0.1500 - val_mae: 0.0712\n",
      "Epoch 185/250\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0088 - acc: 0.1750 - mae: 0.0644 - val_loss: 0.0113 - val_acc: 0.1500 - val_mae: 0.0712\n",
      "Epoch 186/250\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0088 - acc: 0.1750 - mae: 0.0644 - val_loss: 0.0113 - val_acc: 0.1500 - val_mae: 0.0711\n",
      "Epoch 187/250\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0088 - acc: 0.1750 - mae: 0.0644 - val_loss: 0.0113 - val_acc: 0.1500 - val_mae: 0.0711\n",
      "Epoch 188/250\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0088 - acc: 0.1750 - mae: 0.0643 - val_loss: 0.0113 - val_acc: 0.1500 - val_mae: 0.0711\n",
      "Epoch 189/250\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0088 - acc: 0.1750 - mae: 0.0643 - val_loss: 0.0113 - val_acc: 0.1500 - val_mae: 0.0711\n",
      "Epoch 190/250\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0088 - acc: 0.1750 - mae: 0.0643 - val_loss: 0.0113 - val_acc: 0.1500 - val_mae: 0.0710\n",
      "Epoch 191/250\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0088 - acc: 0.1750 - mae: 0.0642 - val_loss: 0.0113 - val_acc: 0.1500 - val_mae: 0.0710\n",
      "Epoch 192/250\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0088 - acc: 0.1750 - mae: 0.0642 - val_loss: 0.0113 - val_acc: 0.1500 - val_mae: 0.0710\n",
      "Epoch 193/250\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0088 - acc: 0.1750 - mae: 0.0642 - val_loss: 0.0113 - val_acc: 0.1500 - val_mae: 0.0709\n",
      "Epoch 194/250\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0088 - acc: 0.1750 - mae: 0.0641 - val_loss: 0.0113 - val_acc: 0.1500 - val_mae: 0.0709\n",
      "Epoch 195/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0088 - acc: 0.1750 - mae: 0.0641 - val_loss: 0.0112 - val_acc: 0.1500 - val_mae: 0.0709\n",
      "Epoch 196/250\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0088 - acc: 0.1750 - mae: 0.0641 - val_loss: 0.0112 - val_acc: 0.1500 - val_mae: 0.0709\n",
      "Epoch 197/250\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0087 - acc: 0.1750 - mae: 0.0641 - val_loss: 0.0112 - val_acc: 0.1500 - val_mae: 0.0708\n",
      "Epoch 198/250\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0087 - acc: 0.1750 - mae: 0.0640 - val_loss: 0.0112 - val_acc: 0.1500 - val_mae: 0.0708\n",
      "Epoch 199/250\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0087 - acc: 0.1750 - mae: 0.0640 - val_loss: 0.0112 - val_acc: 0.1500 - val_mae: 0.0708\n",
      "Epoch 200/250\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0087 - acc: 0.1750 - mae: 0.0640 - val_loss: 0.0112 - val_acc: 0.1500 - val_mae: 0.0708\n",
      "Epoch 201/250\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0087 - acc: 0.1750 - mae: 0.0640 - val_loss: 0.0112 - val_acc: 0.1500 - val_mae: 0.0707\n",
      "Epoch 202/250\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0087 - acc: 0.1750 - mae: 0.0639 - val_loss: 0.0112 - val_acc: 0.1500 - val_mae: 0.0707\n",
      "Epoch 203/250\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0087 - acc: 0.1750 - mae: 0.0639 - val_loss: 0.0112 - val_acc: 0.1500 - val_mae: 0.0707\n",
      "Epoch 204/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0087 - acc: 0.1750 - mae: 0.0639 - val_loss: 0.0112 - val_acc: 0.1500 - val_mae: 0.0707\n",
      "Epoch 205/250\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0087 - acc: 0.1750 - mae: 0.0639 - val_loss: 0.0112 - val_acc: 0.1500 - val_mae: 0.0706\n",
      "Epoch 206/250\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0087 - acc: 0.1750 - mae: 0.0638 - val_loss: 0.0112 - val_acc: 0.1500 - val_mae: 0.0706\n",
      "Epoch 207/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0087 - acc: 0.1750 - mae: 0.0638 - val_loss: 0.0112 - val_acc: 0.1500 - val_mae: 0.0706\n",
      "Epoch 208/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0087 - acc: 0.1750 - mae: 0.0638 - val_loss: 0.0112 - val_acc: 0.1500 - val_mae: 0.0706\n",
      "Epoch 209/250\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0087 - acc: 0.1750 - mae: 0.0638 - val_loss: 0.0112 - val_acc: 0.1500 - val_mae: 0.0705\n",
      "Epoch 210/250\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0087 - acc: 0.1750 - mae: 0.0637 - val_loss: 0.0112 - val_acc: 0.1500 - val_mae: 0.0705\n",
      "Epoch 211/250\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0087 - acc: 0.1750 - mae: 0.0637 - val_loss: 0.0112 - val_acc: 0.1500 - val_mae: 0.0705\n",
      "Epoch 212/250\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0087 - acc: 0.1750 - mae: 0.0637 - val_loss: 0.0111 - val_acc: 0.1500 - val_mae: 0.0705\n",
      "Epoch 213/250\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0087 - acc: 0.1750 - mae: 0.0637 - val_loss: 0.0111 - val_acc: 0.1500 - val_mae: 0.0704\n",
      "Epoch 214/250\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0086 - acc: 0.1750 - mae: 0.0636 - val_loss: 0.0111 - val_acc: 0.1500 - val_mae: 0.0704\n",
      "Epoch 215/250\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0086 - acc: 0.1750 - mae: 0.0636 - val_loss: 0.0111 - val_acc: 0.1500 - val_mae: 0.0704\n",
      "Epoch 216/250\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0086 - acc: 0.1750 - mae: 0.0636 - val_loss: 0.0111 - val_acc: 0.1500 - val_mae: 0.0704\n",
      "Epoch 217/250\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0086 - acc: 0.1750 - mae: 0.0636 - val_loss: 0.0111 - val_acc: 0.1500 - val_mae: 0.0704\n",
      "Epoch 218/250\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0086 - acc: 0.1750 - mae: 0.0636 - val_loss: 0.0111 - val_acc: 0.1500 - val_mae: 0.0703\n",
      "Epoch 219/250\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0086 - acc: 0.1750 - mae: 0.0635 - val_loss: 0.0111 - val_acc: 0.1500 - val_mae: 0.0703\n",
      "Epoch 220/250\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0086 - acc: 0.1750 - mae: 0.0635 - val_loss: 0.0111 - val_acc: 0.1500 - val_mae: 0.0703\n",
      "Epoch 221/250\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0086 - acc: 0.1750 - mae: 0.0635 - val_loss: 0.0111 - val_acc: 0.1500 - val_mae: 0.0703\n",
      "Epoch 222/250\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0086 - acc: 0.1750 - mae: 0.0635 - val_loss: 0.0111 - val_acc: 0.1500 - val_mae: 0.0703\n",
      "Epoch 223/250\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0086 - acc: 0.1750 - mae: 0.0635 - val_loss: 0.0111 - val_acc: 0.1500 - val_mae: 0.0703\n",
      "Epoch 224/250\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0086 - acc: 0.1750 - mae: 0.0635 - val_loss: 0.0111 - val_acc: 0.1500 - val_mae: 0.0702\n",
      "Epoch 225/250\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0086 - acc: 0.1750 - mae: 0.0634 - val_loss: 0.0111 - val_acc: 0.1500 - val_mae: 0.0702\n",
      "Epoch 226/250\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0086 - acc: 0.1750 - mae: 0.0634 - val_loss: 0.0111 - val_acc: 0.1500 - val_mae: 0.0702\n",
      "Epoch 227/250\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0086 - acc: 0.1750 - mae: 0.0634 - val_loss: 0.0111 - val_acc: 0.1500 - val_mae: 0.0702\n",
      "Epoch 228/250\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0086 - acc: 0.1750 - mae: 0.0634 - val_loss: 0.0111 - val_acc: 0.1500 - val_mae: 0.0702\n",
      "Epoch 229/250\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0086 - acc: 0.1750 - mae: 0.0634 - val_loss: 0.0111 - val_acc: 0.1500 - val_mae: 0.0702\n",
      "Epoch 230/250\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0086 - acc: 0.1750 - mae: 0.0634 - val_loss: 0.0111 - val_acc: 0.1500 - val_mae: 0.0701\n",
      "Epoch 231/250\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0086 - acc: 0.1750 - mae: 0.0633 - val_loss: 0.0111 - val_acc: 0.1500 - val_mae: 0.0701\n",
      "Epoch 232/250\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0086 - acc: 0.1750 - mae: 0.0633 - val_loss: 0.0111 - val_acc: 0.1500 - val_mae: 0.0701\n",
      "Epoch 233/250\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0086 - acc: 0.1750 - mae: 0.0633 - val_loss: 0.0111 - val_acc: 0.1500 - val_mae: 0.0701\n",
      "Epoch 234/250\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0086 - acc: 0.1750 - mae: 0.0633 - val_loss: 0.0111 - val_acc: 0.1500 - val_mae: 0.0701\n",
      "Epoch 235/250\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0086 - acc: 0.1750 - mae: 0.0633 - val_loss: 0.0110 - val_acc: 0.1500 - val_mae: 0.0701\n",
      "Epoch 236/250\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0086 - acc: 0.1750 - mae: 0.0633 - val_loss: 0.0110 - val_acc: 0.1500 - val_mae: 0.0701\n",
      "Epoch 237/250\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0085 - acc: 0.1750 - mae: 0.0632 - val_loss: 0.0110 - val_acc: 0.1500 - val_mae: 0.0700\n",
      "Epoch 238/250\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0085 - acc: 0.1750 - mae: 0.0632 - val_loss: 0.0110 - val_acc: 0.1500 - val_mae: 0.0700\n",
      "Epoch 239/250\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0085 - acc: 0.1750 - mae: 0.0632 - val_loss: 0.0110 - val_acc: 0.1500 - val_mae: 0.0700\n",
      "Epoch 240/250\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0085 - acc: 0.1750 - mae: 0.0632 - val_loss: 0.0110 - val_acc: 0.1500 - val_mae: 0.0700\n",
      "Epoch 241/250\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0085 - acc: 0.1750 - mae: 0.0632 - val_loss: 0.0110 - val_acc: 0.1500 - val_mae: 0.0700\n",
      "Epoch 242/250\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0085 - acc: 0.1750 - mae: 0.0632 - val_loss: 0.0110 - val_acc: 0.1500 - val_mae: 0.0700\n",
      "Epoch 243/250\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0085 - acc: 0.1750 - mae: 0.0632 - val_loss: 0.0110 - val_acc: 0.1500 - val_mae: 0.0700\n",
      "Epoch 244/250\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0085 - acc: 0.1750 - mae: 0.0631 - val_loss: 0.0110 - val_acc: 0.1500 - val_mae: 0.0699\n",
      "Epoch 245/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0085 - acc: 0.1750 - mae: 0.0631 - val_loss: 0.0110 - val_acc: 0.1500 - val_mae: 0.0699\n",
      "Epoch 246/250\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0085 - acc: 0.1750 - mae: 0.0631 - val_loss: 0.0110 - val_acc: 0.1500 - val_mae: 0.0699\n",
      "Epoch 247/250\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0085 - acc: 0.1750 - mae: 0.0631 - val_loss: 0.0110 - val_acc: 0.1500 - val_mae: 0.0699\n",
      "Epoch 248/250\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0085 - acc: 0.1750 - mae: 0.0631 - val_loss: 0.0110 - val_acc: 0.1500 - val_mae: 0.0699\n",
      "Epoch 249/250\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0085 - acc: 0.1750 - mae: 0.0631 - val_loss: 0.0110 - val_acc: 0.1500 - val_mae: 0.0699\n",
      "Epoch 250/250\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0085 - acc: 0.1750 - mae: 0.0631 - val_loss: 0.0110 - val_acc: 0.1500 - val_mae: 0.0699\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 644)]             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                12900     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 644)               13524     \n",
      "=================================================================\n",
      "Total params: 26,424\n",
      "Trainable params: 26,424\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# calculated log returns (i.e. the log of the difference between the price x+1 and price x)\n",
    "# windows of train.shape[1] consecutive returns will be produced. \n",
    "# normalized with a MinMaxScaler to the range [0,1].\n",
    "\n",
    "epochs = 250\n",
    "\n",
    "class simple_autoencoder():\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def reduced_dim(self):\n",
    "        encoding_dim = 20\n",
    "        window_length = training_set_scaled.shape[1]\n",
    "        input_window = Input(shape=(window_length,))\n",
    "        # encoded representation of the input\n",
    "        encoded = Dense(encoding_dim, activation='tanh')(input_window) #tanh, linear, leakyrelu\n",
    "        # model mapping an input to its encoded representation\n",
    "        encoder = Model(input_window, encoded)\n",
    "        return pd.DataFrame(encoder.predict(test_set_scaled)).head()\n",
    "\n",
    "    def model(self,optimizer = \"Adam\", score = \"acc\", loss = \"mean_squared_error\", epochs = 250):\n",
    "        encoding_dim = 20\n",
    "        window_length = training_set_scaled.shape[1]\n",
    "        # input placeholder\n",
    "        input_window = Input(shape=(window_length,))\n",
    "        # encoded representation of the input\n",
    "        encoded = Dense(encoding_dim, activation='tanh')(input_window) #tanh, linear, leakyrelu\n",
    "        # lossy reconstruction of the input\n",
    "        decoded = Dense(window_length, activation='linear')(encoded) #linear\n",
    "        # model mapping an input to its reconstruction\n",
    "        simple_autoencoder = Model(input_window, decoded)\n",
    "        simple_autoencoder.summary()\n",
    "        sae = simple_autoencoder.compile(optimizer=optimizer, loss=loss, metrics=['acc','mae']) #MSE\n",
    "        return simple_autoencoder\n",
    "\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-5, patience=5, mode='auto', verbose = 1)\n",
    "checkpointer = ModelCheckpoint(filepath=\"best_weights.hdf5\",save_best_only=True)\n",
    "\n",
    "\n",
    "model = simple_autoencoder()\n",
    "history = model.model().fit(training_set_scaled, training_set_scaled,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=1024,\n",
    "                    shuffle=True,\n",
    "                    validation_split = 0.2,\n",
    "                    callbacks = [monitor, checkpointer])       \n",
    "#                   validation_data=(test_set_scaled, test_set_scaled))\n",
    "\n",
    "decoded_stocks = simple_autoencoder().model().predict(test_set_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.622507</td>\n",
       "      <td>0.407368</td>\n",
       "      <td>-0.214510</td>\n",
       "      <td>0.806359</td>\n",
       "      <td>0.487520</td>\n",
       "      <td>0.519140</td>\n",
       "      <td>0.658717</td>\n",
       "      <td>-0.061238</td>\n",
       "      <td>0.400183</td>\n",
       "      <td>-0.082774</td>\n",
       "      <td>-0.018668</td>\n",
       "      <td>0.477773</td>\n",
       "      <td>0.408408</td>\n",
       "      <td>-0.225123</td>\n",
       "      <td>-0.746708</td>\n",
       "      <td>-0.216064</td>\n",
       "      <td>0.514340</td>\n",
       "      <td>-0.219927</td>\n",
       "      <td>0.484705</td>\n",
       "      <td>0.739327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.665546</td>\n",
       "      <td>0.332077</td>\n",
       "      <td>0.306328</td>\n",
       "      <td>0.845157</td>\n",
       "      <td>0.378590</td>\n",
       "      <td>0.498875</td>\n",
       "      <td>0.787913</td>\n",
       "      <td>-0.097629</td>\n",
       "      <td>0.310637</td>\n",
       "      <td>0.109444</td>\n",
       "      <td>0.281932</td>\n",
       "      <td>-0.021151</td>\n",
       "      <td>0.600632</td>\n",
       "      <td>-0.095384</td>\n",
       "      <td>-0.797391</td>\n",
       "      <td>0.482128</td>\n",
       "      <td>0.595226</td>\n",
       "      <td>-0.268552</td>\n",
       "      <td>0.302145</td>\n",
       "      <td>0.791331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.626102</td>\n",
       "      <td>0.192291</td>\n",
       "      <td>0.366238</td>\n",
       "      <td>0.772201</td>\n",
       "      <td>0.527536</td>\n",
       "      <td>0.457795</td>\n",
       "      <td>0.799344</td>\n",
       "      <td>-0.286114</td>\n",
       "      <td>0.254085</td>\n",
       "      <td>-0.166339</td>\n",
       "      <td>0.422577</td>\n",
       "      <td>0.313438</td>\n",
       "      <td>0.684595</td>\n",
       "      <td>-0.085658</td>\n",
       "      <td>-0.773745</td>\n",
       "      <td>0.030570</td>\n",
       "      <td>0.324075</td>\n",
       "      <td>-0.544847</td>\n",
       "      <td>0.326352</td>\n",
       "      <td>0.835627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.629605</td>\n",
       "      <td>0.270956</td>\n",
       "      <td>0.066702</td>\n",
       "      <td>0.848207</td>\n",
       "      <td>0.414008</td>\n",
       "      <td>0.404725</td>\n",
       "      <td>0.706411</td>\n",
       "      <td>-0.456213</td>\n",
       "      <td>0.367470</td>\n",
       "      <td>-0.307985</td>\n",
       "      <td>0.075451</td>\n",
       "      <td>0.113194</td>\n",
       "      <td>0.717568</td>\n",
       "      <td>0.021446</td>\n",
       "      <td>-0.628271</td>\n",
       "      <td>0.075955</td>\n",
       "      <td>0.325393</td>\n",
       "      <td>-0.511374</td>\n",
       "      <td>0.504569</td>\n",
       "      <td>0.697474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.630929</td>\n",
       "      <td>0.530425</td>\n",
       "      <td>0.144248</td>\n",
       "      <td>0.856890</td>\n",
       "      <td>0.521062</td>\n",
       "      <td>0.379760</td>\n",
       "      <td>0.675277</td>\n",
       "      <td>-0.272479</td>\n",
       "      <td>0.402648</td>\n",
       "      <td>-0.057378</td>\n",
       "      <td>0.184452</td>\n",
       "      <td>0.217882</td>\n",
       "      <td>0.335407</td>\n",
       "      <td>0.046215</td>\n",
       "      <td>-0.723375</td>\n",
       "      <td>0.126813</td>\n",
       "      <td>0.527359</td>\n",
       "      <td>-0.443565</td>\n",
       "      <td>0.399089</td>\n",
       "      <td>0.710649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -0.622507  0.407368 -0.214510  0.806359  0.487520  0.519140  0.658717   \n",
       "1 -0.665546  0.332077  0.306328  0.845157  0.378590  0.498875  0.787913   \n",
       "2 -0.626102  0.192291  0.366238  0.772201  0.527536  0.457795  0.799344   \n",
       "3 -0.629605  0.270956  0.066702  0.848207  0.414008  0.404725  0.706411   \n",
       "4 -0.630929  0.530425  0.144248  0.856890  0.521062  0.379760  0.675277   \n",
       "\n",
       "          7         8         9        10        11        12        13  \\\n",
       "0 -0.061238  0.400183 -0.082774 -0.018668  0.477773  0.408408 -0.225123   \n",
       "1 -0.097629  0.310637  0.109444  0.281932 -0.021151  0.600632 -0.095384   \n",
       "2 -0.286114  0.254085 -0.166339  0.422577  0.313438  0.684595 -0.085658   \n",
       "3 -0.456213  0.367470 -0.307985  0.075451  0.113194  0.717568  0.021446   \n",
       "4 -0.272479  0.402648 -0.057378  0.184452  0.217882  0.335407  0.046215   \n",
       "\n",
       "         14        15        16        17        18        19  \n",
       "0 -0.746708 -0.216064  0.514340 -0.219927  0.484705  0.739327  \n",
       "1 -0.797391  0.482128  0.595226 -0.268552  0.302145  0.791331  \n",
       "2 -0.773745  0.030570  0.324075 -0.544847  0.326352  0.835627  \n",
       "3 -0.628271  0.075955  0.325393 -0.511374  0.504569  0.697474  \n",
       "4 -0.723375  0.126813  0.527359 -0.443565  0.399089  0.710649  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_autoencoder().reduced_dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    ax = plt.subplot(1, 4, 1)\n",
    "    plt.plot(history.history[\"loss\"])\n",
    "    plt.title(\"Train loss\")\n",
    "    ax = plt.subplot(1, 4, 2)\n",
    "    plt.plot(history.history[\"val_loss\"])\n",
    "    plt.title(\"Test loss\")\n",
    "    ax = plt.subplot(1, 4, 3)\n",
    "    plt.plot(history.history[\"val_acc\"])\n",
    "    plt.title(\"Accuracy\")\n",
    "    ax = plt.subplot(1, 4, 4)\n",
    "    plt.plot(history.history[\"val_mae\"])\n",
    "    plt.title(\"Mean Absolute Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loss vs Epoch\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model_regressor = KerasRegressor(simple_autoencoder().model, verbose=1, batch_size=10, epochs=10)\n",
    "#define the grid search parameters\n",
    "#dimensions = []\n",
    "#dropout = []\n",
    "#batch_size = [10,20]\n",
    "loss = ['mean_squared_error']\n",
    "optimizer = ['Adam', 'SGD', 'RMSprop']\n",
    "epochs = [10, 15]\n",
    "scoring = ['acc']\n",
    "\n",
    "param_grid = dict(optimizer=optimizer,score = scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anjalichauhan/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:278: UserWarning: The total space of parameters 3 is smaller than n_iter=100. Running 3 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 644)]             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 20)                12900     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 644)               13524     \n",
      "=================================================================\n",
      "Total params: 26,424\n",
      "Trainable params: 26,424\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "32/32 [==============================] - 1s 2ms/step - loss: 0.2143 - acc: 0.0042 - mae: 0.4084\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0198 - acc: 0.1338 - mae: 0.0984\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0095 - acc: 0.1838 - mae: 0.0677\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0090 - acc: 0.1821 - mae: 0.0651\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0092 - acc: 0.1960 - mae: 0.0657\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0092 - acc: 0.1758 - mae: 0.0655\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0090 - acc: 0.2082 - mae: 0.0648\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0088 - acc: 0.1945 - mae: 0.0639\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0084 - acc: 0.2036 - mae: 0.0620\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0088 - acc: 0.1885 - mae: 0.0641\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0132 - acc: 0.0500 - mae: 0.0816\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 644)]             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 20)                12900     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 644)               13524     \n",
      "=================================================================\n",
      "Total params: 26,424\n",
      "Trainable params: 26,424\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "32/32 [==============================] - 1s 1ms/step - loss: 0.2115 - acc: 0.0109 - mae: 0.3995 \n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0217 - acc: 0.1905 - mae: 0.1046\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0110 - acc: 0.1491 - mae: 0.0733\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0111 - acc: 0.1853 - mae: 0.0725\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0104 - acc: 0.1851 - mae: 0.0705\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0098 - acc: 0.1693 - mae: 0.0682\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0094 - acc: 0.1938 - mae: 0.0659\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0092 - acc: 0.1576 - mae: 0.0659\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0098 - acc: 0.1906 - mae: 0.0675\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0091 - acc: 0.1573 - mae: 0.0653\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0083 - acc: 0.1625 - mae: 0.0627\n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 644)]             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 20)                12900     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 644)               13524     \n",
      "=================================================================\n",
      "Total params: 26,424\n",
      "Trainable params: 26,424\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "32/32 [==============================] - 1s 1ms/step - loss: 0.2110 - acc: 0.0000e+00 - mae: 0.4031\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 996us/step - loss: 0.0191 - acc: 0.0169 - mae: 0.0988\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0110 - acc: 0.1782 - mae: 0.0727\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0107 - acc: 0.2065 - mae: 0.0714\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0108 - acc: 0.1599 - mae: 0.0708\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0104 - acc: 0.1429 - mae: 0.0700\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0101 - acc: 0.1522 - mae: 0.0690\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0096 - acc: 0.1849 - mae: 0.0668\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0098 - acc: 0.1438 - mae: 0.0674\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0091 - acc: 0.1668 - mae: 0.0650\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0075 - acc: 0.2750 - mae: 0.0599\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 644)]             0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 20)                12900     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 644)               13524     \n",
      "=================================================================\n",
      "Total params: 26,424\n",
      "Trainable params: 26,424\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "32/32 [==============================] - 1s 1ms/step - loss: 0.2000 - acc: 0.0087 - mae: 0.3874 \n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0187 - acc: 0.1603 - mae: 0.0979\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0109 - acc: 0.2021 - mae: 0.0730\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0106 - acc: 0.1624 - mae: 0.0715\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0109 - acc: 0.1703 - mae: 0.0715\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0102 - acc: 0.1537 - mae: 0.0691\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0101 - acc: 0.1737 - mae: 0.0687\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0095 - acc: 0.1646 - mae: 0.0669\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0098 - acc: 0.1432 - mae: 0.0677\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0090 - acc: 0.1607 - mae: 0.0651\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0070 - acc: 0.2125 - mae: 0.0568\n",
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 644)]             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 20)                12900     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 644)               13524     \n",
      "=================================================================\n",
      "Total params: 26,424\n",
      "Trainable params: 26,424\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2169 - acc: 0.0097 - mae: 0.4074 \n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0229 - acc: 0.0107 - mae: 0.1067\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0108 - acc: 0.1258 - mae: 0.0722\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0101 - acc: 0.1920 - mae: 0.0694\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0100 - acc: 0.2126 - mae: 0.0689\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0094 - acc: 0.1806 - mae: 0.0667\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0093 - acc: 0.1913 - mae: 0.0665\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0088 - acc: 0.1742 - mae: 0.0641\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0089 - acc: 0.1853 - mae: 0.0652\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0088 - acc: 0.1991 - mae: 0.0643\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0116 - acc: 0.1500 - mae: 0.0734\n",
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 644)]             0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 20)                12900     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 644)               13524     \n",
      "=================================================================\n",
      "Total params: 26,424\n",
      "Trainable params: 26,424\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "32/32 [==============================] - 1s 1ms/step - loss: 0.2998 - acc: 2.8715e-04 - mae: 0.5081\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2890 - acc: 0.0012 - mae: 0.5008\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2827 - acc: 0.0078 - mae: 0.4961\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2756 - acc: 0.0000e+00 - mae: 0.4901\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2727 - acc: 0.0023 - mae: 0.4877\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2694 - acc: 7.1311e-04 - mae: 0.4849\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2643 - acc: 7.1311e-04 - mae: 0.4806\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2586 - acc: 0.0068 - mae: 0.4743\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2594 - acc: 0.0028 - mae: 0.4747\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2499 - acc: 0.0000e+00 - mae: 0.4649\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2500 - acc: 0.0000e+00 - mae: 0.4590\n",
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, 644)]             0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 20)                12900     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 644)               13524     \n",
      "=================================================================\n",
      "Total params: 26,424\n",
      "Trainable params: 26,424\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2917 - acc: 0.0061 - mae: 0.5010 \n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2806 - acc: 3.8816e-04 - mae: 0.4932\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2708 - acc: 0.0045 - mae: 0.4865\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2658 - acc: 8.2966e-04 - mae: 0.4823\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2629 - acc: 0.0045 - mae: 0.4797\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2601 - acc: 0.0028 - mae: 0.4764\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2557 - acc: 0.0021 - mae: 0.4719\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2553 - acc: 0.0000e+00 - mae: 0.4703\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2467 - acc: 0.0000e+00 - mae: 0.4615\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2456 - acc: 0.0000e+00 - mae: 0.4585\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2454 - acc: 0.0000e+00 - mae: 0.4603\n",
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        [(None, 644)]             0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 20)                12900     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 644)               13524     \n",
      "=================================================================\n",
      "Total params: 26,424\n",
      "Trainable params: 26,424\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "32/32 [==============================] - 1s 1ms/step - loss: 0.2861 - acc: 0.0000e+00 - mae: 0.5006\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2795 - acc: 0.0000e+00 - mae: 0.4957\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2710 - acc: 0.0000e+00 - mae: 0.4871\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2692 - acc: 0.0000e+00 - mae: 0.4866\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2684 - acc: 0.0000e+00 - mae: 0.4844\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2646 - acc: 0.0000e+00 - mae: 0.4803\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2592 - acc: 0.0000e+00 - mae: 0.4747\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2610 - acc: 0.0000e+00 - mae: 0.4756\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2600 - acc: 0.0000e+00 - mae: 0.4743\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2477 - acc: 0.0000e+00 - mae: 0.4617\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2461 - acc: 0.0000e+00 - mae: 0.4616\n",
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        [(None, 644)]             0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 20)                12900     \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 644)               13524     \n",
      "=================================================================\n",
      "Total params: 26,424\n",
      "Trainable params: 26,424\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "32/32 [==============================] - 1s 1ms/step - loss: 0.2961 - acc: 0.0021 - mae: 0.5068 \n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2910 - acc: 0.0094 - mae: 0.5047\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2802 - acc: 0.0016 - mae: 0.4942\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2779 - acc: 0.0025 - mae: 0.4931\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2711 - acc: 0.0000e+00 - mae: 0.4868\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2647 - acc: 0.0000e+00 - mae: 0.4814\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2666 - acc: 0.0000e+00 - mae: 0.4820\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2608 - acc: 0.0000e+00 - mae: 0.4762\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2579 - acc: 0.0000e+00 - mae: 0.4730\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2510 - acc: 0.0000e+00 - mae: 0.4649\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2494 - acc: 0.0000e+00 - mae: 0.4656\n",
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        [(None, 644)]             0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 20)                12900     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 644)               13524     \n",
      "=================================================================\n",
      "Total params: 26,424\n",
      "Trainable params: 26,424\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "32/32 [==============================] - 1s 1ms/step - loss: 0.2867 - acc: 0.0000e+00 - mae: 0.5019\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2748 - acc: 3.8816e-04 - mae: 0.4922\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2750 - acc: 0.0154 - mae: 0.4928\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2712 - acc: 0.0087 - mae: 0.4888\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2632 - acc: 0.0157 - mae: 0.4819\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2631 - acc: 0.0057 - mae: 0.4812\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2583 - acc: 0.0076 - mae: 0.4765\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2601 - acc: 0.0108 - mae: 0.4770\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.2525 - acc: 0.0204 - mae: 0.4690\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2500 - acc: 0.0153 - mae: 0.4654\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2536 - acc: 0.0000e+00 - mae: 0.4669\n",
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_14 (InputLayer)        [(None, 644)]             0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 20)                12900     \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 644)               13524     \n",
      "=================================================================\n",
      "Total params: 26,424\n",
      "Trainable params: 26,424\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "32/32 [==============================] - 1s 1ms/step - loss: 0.1669 - acc: 3.8816e-04 - mae: 0.3350\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0115 - acc: 0.1659 - mae: 0.0743\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0095 - acc: 0.2089 - mae: 0.0679\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0088 - acc: 0.2067 - mae: 0.0650\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0089 - acc: 0.2043 - mae: 0.0654\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0088 - acc: 0.1853 - mae: 0.0656\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0094 - acc: 0.1993 - mae: 0.0671\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0087 - acc: 0.1712 - mae: 0.0646\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0094 - acc: 0.1685 - mae: 0.0656\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0089 - acc: 0.1855 - mae: 0.0647\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0125 - acc: 0.1000 - mae: 0.0796\n",
      "Model: \"model_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_15 (InputLayer)        [(None, 644)]             0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 20)                12900     \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 644)               13524     \n",
      "=================================================================\n",
      "Total params: 26,424\n",
      "Trainable params: 26,424\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "32/32 [==============================] - 1s 3ms/step - loss: 0.1627 - acc: 0.0000e+00 - mae: 0.3305\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0146 - acc: 0.0140 - mae: 0.0845\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0108 - acc: 0.1702 - mae: 0.0725\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0105 - acc: 0.1850 - mae: 0.0708\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0093 - acc: 0.1989 - mae: 0.0669\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0096 - acc: 0.1591 - mae: 0.0677\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0103 - acc: 0.1832 - mae: 0.0706\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0101 - acc: 0.1671 - mae: 0.0701\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0098 - acc: 0.1831 - mae: 0.0682\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0101 - acc: 0.1505 - mae: 0.0706\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0085 - acc: 0.1625 - mae: 0.0634\n",
      "Model: \"model_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_16 (InputLayer)        [(None, 644)]             0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 20)                12900     \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 644)               13524     \n",
      "=================================================================\n",
      "Total params: 26,424\n",
      "Trainable params: 26,424\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "32/32 [==============================] - 1s 1ms/step - loss: 0.1686 - acc: 0.0111 - mae: 0.3415 \n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0137 - acc: 0.0196 - mae: 0.0823\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0113 - acc: 0.1463 - mae: 0.0740\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0100 - acc: 0.1386 - mae: 0.0695\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0103 - acc: 0.1477 - mae: 0.0703\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0106 - acc: 0.1671 - mae: 0.0711\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0103 - acc: 0.1364 - mae: 0.0708\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0093 - acc: 0.1656 - mae: 0.0672\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0106 - acc: 0.1476 - mae: 0.0715\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0099 - acc: 0.1600 - mae: 0.0698\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0076 - acc: 0.2750 - mae: 0.0610\n",
      "Model: \"model_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_17 (InputLayer)        [(None, 644)]             0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 20)                12900     \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 644)               13524     \n",
      "=================================================================\n",
      "Total params: 26,424\n",
      "Trainable params: 26,424\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "32/32 [==============================] - 1s 1ms/step - loss: 0.1641 - acc: 0.0043 - mae: 0.3377 \n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0132 - acc: 0.1544 - mae: 0.0815\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0114 - acc: 0.1683 - mae: 0.0742\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0097 - acc: 0.1903 - mae: 0.0687\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0109 - acc: 0.1527 - mae: 0.0737\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0105 - acc: 0.1703 - mae: 0.0709\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0104 - acc: 0.1667 - mae: 0.0716\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0100 - acc: 0.1624 - mae: 0.0697\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0102 - acc: 0.1605 - mae: 0.0702\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0102 - acc: 0.1575 - mae: 0.0705\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0072 - acc: 0.2125 - mae: 0.0581\n",
      "Model: \"model_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_18 (InputLayer)        [(None, 644)]             0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 20)                12900     \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 644)               13524     \n",
      "=================================================================\n",
      "Total params: 26,424\n",
      "Trainable params: 26,424\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "32/32 [==============================] - 2s 9ms/step - loss: 0.1681 - acc: 0.0021 - mae: 0.3419\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0134 - acc: 0.0241 - mae: 0.0812\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0101 - acc: 0.1630 - mae: 0.0702\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0095 - acc: 0.1987 - mae: 0.0676\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0094 - acc: 0.1904 - mae: 0.0673\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0095 - acc: 0.1956 - mae: 0.0691\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0092 - acc: 0.1998 - mae: 0.0664\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0094 - acc: 0.1551 - mae: 0.0682\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0089 - acc: 0.1643 - mae: 0.0656\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0093 - acc: 0.1532 - mae: 0.0679\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0112 - acc: 0.1500 - mae: 0.0710\n",
      "Model: \"model_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_19 (InputLayer)        [(None, 644)]             0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 20)                12900     \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 644)               13524     \n",
      "=================================================================\n",
      "Total params: 26,424\n",
      "Trainable params: 26,424\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "40/40 [==============================] - 1s 2ms/step - loss: 0.1593 - acc: 0.0028 - mae: 0.3257\n",
      "Epoch 2/10\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.0114 - acc: 0.1788 - mae: 0.0750\n",
      "Epoch 3/10\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.0107 - acc: 0.1722 - mae: 0.0725\n",
      "Epoch 4/10\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.0097 - acc: 0.1574 - mae: 0.0687\n",
      "Epoch 5/10\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.0095 - acc: 0.1934 - mae: 0.0676\n",
      "Epoch 6/10\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.0100 - acc: 0.1520 - mae: 0.0696\n",
      "Epoch 7/10\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.0094 - acc: 0.1592 - mae: 0.0668\n",
      "Epoch 8/10\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.0098 - acc: 0.1664 - mae: 0.0686\n",
      "Epoch 9/10\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.0092 - acc: 0.1852 - mae: 0.0667\n",
      "Epoch 10/10\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.0096 - acc: 0.1882 - mae: 0.0678\n"
     ]
    }
   ],
   "source": [
    "random_search = RandomizedSearchCV(estimator=model_regressor, param_distributions=param_grid, n_iter=100)\n",
    "random_search.fit(training_set_scaled, training_set_scaled)\n",
    "\n",
    "random_best_parameters = random_search.best_params_\n",
    "random_best_accuracy = random_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'acc', 'optimizer': 'RMSprop'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.009411393105983734"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
