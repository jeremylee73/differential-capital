{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Conv1D, MaxPooling1D, UpSampling1D, BatchNormalization, LSTM, RepeatVector\n",
    "from keras.models import Model\n",
    "from keras.models import model_from_json\n",
    "from keras import regularizers\n",
    "import datetime\n",
    "import time\n",
    "import requests as req\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Data/risk_adj_returns.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-dee4c8416d89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m## Risk Adjusted Returns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../Data/risk_adj_returns.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdrop_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcompression\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"infer\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# 1) try standard library Pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Data/risk_adj_returns.pkl'"
     ]
    }
   ],
   "source": [
    "## USE: full log(returns)/returns dataframe\n",
    "## Risk Adjusted Returns\n",
    "\n",
    "df = pd.read_pickle(\"../Data/risk_adj_returns.pkl\").iloc[1:]\n",
    "\n",
    "drop_columns = []\n",
    "for col in df.columns:\n",
    "    if df[col].isnull().all() == True:\n",
    "        drop_columns.append(col)\n",
    "        \n",
    "df.drop(columns=drop_columns, inplace=True)\n",
    "\n",
    "# df['pct_change'] = df.close.pct_change()\n",
    "# df['log_ret'] = np.log(df.close) - np.log(df.close.shift(1))\n",
    "# df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.dropna(how='any',axis=0) #All rows have NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_investable(t, n_rows):\n",
    "    \"Find stocks in investable universe at time t\\\n",
    "    (stocks in the S&P500 that have prices recorded for the last n_rows days)\"\n",
    "    \n",
    "    df_investable = df.copy(deep = True).sort_index(ascending = False)\n",
    "    \n",
    "    #add 1 date to get the test features in investable\n",
    "    t = t + pd.DateOffset(1)\n",
    "    \n",
    "    #if t is now a non-trading day, advance until we reach a valid trading day\n",
    "    while t not in df_investable.index:\n",
    "        t = t + pd.DateOffset(1)\n",
    "    \n",
    "    t_index = df_investable.index.get_loc(t)\n",
    "    \n",
    "    #take n_rows worth of data upto time specified\n",
    "    df_investable = df_investable.iloc[t_index + 1:t_index + n_rows + 1]\n",
    "    \n",
    "    #find all stocks that exist in the S&P at this time period\n",
    "    investable_universe = []\n",
    "    for col in df_investable.columns:\n",
    "        if ~df_investable[col].iloc[:n_rows].isna().any():\n",
    "            investable_universe.append(col)\n",
    "        \n",
    "    df_investable = df_investable[investable_universe]\n",
    "    \n",
    "    return df_investable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_investable(pd.to_datetime('2018-05-11'), 200)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "tts = train_test_split(df, test_size=0.2, shuffle=False)\n",
    "train = tts[0]\n",
    "test = tts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>916328</th>\n",
       "      <th>936365</th>\n",
       "      <th>905271</th>\n",
       "      <th>905113</th>\n",
       "      <th>905802</th>\n",
       "      <th>905425</th>\n",
       "      <th>906156</th>\n",
       "      <th>916305</th>\n",
       "      <th>992816</th>\n",
       "      <th>921093</th>\n",
       "      <th>...</th>\n",
       "      <th>311917</th>\n",
       "      <th>69568X</th>\n",
       "      <th>543755</th>\n",
       "      <th>77463M</th>\n",
       "      <th>29235J</th>\n",
       "      <th>131745</th>\n",
       "      <th>69487D</th>\n",
       "      <th>68157P</th>\n",
       "      <th>9110RA</th>\n",
       "      <th>292703</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-24</th>\n",
       "      <td>0.086843</td>\n",
       "      <td>-0.060878</td>\n",
       "      <td>-0.046227</td>\n",
       "      <td>-0.035378</td>\n",
       "      <td>-0.040604</td>\n",
       "      <td>-0.107052</td>\n",
       "      <td>-0.018406</td>\n",
       "      <td>0.023516</td>\n",
       "      <td>-0.081765</td>\n",
       "      <td>-0.048425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039346</td>\n",
       "      <td>-0.069803</td>\n",
       "      <td>-0.049203</td>\n",
       "      <td>-0.046054</td>\n",
       "      <td>-0.065294</td>\n",
       "      <td>-0.037852</td>\n",
       "      <td>-0.053071</td>\n",
       "      <td>-0.057027</td>\n",
       "      <td>-0.058082</td>\n",
       "      <td>-0.098818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-23</th>\n",
       "      <td>-0.101296</td>\n",
       "      <td>0.004714</td>\n",
       "      <td>-0.050551</td>\n",
       "      <td>-0.061059</td>\n",
       "      <td>-0.046081</td>\n",
       "      <td>-0.046885</td>\n",
       "      <td>-0.066144</td>\n",
       "      <td>0.003933</td>\n",
       "      <td>-0.045001</td>\n",
       "      <td>-0.077185</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031648</td>\n",
       "      <td>-0.029116</td>\n",
       "      <td>-0.012979</td>\n",
       "      <td>0.030010</td>\n",
       "      <td>-0.026629</td>\n",
       "      <td>-0.066076</td>\n",
       "      <td>-0.050037</td>\n",
       "      <td>-0.054079</td>\n",
       "      <td>-0.012997</td>\n",
       "      <td>-0.026405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-22</th>\n",
       "      <td>-0.061533</td>\n",
       "      <td>-0.024552</td>\n",
       "      <td>-0.054732</td>\n",
       "      <td>-0.032841</td>\n",
       "      <td>-0.009791</td>\n",
       "      <td>-0.064273</td>\n",
       "      <td>-0.048052</td>\n",
       "      <td>-0.057855</td>\n",
       "      <td>-0.064134</td>\n",
       "      <td>0.078599</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.067072</td>\n",
       "      <td>-0.027526</td>\n",
       "      <td>-0.042841</td>\n",
       "      <td>-0.011384</td>\n",
       "      <td>-0.040361</td>\n",
       "      <td>-0.031047</td>\n",
       "      <td>-0.033746</td>\n",
       "      <td>-0.047707</td>\n",
       "      <td>-0.000439</td>\n",
       "      <td>-0.116270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-19</th>\n",
       "      <td>-0.062797</td>\n",
       "      <td>-0.016724</td>\n",
       "      <td>-0.052486</td>\n",
       "      <td>-0.061087</td>\n",
       "      <td>-0.114825</td>\n",
       "      <td>-0.091019</td>\n",
       "      <td>-0.108066</td>\n",
       "      <td>-0.039338</td>\n",
       "      <td>-0.055672</td>\n",
       "      <td>-0.027002</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034127</td>\n",
       "      <td>-0.007381</td>\n",
       "      <td>-0.016237</td>\n",
       "      <td>-0.017345</td>\n",
       "      <td>-0.038328</td>\n",
       "      <td>-0.000300</td>\n",
       "      <td>-0.037129</td>\n",
       "      <td>0.011143</td>\n",
       "      <td>0.036480</td>\n",
       "      <td>-0.025287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-18</th>\n",
       "      <td>-0.073463</td>\n",
       "      <td>0.006139</td>\n",
       "      <td>-0.107247</td>\n",
       "      <td>-0.065964</td>\n",
       "      <td>-0.062529</td>\n",
       "      <td>-0.117557</td>\n",
       "      <td>-0.081579</td>\n",
       "      <td>-0.036407</td>\n",
       "      <td>-0.043482</td>\n",
       "      <td>-0.101182</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.083494</td>\n",
       "      <td>-0.048454</td>\n",
       "      <td>-0.031065</td>\n",
       "      <td>-0.112387</td>\n",
       "      <td>-0.059193</td>\n",
       "      <td>-0.065166</td>\n",
       "      <td>-0.033061</td>\n",
       "      <td>-0.057664</td>\n",
       "      <td>-0.052179</td>\n",
       "      <td>-0.061829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 651 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              916328    936365    905271    905113    905802    905425  \\\n",
       "date                                                                     \n",
       "2018-01-24  0.086843 -0.060878 -0.046227 -0.035378 -0.040604 -0.107052   \n",
       "2018-01-23 -0.101296  0.004714 -0.050551 -0.061059 -0.046081 -0.046885   \n",
       "2018-01-22 -0.061533 -0.024552 -0.054732 -0.032841 -0.009791 -0.064273   \n",
       "2018-01-19 -0.062797 -0.016724 -0.052486 -0.061087 -0.114825 -0.091019   \n",
       "2018-01-18 -0.073463  0.006139 -0.107247 -0.065964 -0.062529 -0.117557   \n",
       "\n",
       "              906156    916305    992816    921093  ...    311917    69568X  \\\n",
       "date                                                ...                       \n",
       "2018-01-24 -0.018406  0.023516 -0.081765 -0.048425  ...  0.039346 -0.069803   \n",
       "2018-01-23 -0.066144  0.003933 -0.045001 -0.077185  ... -0.031648 -0.029116   \n",
       "2018-01-22 -0.048052 -0.057855 -0.064134  0.078599  ... -0.067072 -0.027526   \n",
       "2018-01-19 -0.108066 -0.039338 -0.055672 -0.027002  ... -0.034127 -0.007381   \n",
       "2018-01-18 -0.081579 -0.036407 -0.043482 -0.101182  ... -0.083494 -0.048454   \n",
       "\n",
       "              543755    77463M    29235J    131745    69487D    68157P  \\\n",
       "date                                                                     \n",
       "2018-01-24 -0.049203 -0.046054 -0.065294 -0.037852 -0.053071 -0.057027   \n",
       "2018-01-23 -0.012979  0.030010 -0.026629 -0.066076 -0.050037 -0.054079   \n",
       "2018-01-22 -0.042841 -0.011384 -0.040361 -0.031047 -0.033746 -0.047707   \n",
       "2018-01-19 -0.016237 -0.017345 -0.038328 -0.000300 -0.037129  0.011143   \n",
       "2018-01-18 -0.031065 -0.112387 -0.059193 -0.065166 -0.033061 -0.057664   \n",
       "\n",
       "              9110RA    292703  \n",
       "date                            \n",
       "2018-01-24 -0.058082 -0.098818  \n",
       "2018-01-23 -0.012997 -0.026405  \n",
       "2018-01-22 -0.000439 -0.116270  \n",
       "2018-01-19  0.036480 -0.025287  \n",
       "2018-01-18 -0.052179 -0.061829  \n",
       "\n",
       "[5 rows x 651 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>916328</th>\n",
       "      <th>936365</th>\n",
       "      <th>905271</th>\n",
       "      <th>905113</th>\n",
       "      <th>905802</th>\n",
       "      <th>905425</th>\n",
       "      <th>906156</th>\n",
       "      <th>916305</th>\n",
       "      <th>992816</th>\n",
       "      <th>921093</th>\n",
       "      <th>...</th>\n",
       "      <th>311917</th>\n",
       "      <th>69568X</th>\n",
       "      <th>543755</th>\n",
       "      <th>77463M</th>\n",
       "      <th>29235J</th>\n",
       "      <th>131745</th>\n",
       "      <th>69487D</th>\n",
       "      <th>68157P</th>\n",
       "      <th>9110RA</th>\n",
       "      <th>292703</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-12-22</th>\n",
       "      <td>-0.078807</td>\n",
       "      <td>-0.084029</td>\n",
       "      <td>-0.066136</td>\n",
       "      <td>-0.047499</td>\n",
       "      <td>-0.014851</td>\n",
       "      <td>-0.093806</td>\n",
       "      <td>-0.049523</td>\n",
       "      <td>-0.065613</td>\n",
       "      <td>-0.045515</td>\n",
       "      <td>-0.058053</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069039</td>\n",
       "      <td>-0.069658</td>\n",
       "      <td>-0.037094</td>\n",
       "      <td>-0.032531</td>\n",
       "      <td>-0.054486</td>\n",
       "      <td>-0.086048</td>\n",
       "      <td>-0.045697</td>\n",
       "      <td>-0.039031</td>\n",
       "      <td>-0.036931</td>\n",
       "      <td>-0.066122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-21</th>\n",
       "      <td>-0.085026</td>\n",
       "      <td>-0.045442</td>\n",
       "      <td>-0.064580</td>\n",
       "      <td>0.014185</td>\n",
       "      <td>0.089568</td>\n",
       "      <td>-0.133506</td>\n",
       "      <td>-0.056680</td>\n",
       "      <td>-0.037322</td>\n",
       "      <td>-0.036900</td>\n",
       "      <td>-0.048026</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.072542</td>\n",
       "      <td>-0.020915</td>\n",
       "      <td>-0.060163</td>\n",
       "      <td>-0.009458</td>\n",
       "      <td>-0.068158</td>\n",
       "      <td>-0.010737</td>\n",
       "      <td>-0.046480</td>\n",
       "      <td>-0.026167</td>\n",
       "      <td>-0.017515</td>\n",
       "      <td>-0.077143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-20</th>\n",
       "      <td>-0.063545</td>\n",
       "      <td>-0.027817</td>\n",
       "      <td>-0.065008</td>\n",
       "      <td>-0.001230</td>\n",
       "      <td>-0.014661</td>\n",
       "      <td>-0.089004</td>\n",
       "      <td>-0.074594</td>\n",
       "      <td>-0.032630</td>\n",
       "      <td>-0.047993</td>\n",
       "      <td>-0.087811</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063648</td>\n",
       "      <td>-0.046587</td>\n",
       "      <td>-0.052069</td>\n",
       "      <td>0.000284</td>\n",
       "      <td>-0.025493</td>\n",
       "      <td>0.057820</td>\n",
       "      <td>-0.040007</td>\n",
       "      <td>-0.038349</td>\n",
       "      <td>-0.051338</td>\n",
       "      <td>-0.067136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-19</th>\n",
       "      <td>-0.050861</td>\n",
       "      <td>-0.036636</td>\n",
       "      <td>-0.063441</td>\n",
       "      <td>-0.048355</td>\n",
       "      <td>-0.068352</td>\n",
       "      <td>-0.146375</td>\n",
       "      <td>-0.071386</td>\n",
       "      <td>-0.033276</td>\n",
       "      <td>-0.069767</td>\n",
       "      <td>-0.073746</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.055227</td>\n",
       "      <td>-0.075748</td>\n",
       "      <td>-0.034308</td>\n",
       "      <td>-0.064443</td>\n",
       "      <td>-0.058609</td>\n",
       "      <td>-0.013522</td>\n",
       "      <td>-0.052349</td>\n",
       "      <td>-0.074513</td>\n",
       "      <td>-0.041667</td>\n",
       "      <td>-0.023936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-18</th>\n",
       "      <td>-0.014909</td>\n",
       "      <td>0.075835</td>\n",
       "      <td>-0.033337</td>\n",
       "      <td>-0.019492</td>\n",
       "      <td>-0.041249</td>\n",
       "      <td>-0.148475</td>\n",
       "      <td>-0.023194</td>\n",
       "      <td>-0.033592</td>\n",
       "      <td>-0.013466</td>\n",
       "      <td>-0.094124</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033586</td>\n",
       "      <td>-0.058820</td>\n",
       "      <td>-0.016263</td>\n",
       "      <td>-0.123514</td>\n",
       "      <td>0.001390</td>\n",
       "      <td>-0.078713</td>\n",
       "      <td>-0.037056</td>\n",
       "      <td>-0.002008</td>\n",
       "      <td>0.040300</td>\n",
       "      <td>-0.048953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 651 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              916328    936365    905271    905113    905802    905425  \\\n",
       "date                                                                     \n",
       "2017-12-22 -0.078807 -0.084029 -0.066136 -0.047499 -0.014851 -0.093806   \n",
       "2017-12-21 -0.085026 -0.045442 -0.064580  0.014185  0.089568 -0.133506   \n",
       "2017-12-20 -0.063545 -0.027817 -0.065008 -0.001230 -0.014661 -0.089004   \n",
       "2017-12-19 -0.050861 -0.036636 -0.063441 -0.048355 -0.068352 -0.146375   \n",
       "2017-12-18 -0.014909  0.075835 -0.033337 -0.019492 -0.041249 -0.148475   \n",
       "\n",
       "              906156    916305    992816    921093  ...    311917    69568X  \\\n",
       "date                                                ...                       \n",
       "2017-12-22 -0.049523 -0.065613 -0.045515 -0.058053  ... -0.069039 -0.069658   \n",
       "2017-12-21 -0.056680 -0.037322 -0.036900 -0.048026  ... -0.072542 -0.020915   \n",
       "2017-12-20 -0.074594 -0.032630 -0.047993 -0.087811  ... -0.063648 -0.046587   \n",
       "2017-12-19 -0.071386 -0.033276 -0.069767 -0.073746  ... -0.055227 -0.075748   \n",
       "2017-12-18 -0.023194 -0.033592 -0.013466 -0.094124  ... -0.033586 -0.058820   \n",
       "\n",
       "              543755    77463M    29235J    131745    69487D    68157P  \\\n",
       "date                                                                     \n",
       "2017-12-22 -0.037094 -0.032531 -0.054486 -0.086048 -0.045697 -0.039031   \n",
       "2017-12-21 -0.060163 -0.009458 -0.068158 -0.010737 -0.046480 -0.026167   \n",
       "2017-12-20 -0.052069  0.000284 -0.025493  0.057820 -0.040007 -0.038349   \n",
       "2017-12-19 -0.034308 -0.064443 -0.058609 -0.013522 -0.052349 -0.074513   \n",
       "2017-12-18 -0.016263 -0.123514  0.001390 -0.078713 -0.037056 -0.002008   \n",
       "\n",
       "              9110RA    292703  \n",
       "date                            \n",
       "2017-12-22 -0.036931 -0.066122  \n",
       "2017-12-21 -0.017515 -0.077143  \n",
       "2017-12-20 -0.051338 -0.067136  \n",
       "2017-12-19 -0.041667 -0.023936  \n",
       "2017-12-18  0.040300 -0.048953  \n",
       "\n",
       "[5 rows x 651 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>641</th>\n",
       "      <th>642</th>\n",
       "      <th>643</th>\n",
       "      <th>644</th>\n",
       "      <th>645</th>\n",
       "      <th>646</th>\n",
       "      <th>647</th>\n",
       "      <th>648</th>\n",
       "      <th>649</th>\n",
       "      <th>650</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.661406</td>\n",
       "      <td>0.268924</td>\n",
       "      <td>0.592576</td>\n",
       "      <td>0.898381</td>\n",
       "      <td>0.463903</td>\n",
       "      <td>0.520905</td>\n",
       "      <td>0.428907</td>\n",
       "      <td>0.635693</td>\n",
       "      <td>0.393566</td>\n",
       "      <td>0.393625</td>\n",
       "      <td>...</td>\n",
       "      <td>0.343729</td>\n",
       "      <td>0.461508</td>\n",
       "      <td>0.571224</td>\n",
       "      <td>0.267475</td>\n",
       "      <td>0.341550</td>\n",
       "      <td>0.413940</td>\n",
       "      <td>0.525361</td>\n",
       "      <td>0.767988</td>\n",
       "      <td>0.145999</td>\n",
       "      <td>0.538990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.749938</td>\n",
       "      <td>0.412923</td>\n",
       "      <td>0.592817</td>\n",
       "      <td>0.907950</td>\n",
       "      <td>0.652900</td>\n",
       "      <td>0.707915</td>\n",
       "      <td>0.492168</td>\n",
       "      <td>0.598313</td>\n",
       "      <td>0.634965</td>\n",
       "      <td>0.433506</td>\n",
       "      <td>...</td>\n",
       "      <td>0.270439</td>\n",
       "      <td>0.505696</td>\n",
       "      <td>0.629955</td>\n",
       "      <td>0.736162</td>\n",
       "      <td>0.652450</td>\n",
       "      <td>0.429399</td>\n",
       "      <td>0.575786</td>\n",
       "      <td>0.686922</td>\n",
       "      <td>0.374450</td>\n",
       "      <td>0.391136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.663560</td>\n",
       "      <td>0.481413</td>\n",
       "      <td>0.649046</td>\n",
       "      <td>0.932297</td>\n",
       "      <td>0.718004</td>\n",
       "      <td>0.183266</td>\n",
       "      <td>0.483571</td>\n",
       "      <td>0.556848</td>\n",
       "      <td>0.555032</td>\n",
       "      <td>0.284877</td>\n",
       "      <td>...</td>\n",
       "      <td>0.512915</td>\n",
       "      <td>0.644981</td>\n",
       "      <td>0.257864</td>\n",
       "      <td>0.492219</td>\n",
       "      <td>0.624836</td>\n",
       "      <td>0.370430</td>\n",
       "      <td>0.532337</td>\n",
       "      <td>0.847700</td>\n",
       "      <td>0.250746</td>\n",
       "      <td>0.469572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.418774</td>\n",
       "      <td>0.348696</td>\n",
       "      <td>0.540830</td>\n",
       "      <td>0.937832</td>\n",
       "      <td>0.755931</td>\n",
       "      <td>0.027789</td>\n",
       "      <td>0.514947</td>\n",
       "      <td>0.570130</td>\n",
       "      <td>0.530440</td>\n",
       "      <td>0.453406</td>\n",
       "      <td>...</td>\n",
       "      <td>0.285238</td>\n",
       "      <td>0.526607</td>\n",
       "      <td>0.378104</td>\n",
       "      <td>0.271760</td>\n",
       "      <td>0.501054</td>\n",
       "      <td>0.520791</td>\n",
       "      <td>0.558434</td>\n",
       "      <td>0.748023</td>\n",
       "      <td>0.283049</td>\n",
       "      <td>0.408207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.642690</td>\n",
       "      <td>0.472731</td>\n",
       "      <td>0.592738</td>\n",
       "      <td>0.963754</td>\n",
       "      <td>0.602166</td>\n",
       "      <td>0.300429</td>\n",
       "      <td>0.443615</td>\n",
       "      <td>0.710329</td>\n",
       "      <td>0.557165</td>\n",
       "      <td>0.311118</td>\n",
       "      <td>...</td>\n",
       "      <td>0.299173</td>\n",
       "      <td>0.729053</td>\n",
       "      <td>0.713267</td>\n",
       "      <td>0.578629</td>\n",
       "      <td>0.480257</td>\n",
       "      <td>0.412264</td>\n",
       "      <td>0.443215</td>\n",
       "      <td>0.867883</td>\n",
       "      <td>0.224044</td>\n",
       "      <td>0.394355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 651 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.661406  0.268924  0.592576  0.898381  0.463903  0.520905  0.428907   \n",
       "1  0.749938  0.412923  0.592817  0.907950  0.652900  0.707915  0.492168   \n",
       "2  0.663560  0.481413  0.649046  0.932297  0.718004  0.183266  0.483571   \n",
       "3  0.418774  0.348696  0.540830  0.937832  0.755931  0.027789  0.514947   \n",
       "4  0.642690  0.472731  0.592738  0.963754  0.602166  0.300429  0.443615   \n",
       "\n",
       "        7         8         9    ...       641       642       643       644  \\\n",
       "0  0.635693  0.393566  0.393625  ...  0.343729  0.461508  0.571224  0.267475   \n",
       "1  0.598313  0.634965  0.433506  ...  0.270439  0.505696  0.629955  0.736162   \n",
       "2  0.556848  0.555032  0.284877  ...  0.512915  0.644981  0.257864  0.492219   \n",
       "3  0.570130  0.530440  0.453406  ...  0.285238  0.526607  0.378104  0.271760   \n",
       "4  0.710329  0.557165  0.311118  ...  0.299173  0.729053  0.713267  0.578629   \n",
       "\n",
       "        645       646       647       648       649       650  \n",
       "0  0.341550  0.413940  0.525361  0.767988  0.145999  0.538990  \n",
       "1  0.652450  0.429399  0.575786  0.686922  0.374450  0.391136  \n",
       "2  0.624836  0.370430  0.532337  0.847700  0.250746  0.469572  \n",
       "3  0.501054  0.520791  0.558434  0.748023  0.283049  0.408207  \n",
       "4  0.480257  0.412264  0.443215  0.867883  0.224044  0.394355  \n",
       "\n",
       "[5 rows x 651 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Scaling\n",
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "training_set_scaled = sc.fit_transform(train)\n",
    "test_set_scaled = sc.fit_transform(test)\n",
    "pd.DataFrame(training_set_scaled).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple multi-layer percepetron (MLP) autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 651)]             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 1956      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 651)               2604      \n",
      "=================================================================\n",
      "Total params: 4,560\n",
      "Trainable params: 4,560\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/250\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.3244 - val_loss: 0.3063\n",
      "Epoch 2/250\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.3210 - val_loss: 0.3045\n",
      "Epoch 3/250\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3183 - val_loss: 0.3028\n",
      "Epoch 4/250\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.3161 - val_loss: 0.3006\n",
      "Epoch 5/250\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3137 - val_loss: 0.2978\n",
      "Epoch 6/250\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3106 - val_loss: 0.2947\n",
      "Epoch 7/250\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.3072 - val_loss: 0.2914\n",
      "Epoch 8/250\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.3037 - val_loss: 0.2882\n",
      "Epoch 9/250\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.3003 - val_loss: 0.2851\n",
      "Epoch 10/250\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.2970 - val_loss: 0.2819\n",
      "Epoch 11/250\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2936 - val_loss: 0.2787\n",
      "Epoch 12/250\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.2901 - val_loss: 0.2755\n",
      "Epoch 13/250\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.2866 - val_loss: 0.2722\n",
      "Epoch 14/250\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2829 - val_loss: 0.2688\n",
      "Epoch 15/250\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.2792 - val_loss: 0.2655\n",
      "Epoch 16/250\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.2754 - val_loss: 0.2621\n",
      "Epoch 17/250\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.2717 - val_loss: 0.2587\n",
      "Epoch 18/250\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.2679 - val_loss: 0.2553\n",
      "Epoch 19/250\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.2641 - val_loss: 0.2519\n",
      "Epoch 20/250\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2604 - val_loss: 0.2485\n",
      "Epoch 21/250\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.2566 - val_loss: 0.2452\n",
      "Epoch 22/250\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.2529 - val_loss: 0.2419\n",
      "Epoch 23/250\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.2492 - val_loss: 0.2386\n",
      "Epoch 24/250\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.2456 - val_loss: 0.2354\n",
      "Epoch 25/250\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.2419 - val_loss: 0.2322\n",
      "Epoch 26/250\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.2384 - val_loss: 0.2290\n",
      "Epoch 27/250\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.2348 - val_loss: 0.2259\n",
      "Epoch 28/250\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.2313 - val_loss: 0.2228\n",
      "Epoch 29/250\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.2279 - val_loss: 0.2198\n",
      "Epoch 30/250\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.2244 - val_loss: 0.2168\n",
      "Epoch 31/250\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.2211 - val_loss: 0.2139\n",
      "Epoch 32/250\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.2177 - val_loss: 0.2110\n",
      "Epoch 33/250\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.2145 - val_loss: 0.2081\n",
      "Epoch 34/250\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.2112 - val_loss: 0.2053\n",
      "Epoch 35/250\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.2080 - val_loss: 0.2026\n",
      "Epoch 36/250\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.2049 - val_loss: 0.1999\n",
      "Epoch 37/250\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.2018 - val_loss: 0.1972\n",
      "Epoch 38/250\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.1987 - val_loss: 0.1946\n",
      "Epoch 39/250\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.1957 - val_loss: 0.1920\n",
      "Epoch 40/250\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.1928 - val_loss: 0.1895\n",
      "Epoch 41/250\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.1899 - val_loss: 0.1870\n",
      "Epoch 42/250\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1870 - val_loss: 0.1846\n",
      "Epoch 43/250\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1841 - val_loss: 0.1822\n",
      "Epoch 44/250\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.1814 - val_loss: 0.1799\n",
      "Epoch 45/250\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1786 - val_loss: 0.1776\n",
      "Epoch 46/250\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1759 - val_loss: 0.1753\n",
      "Epoch 47/250\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.1733 - val_loss: 0.1731\n",
      "Epoch 48/250\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1706 - val_loss: 0.1709\n",
      "Epoch 49/250\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1681 - val_loss: 0.1688\n",
      "Epoch 50/250\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.1655 - val_loss: 0.1667\n",
      "Epoch 51/250\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1631 - val_loss: 0.1647\n",
      "Epoch 52/250\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1606 - val_loss: 0.1627\n",
      "Epoch 53/250\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.1582 - val_loss: 0.1607\n",
      "Epoch 54/250\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1558 - val_loss: 0.1587\n",
      "Epoch 55/250\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1535 - val_loss: 0.1568\n",
      "Epoch 56/250\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1512 - val_loss: 0.1550\n",
      "Epoch 57/250\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1489 - val_loss: 0.1532\n",
      "Epoch 58/250\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.1467 - val_loss: 0.1514\n",
      "Epoch 59/250\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1446 - val_loss: 0.1496\n",
      "Epoch 60/250\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.1424 - val_loss: 0.1479\n",
      "Epoch 61/250\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1403 - val_loss: 0.1462\n",
      "Epoch 62/250\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1382 - val_loss: 0.1446\n",
      "Epoch 63/250\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1362 - val_loss: 0.1430\n",
      "Epoch 64/250\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.1342 - val_loss: 0.1414\n",
      "Epoch 65/250\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1322 - val_loss: 0.1399\n",
      "Epoch 66/250\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1303 - val_loss: 0.1383\n",
      "Epoch 67/250\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.1284 - val_loss: 0.1369\n",
      "Epoch 68/250\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.1265 - val_loss: 0.1354\n",
      "Epoch 69/250\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1247 - val_loss: 0.1340\n",
      "Epoch 70/250\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1229 - val_loss: 0.1326\n",
      "Epoch 71/250\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1211 - val_loss: 0.1312\n",
      "Epoch 72/250\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1193 - val_loss: 0.1299\n",
      "Epoch 73/250\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1176 - val_loss: 0.1286\n",
      "Epoch 74/250\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1159 - val_loss: 0.1273\n",
      "Epoch 75/250\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1143 - val_loss: 0.1261\n",
      "Epoch 76/250\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1127 - val_loss: 0.1249\n",
      "Epoch 77/250\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1111 - val_loss: 0.1237\n",
      "Epoch 78/250\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1095 - val_loss: 0.1225\n",
      "Epoch 79/250\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1079 - val_loss: 0.1214\n",
      "Epoch 80/250\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1064 - val_loss: 0.1203\n",
      "Epoch 81/250\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1049 - val_loss: 0.1192\n",
      "Epoch 82/250\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1035 - val_loss: 0.1181\n",
      "Epoch 83/250\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1020 - val_loss: 0.1171\n",
      "Epoch 84/250\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1006 - val_loss: 0.1161\n",
      "Epoch 85/250\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0992 - val_loss: 0.1151\n",
      "Epoch 86/250\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0979 - val_loss: 0.1141\n",
      "Epoch 87/250\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0965 - val_loss: 0.1132\n",
      "Epoch 88/250\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0952 - val_loss: 0.1122\n",
      "Epoch 89/250\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0939 - val_loss: 0.1113\n",
      "Epoch 90/250\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0927 - val_loss: 0.1104\n",
      "Epoch 91/250\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0914 - val_loss: 0.1096\n",
      "Epoch 92/250\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0902 - val_loss: 0.1087\n",
      "Epoch 93/250\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0890 - val_loss: 0.1079\n",
      "Epoch 94/250\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0878 - val_loss: 0.1071\n",
      "Epoch 95/250\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0866 - val_loss: 0.1063\n",
      "Epoch 96/250\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0855 - val_loss: 0.1056\n",
      "Epoch 97/250\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0844 - val_loss: 0.1048\n",
      "Epoch 98/250\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0833 - val_loss: 0.1041\n",
      "Epoch 99/250\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0822 - val_loss: 0.1034\n",
      "Epoch 100/250\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0811 - val_loss: 0.1027\n",
      "Epoch 101/250\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0801 - val_loss: 0.1021\n",
      "Epoch 102/250\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0791 - val_loss: 0.1014\n",
      "Epoch 103/250\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0781 - val_loss: 0.1008\n",
      "Epoch 104/250\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0771 - val_loss: 0.1001\n",
      "Epoch 105/250\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0761 - val_loss: 0.0995\n",
      "Epoch 106/250\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0752 - val_loss: 0.0989\n",
      "Epoch 107/250\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0742 - val_loss: 0.0984\n",
      "Epoch 108/250\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0733 - val_loss: 0.0978\n",
      "Epoch 109/250\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0724 - val_loss: 0.0973\n",
      "Epoch 110/250\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0715 - val_loss: 0.0967\n",
      "Epoch 111/250\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0706 - val_loss: 0.0962\n",
      "Epoch 112/250\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0698 - val_loss: 0.0957\n",
      "Epoch 113/250\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0689 - val_loss: 0.0952\n",
      "Epoch 114/250\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0681 - val_loss: 0.0947\n",
      "Epoch 115/250\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0673 - val_loss: 0.0943\n",
      "Epoch 116/250\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0665 - val_loss: 0.0938\n",
      "Epoch 117/250\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0657 - val_loss: 0.0934\n",
      "Epoch 118/250\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0649 - val_loss: 0.0929\n",
      "Epoch 119/250\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0641 - val_loss: 0.0925\n",
      "Epoch 120/250\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0634 - val_loss: 0.0921\n",
      "Epoch 121/250\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0626 - val_loss: 0.0917\n",
      "Epoch 122/250\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0619 - val_loss: 0.0913\n",
      "Epoch 123/250\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0612 - val_loss: 0.0909\n",
      "Epoch 124/250\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0605 - val_loss: 0.0905\n",
      "Epoch 125/250\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0598 - val_loss: 0.0902\n",
      "Epoch 126/250\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0591 - val_loss: 0.0898\n",
      "Epoch 127/250\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0585 - val_loss: 0.0895\n",
      "Epoch 128/250\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0578 - val_loss: 0.0891\n",
      "Epoch 129/250\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0572 - val_loss: 0.0888\n",
      "Epoch 130/250\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0565 - val_loss: 0.0885\n",
      "Epoch 131/250\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0559 - val_loss: 0.0882\n",
      "Epoch 132/250\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0553 - val_loss: 0.0879\n",
      "Epoch 133/250\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0547 - val_loss: 0.0876\n",
      "Epoch 134/250\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0541 - val_loss: 0.0874\n",
      "Epoch 135/250\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0535 - val_loss: 0.0871\n",
      "Epoch 136/250\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0530 - val_loss: 0.0869\n",
      "Epoch 137/250\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0524 - val_loss: 0.0866\n",
      "Epoch 138/250\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0519 - val_loss: 0.0864\n",
      "Epoch 139/250\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0513 - val_loss: 0.0862\n",
      "Epoch 140/250\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0508 - val_loss: 0.0860\n",
      "Epoch 141/250\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0503 - val_loss: 0.0857\n",
      "Epoch 142/250\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0498 - val_loss: 0.0855\n",
      "Epoch 143/250\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0493 - val_loss: 0.0853\n",
      "Epoch 144/250\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0488 - val_loss: 0.0851\n",
      "Epoch 145/250\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0483 - val_loss: 0.0849\n",
      "Epoch 146/250\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0479 - val_loss: 0.0847\n",
      "Epoch 147/250\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0474 - val_loss: 0.0846\n",
      "Epoch 148/250\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0469 - val_loss: 0.0844\n",
      "Epoch 149/250\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0465 - val_loss: 0.0842\n",
      "Epoch 150/250\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0461 - val_loss: 0.0840\n",
      "Epoch 151/250\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0456 - val_loss: 0.0839\n",
      "Epoch 152/250\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0452 - val_loss: 0.0837\n",
      "Epoch 153/250\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0448 - val_loss: 0.0835\n",
      "Epoch 154/250\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0444 - val_loss: 0.0834\n",
      "Epoch 155/250\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0440 - val_loss: 0.0832\n",
      "Epoch 156/250\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0436 - val_loss: 0.0831\n",
      "Epoch 157/250\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0432 - val_loss: 0.0829\n",
      "Epoch 158/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0428 - val_loss: 0.0828\n",
      "Epoch 159/250\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0425 - val_loss: 0.0827\n",
      "Epoch 160/250\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0421 - val_loss: 0.0826\n",
      "Epoch 161/250\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0417 - val_loss: 0.0825\n",
      "Epoch 162/250\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0414 - val_loss: 0.0823\n",
      "Epoch 163/250\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0410 - val_loss: 0.0822\n",
      "Epoch 164/250\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0407 - val_loss: 0.0821\n",
      "Epoch 165/250\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0404 - val_loss: 0.0821\n",
      "Epoch 166/250\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0400 - val_loss: 0.0820\n",
      "Epoch 167/250\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0397 - val_loss: 0.0819\n",
      "Epoch 168/250\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0394 - val_loss: 0.0818\n",
      "Epoch 169/250\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0391 - val_loss: 0.0817\n",
      "Epoch 170/250\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0388 - val_loss: 0.0817\n",
      "Epoch 171/250\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0385 - val_loss: 0.0816\n",
      "Epoch 172/250\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0382 - val_loss: 0.0815\n",
      "Epoch 173/250\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0379 - val_loss: 0.0814\n",
      "Epoch 174/250\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0376 - val_loss: 0.0814\n",
      "Epoch 175/250\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0373 - val_loss: 0.0813\n",
      "Epoch 176/250\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0370 - val_loss: 0.0813\n",
      "Epoch 177/250\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0368 - val_loss: 0.0812\n",
      "Epoch 178/250\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0365 - val_loss: 0.0812\n",
      "Epoch 179/250\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0362 - val_loss: 0.0812\n",
      "Epoch 180/250\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0360 - val_loss: 0.0811\n",
      "Epoch 181/250\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0357 - val_loss: 0.0811\n",
      "Epoch 182/250\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0355 - val_loss: 0.0811\n",
      "Epoch 183/250\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0352 - val_loss: 0.0811\n",
      "Epoch 184/250\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0350 - val_loss: 0.0810\n",
      "Epoch 185/250\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0347 - val_loss: 0.0810\n",
      "Epoch 186/250\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0345 - val_loss: 0.0810\n",
      "Epoch 187/250\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0343 - val_loss: 0.0810\n",
      "Epoch 188/250\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0341 - val_loss: 0.0810\n",
      "Epoch 189/250\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0338 - val_loss: 0.0810\n",
      "Epoch 190/250\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0336 - val_loss: 0.0810\n",
      "Epoch 191/250\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0334 - val_loss: 0.0810\n",
      "Epoch 192/250\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0332 - val_loss: 0.0810\n",
      "Epoch 193/250\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0330 - val_loss: 0.0810\n",
      "Epoch 194/250\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0328 - val_loss: 0.0810\n",
      "Epoch 195/250\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0326 - val_loss: 0.0810\n",
      "Epoch 196/250\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0324 - val_loss: 0.0810\n",
      "Epoch 197/250\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0322 - val_loss: 0.0811\n",
      "Epoch 198/250\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0320 - val_loss: 0.0811\n",
      "Epoch 199/250\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0319 - val_loss: 0.0811\n",
      "Epoch 200/250\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0317 - val_loss: 0.0811\n",
      "Epoch 201/250\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0315 - val_loss: 0.0812\n",
      "Epoch 202/250\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0313 - val_loss: 0.0812\n",
      "Epoch 203/250\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0312 - val_loss: 0.0812\n",
      "Epoch 204/250\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0310 - val_loss: 0.0812\n",
      "Epoch 205/250\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0308 - val_loss: 0.0813\n",
      "Epoch 206/250\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0307 - val_loss: 0.0813\n",
      "Epoch 207/250\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0305 - val_loss: 0.0813\n",
      "Epoch 208/250\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0304 - val_loss: 0.0814\n",
      "Epoch 209/250\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0302 - val_loss: 0.0814\n",
      "Epoch 210/250\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0301 - val_loss: 0.0814\n",
      "Epoch 211/250\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0299 - val_loss: 0.0815\n",
      "Epoch 212/250\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0298 - val_loss: 0.0815\n",
      "Epoch 213/250\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0297 - val_loss: 0.0816\n",
      "Epoch 214/250\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0295 - val_loss: 0.0816\n",
      "Epoch 215/250\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0294 - val_loss: 0.0816\n",
      "Epoch 216/250\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0293 - val_loss: 0.0817\n",
      "Epoch 217/250\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0291 - val_loss: 0.0817\n",
      "Epoch 218/250\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0290 - val_loss: 0.0818\n",
      "Epoch 219/250\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0289 - val_loss: 0.0818\n",
      "Epoch 220/250\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0288 - val_loss: 0.0819\n",
      "Epoch 221/250\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0287 - val_loss: 0.0819\n",
      "Epoch 222/250\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0285 - val_loss: 0.0819\n",
      "Epoch 223/250\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0284 - val_loss: 0.0820\n",
      "Epoch 224/250\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0283 - val_loss: 0.0820\n",
      "Epoch 225/250\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0282 - val_loss: 0.0821\n",
      "Epoch 226/250\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0281 - val_loss: 0.0821\n",
      "Epoch 227/250\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0280 - val_loss: 0.0822\n",
      "Epoch 228/250\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0279 - val_loss: 0.0822\n",
      "Epoch 229/250\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0278 - val_loss: 0.0822\n",
      "Epoch 230/250\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0277 - val_loss: 0.0823\n",
      "Epoch 231/250\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0276 - val_loss: 0.0823\n",
      "Epoch 232/250\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0275 - val_loss: 0.0824\n",
      "Epoch 233/250\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0274 - val_loss: 0.0824\n",
      "Epoch 234/250\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0273 - val_loss: 0.0825\n",
      "Epoch 235/250\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0272 - val_loss: 0.0825\n",
      "Epoch 236/250\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0271 - val_loss: 0.0826\n",
      "Epoch 237/250\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0270 - val_loss: 0.0826\n",
      "Epoch 238/250\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0269 - val_loss: 0.0827\n",
      "Epoch 239/250\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0268 - val_loss: 0.0827\n",
      "Epoch 240/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0267 - val_loss: 0.0828\n",
      "Epoch 241/250\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0266 - val_loss: 0.0828\n",
      "Epoch 242/250\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0266 - val_loss: 0.0829\n",
      "Epoch 243/250\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0265 - val_loss: 0.0829\n",
      "Epoch 244/250\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0264 - val_loss: 0.0830\n",
      "Epoch 245/250\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0263 - val_loss: 0.0830\n",
      "Epoch 246/250\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0262 - val_loss: 0.0831\n",
      "Epoch 247/250\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0261 - val_loss: 0.0831\n",
      "Epoch 248/250\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0261 - val_loss: 0.0832\n",
      "Epoch 249/250\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0260 - val_loss: 0.0832\n",
      "Epoch 250/250\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0259 - val_loss: 0.0833\n"
     ]
    }
   ],
   "source": [
    "# calculated log returns (i.e. the log of the difference between the price x+1 and price x)\n",
    "# windows of train.shape[1] consecutive returns will be produced. \n",
    "# Can be normalized with a MinMaxScaler to the range [0,1]??\n",
    "\n",
    "window_length = training_set_scaled.shape[1]\n",
    "encoding_dim = 3\n",
    "epochs = 250\n",
    "\n",
    "# compress the input to a 3-dimensional latent space. \n",
    "\n",
    "# input placeholder\n",
    "input_window = Input(shape=(window_length,))\n",
    "# encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation='tanh')(input_window) #tanh, linear, leakyrelu\n",
    "# lossy reconstruction of the input\n",
    "decoded = Dense(window_length, activation='linear')(encoded) #linear\n",
    "\n",
    "# model mapping an input to its reconstruction\n",
    "autoencoder = Model(input_window, decoded)\n",
    "\n",
    "# model mapping an input to its encoded representation\n",
    "encoder = Model(input_window, encoded)\n",
    "\n",
    "\n",
    "autoencoder.summary()\n",
    "autoencoder.compile(optimizer='adam', loss='MeanSquaredError') #MSE\n",
    "history = autoencoder.fit(training_set_scaled, training_set_scaled,\n",
    "                epochs=epochs,\n",
    "                batch_size=1024,\n",
    "                shuffle=True,\n",
    "                validation_data=(test_set_scaled, test_set_scaled))\n",
    "\n",
    "decoded_stocks = autoencoder.predict(test_set_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.999518</td>\n",
       "      <td>0.994275</td>\n",
       "      <td>0.999643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.908336</td>\n",
       "      <td>0.292791</td>\n",
       "      <td>0.841821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.998569</td>\n",
       "      <td>0.997075</td>\n",
       "      <td>0.998604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.999847</td>\n",
       "      <td>0.994274</td>\n",
       "      <td>0.998673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.963921</td>\n",
       "      <td>0.936493</td>\n",
       "      <td>0.978588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.996810</td>\n",
       "      <td>0.995636</td>\n",
       "      <td>0.983830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.998032</td>\n",
       "      <td>0.985865</td>\n",
       "      <td>0.998081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.999212</td>\n",
       "      <td>0.990228</td>\n",
       "      <td>0.997315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.998383</td>\n",
       "      <td>0.989224</td>\n",
       "      <td>0.995926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.995750</td>\n",
       "      <td>0.999014</td>\n",
       "      <td>0.998131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.998659</td>\n",
       "      <td>0.995006</td>\n",
       "      <td>0.999174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.924246</td>\n",
       "      <td>0.605332</td>\n",
       "      <td>0.967869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.995831</td>\n",
       "      <td>0.982555</td>\n",
       "      <td>0.996322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.981833</td>\n",
       "      <td>0.973615</td>\n",
       "      <td>0.994862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.988681</td>\n",
       "      <td>0.897790</td>\n",
       "      <td>0.985851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.987088</td>\n",
       "      <td>0.854057</td>\n",
       "      <td>0.989420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.994213</td>\n",
       "      <td>0.954220</td>\n",
       "      <td>0.996957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.996720</td>\n",
       "      <td>0.891146</td>\n",
       "      <td>0.990824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.965192</td>\n",
       "      <td>0.954485</td>\n",
       "      <td>0.990664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.999150</td>\n",
       "      <td>0.998328</td>\n",
       "      <td>0.999636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2\n",
       "0   0.999518  0.994275  0.999643\n",
       "1   0.908336  0.292791  0.841821\n",
       "2   0.998569  0.997075  0.998604\n",
       "3   0.999847  0.994274  0.998673\n",
       "4   0.963921  0.936493  0.978588\n",
       "5   0.996810  0.995636  0.983830\n",
       "6   0.998032  0.985865  0.998081\n",
       "7   0.999212  0.990228  0.997315\n",
       "8   0.998383  0.989224  0.995926\n",
       "9   0.995750  0.999014  0.998131\n",
       "10  0.998659  0.995006  0.999174\n",
       "11  0.924246  0.605332  0.967869\n",
       "12  0.995831  0.982555  0.996322\n",
       "13  0.981833  0.973615  0.994862\n",
       "14  0.988681  0.897790  0.985851\n",
       "15  0.987088  0.854057  0.989420\n",
       "16  0.994213  0.954220  0.996957\n",
       "17  0.996720  0.891146  0.990824\n",
       "18  0.965192  0.954485  0.990664\n",
       "19  0.999150  0.998328  0.999636"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(encoder.predict(test_set_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>641</th>\n",
       "      <th>642</th>\n",
       "      <th>643</th>\n",
       "      <th>644</th>\n",
       "      <th>645</th>\n",
       "      <th>646</th>\n",
       "      <th>647</th>\n",
       "      <th>648</th>\n",
       "      <th>649</th>\n",
       "      <th>650</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.483645</td>\n",
       "      <td>0.361467</td>\n",
       "      <td>0.554238</td>\n",
       "      <td>0.710514</td>\n",
       "      <td>0.568284</td>\n",
       "      <td>0.480299</td>\n",
       "      <td>0.423669</td>\n",
       "      <td>0.573898</td>\n",
       "      <td>0.517194</td>\n",
       "      <td>0.409517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.294879</td>\n",
       "      <td>0.550146</td>\n",
       "      <td>0.515443</td>\n",
       "      <td>0.398634</td>\n",
       "      <td>0.503561</td>\n",
       "      <td>0.404975</td>\n",
       "      <td>0.508967</td>\n",
       "      <td>0.653606</td>\n",
       "      <td>0.294895</td>\n",
       "      <td>0.473652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.395633</td>\n",
       "      <td>0.276065</td>\n",
       "      <td>0.413185</td>\n",
       "      <td>0.520224</td>\n",
       "      <td>0.456285</td>\n",
       "      <td>0.312599</td>\n",
       "      <td>0.358712</td>\n",
       "      <td>0.417652</td>\n",
       "      <td>0.348625</td>\n",
       "      <td>0.263294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.267948</td>\n",
       "      <td>0.460260</td>\n",
       "      <td>0.363751</td>\n",
       "      <td>0.325871</td>\n",
       "      <td>0.368478</td>\n",
       "      <td>0.275772</td>\n",
       "      <td>0.429126</td>\n",
       "      <td>0.467670</td>\n",
       "      <td>0.227519</td>\n",
       "      <td>0.315546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.483616</td>\n",
       "      <td>0.361465</td>\n",
       "      <td>0.554362</td>\n",
       "      <td>0.710801</td>\n",
       "      <td>0.568189</td>\n",
       "      <td>0.480785</td>\n",
       "      <td>0.423600</td>\n",
       "      <td>0.574170</td>\n",
       "      <td>0.517495</td>\n",
       "      <td>0.409927</td>\n",
       "      <td>...</td>\n",
       "      <td>0.294631</td>\n",
       "      <td>0.549918</td>\n",
       "      <td>0.515754</td>\n",
       "      <td>0.398578</td>\n",
       "      <td>0.503796</td>\n",
       "      <td>0.405294</td>\n",
       "      <td>0.508851</td>\n",
       "      <td>0.653842</td>\n",
       "      <td>0.294897</td>\n",
       "      <td>0.474024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.483518</td>\n",
       "      <td>0.361396</td>\n",
       "      <td>0.554118</td>\n",
       "      <td>0.710424</td>\n",
       "      <td>0.568176</td>\n",
       "      <td>0.480281</td>\n",
       "      <td>0.423687</td>\n",
       "      <td>0.573871</td>\n",
       "      <td>0.517027</td>\n",
       "      <td>0.409515</td>\n",
       "      <td>...</td>\n",
       "      <td>0.294782</td>\n",
       "      <td>0.550025</td>\n",
       "      <td>0.515427</td>\n",
       "      <td>0.398576</td>\n",
       "      <td>0.503580</td>\n",
       "      <td>0.405018</td>\n",
       "      <td>0.508951</td>\n",
       "      <td>0.653425</td>\n",
       "      <td>0.294882</td>\n",
       "      <td>0.473588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.472673</td>\n",
       "      <td>0.350223</td>\n",
       "      <td>0.537816</td>\n",
       "      <td>0.688879</td>\n",
       "      <td>0.552320</td>\n",
       "      <td>0.463823</td>\n",
       "      <td>0.412580</td>\n",
       "      <td>0.555803</td>\n",
       "      <td>0.500534</td>\n",
       "      <td>0.394656</td>\n",
       "      <td>...</td>\n",
       "      <td>0.288762</td>\n",
       "      <td>0.535541</td>\n",
       "      <td>0.498433</td>\n",
       "      <td>0.388106</td>\n",
       "      <td>0.487059</td>\n",
       "      <td>0.390251</td>\n",
       "      <td>0.495574</td>\n",
       "      <td>0.633590</td>\n",
       "      <td>0.285212</td>\n",
       "      <td>0.457666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.480985</td>\n",
       "      <td>0.359481</td>\n",
       "      <td>0.551479</td>\n",
       "      <td>0.707980</td>\n",
       "      <td>0.565124</td>\n",
       "      <td>0.479652</td>\n",
       "      <td>0.422500</td>\n",
       "      <td>0.572389</td>\n",
       "      <td>0.514375</td>\n",
       "      <td>0.409019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.292483</td>\n",
       "      <td>0.546619</td>\n",
       "      <td>0.514282</td>\n",
       "      <td>0.396723</td>\n",
       "      <td>0.502650</td>\n",
       "      <td>0.404733</td>\n",
       "      <td>0.507044</td>\n",
       "      <td>0.650095</td>\n",
       "      <td>0.293729</td>\n",
       "      <td>0.472227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.482609</td>\n",
       "      <td>0.360434</td>\n",
       "      <td>0.552554</td>\n",
       "      <td>0.708216</td>\n",
       "      <td>0.566925</td>\n",
       "      <td>0.478271</td>\n",
       "      <td>0.422831</td>\n",
       "      <td>0.571989</td>\n",
       "      <td>0.515217</td>\n",
       "      <td>0.407739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.294561</td>\n",
       "      <td>0.549054</td>\n",
       "      <td>0.513590</td>\n",
       "      <td>0.397744</td>\n",
       "      <td>0.501884</td>\n",
       "      <td>0.403372</td>\n",
       "      <td>0.507954</td>\n",
       "      <td>0.651410</td>\n",
       "      <td>0.294054</td>\n",
       "      <td>0.471757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.482930</td>\n",
       "      <td>0.360841</td>\n",
       "      <td>0.553217</td>\n",
       "      <td>0.709241</td>\n",
       "      <td>0.567430</td>\n",
       "      <td>0.479284</td>\n",
       "      <td>0.423271</td>\n",
       "      <td>0.572914</td>\n",
       "      <td>0.515966</td>\n",
       "      <td>0.408648</td>\n",
       "      <td>...</td>\n",
       "      <td>0.294556</td>\n",
       "      <td>0.549398</td>\n",
       "      <td>0.514507</td>\n",
       "      <td>0.398096</td>\n",
       "      <td>0.502763</td>\n",
       "      <td>0.404255</td>\n",
       "      <td>0.508427</td>\n",
       "      <td>0.652242</td>\n",
       "      <td>0.294454</td>\n",
       "      <td>0.472625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.482557</td>\n",
       "      <td>0.360505</td>\n",
       "      <td>0.552735</td>\n",
       "      <td>0.708676</td>\n",
       "      <td>0.566934</td>\n",
       "      <td>0.478934</td>\n",
       "      <td>0.422991</td>\n",
       "      <td>0.572480</td>\n",
       "      <td>0.515476</td>\n",
       "      <td>0.408337</td>\n",
       "      <td>...</td>\n",
       "      <td>0.294291</td>\n",
       "      <td>0.548903</td>\n",
       "      <td>0.514115</td>\n",
       "      <td>0.397779</td>\n",
       "      <td>0.502397</td>\n",
       "      <td>0.403964</td>\n",
       "      <td>0.508056</td>\n",
       "      <td>0.651646</td>\n",
       "      <td>0.294200</td>\n",
       "      <td>0.472235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.483447</td>\n",
       "      <td>0.361238</td>\n",
       "      <td>0.554222</td>\n",
       "      <td>0.710671</td>\n",
       "      <td>0.567757</td>\n",
       "      <td>0.480956</td>\n",
       "      <td>0.423151</td>\n",
       "      <td>0.574028</td>\n",
       "      <td>0.517653</td>\n",
       "      <td>0.410023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.294275</td>\n",
       "      <td>0.549374</td>\n",
       "      <td>0.515677</td>\n",
       "      <td>0.398280</td>\n",
       "      <td>0.503581</td>\n",
       "      <td>0.405205</td>\n",
       "      <td>0.508323</td>\n",
       "      <td>0.653836</td>\n",
       "      <td>0.294629</td>\n",
       "      <td>0.474135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.483553</td>\n",
       "      <td>0.361372</td>\n",
       "      <td>0.554163</td>\n",
       "      <td>0.710456</td>\n",
       "      <td>0.568108</td>\n",
       "      <td>0.480367</td>\n",
       "      <td>0.423525</td>\n",
       "      <td>0.573854</td>\n",
       "      <td>0.517208</td>\n",
       "      <td>0.409562</td>\n",
       "      <td>...</td>\n",
       "      <td>0.294726</td>\n",
       "      <td>0.549926</td>\n",
       "      <td>0.515424</td>\n",
       "      <td>0.398518</td>\n",
       "      <td>0.503504</td>\n",
       "      <td>0.404970</td>\n",
       "      <td>0.508786</td>\n",
       "      <td>0.653561</td>\n",
       "      <td>0.294804</td>\n",
       "      <td>0.473683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.441631</td>\n",
       "      <td>0.317474</td>\n",
       "      <td>0.482237</td>\n",
       "      <td>0.609172</td>\n",
       "      <td>0.511285</td>\n",
       "      <td>0.387845</td>\n",
       "      <td>0.385501</td>\n",
       "      <td>0.487909</td>\n",
       "      <td>0.433042</td>\n",
       "      <td>0.327988</td>\n",
       "      <td>...</td>\n",
       "      <td>0.284926</td>\n",
       "      <td>0.506139</td>\n",
       "      <td>0.431435</td>\n",
       "      <td>0.360816</td>\n",
       "      <td>0.426484</td>\n",
       "      <td>0.330049</td>\n",
       "      <td>0.464359</td>\n",
       "      <td>0.560301</td>\n",
       "      <td>0.257484</td>\n",
       "      <td>0.389249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.481878</td>\n",
       "      <td>0.359708</td>\n",
       "      <td>0.551501</td>\n",
       "      <td>0.706866</td>\n",
       "      <td>0.565882</td>\n",
       "      <td>0.477289</td>\n",
       "      <td>0.422139</td>\n",
       "      <td>0.570880</td>\n",
       "      <td>0.514152</td>\n",
       "      <td>0.406855</td>\n",
       "      <td>...</td>\n",
       "      <td>0.294120</td>\n",
       "      <td>0.548078</td>\n",
       "      <td>0.512557</td>\n",
       "      <td>0.397062</td>\n",
       "      <td>0.500887</td>\n",
       "      <td>0.402502</td>\n",
       "      <td>0.507102</td>\n",
       "      <td>0.650124</td>\n",
       "      <td>0.293447</td>\n",
       "      <td>0.470777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.479733</td>\n",
       "      <td>0.357172</td>\n",
       "      <td>0.548211</td>\n",
       "      <td>0.702351</td>\n",
       "      <td>0.562118</td>\n",
       "      <td>0.474129</td>\n",
       "      <td>0.418883</td>\n",
       "      <td>0.566854</td>\n",
       "      <td>0.511527</td>\n",
       "      <td>0.403841</td>\n",
       "      <td>...</td>\n",
       "      <td>0.292562</td>\n",
       "      <td>0.544461</td>\n",
       "      <td>0.508834</td>\n",
       "      <td>0.394501</td>\n",
       "      <td>0.496881</td>\n",
       "      <td>0.399010</td>\n",
       "      <td>0.503351</td>\n",
       "      <td>0.646544</td>\n",
       "      <td>0.290941</td>\n",
       "      <td>0.467803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.472955</td>\n",
       "      <td>0.350811</td>\n",
       "      <td>0.536363</td>\n",
       "      <td>0.685815</td>\n",
       "      <td>0.554592</td>\n",
       "      <td>0.457734</td>\n",
       "      <td>0.415426</td>\n",
       "      <td>0.553344</td>\n",
       "      <td>0.495567</td>\n",
       "      <td>0.389815</td>\n",
       "      <td>...</td>\n",
       "      <td>0.292406</td>\n",
       "      <td>0.539659</td>\n",
       "      <td>0.495334</td>\n",
       "      <td>0.389671</td>\n",
       "      <td>0.485609</td>\n",
       "      <td>0.387488</td>\n",
       "      <td>0.499068</td>\n",
       "      <td>0.629963</td>\n",
       "      <td>0.286302</td>\n",
       "      <td>0.452779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.469970</td>\n",
       "      <td>0.347475</td>\n",
       "      <td>0.530301</td>\n",
       "      <td>0.676653</td>\n",
       "      <td>0.550739</td>\n",
       "      <td>0.448226</td>\n",
       "      <td>0.412776</td>\n",
       "      <td>0.545385</td>\n",
       "      <td>0.487751</td>\n",
       "      <td>0.381503</td>\n",
       "      <td>...</td>\n",
       "      <td>0.292932</td>\n",
       "      <td>0.537474</td>\n",
       "      <td>0.487328</td>\n",
       "      <td>0.387064</td>\n",
       "      <td>0.478467</td>\n",
       "      <td>0.380086</td>\n",
       "      <td>0.496186</td>\n",
       "      <td>0.621768</td>\n",
       "      <td>0.283401</td>\n",
       "      <td>0.444497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.479624</td>\n",
       "      <td>0.357290</td>\n",
       "      <td>0.547223</td>\n",
       "      <td>0.700579</td>\n",
       "      <td>0.562980</td>\n",
       "      <td>0.471004</td>\n",
       "      <td>0.420233</td>\n",
       "      <td>0.565488</td>\n",
       "      <td>0.508748</td>\n",
       "      <td>0.401362</td>\n",
       "      <td>...</td>\n",
       "      <td>0.294178</td>\n",
       "      <td>0.546219</td>\n",
       "      <td>0.507177</td>\n",
       "      <td>0.395122</td>\n",
       "      <td>0.496087</td>\n",
       "      <td>0.397614</td>\n",
       "      <td>0.504957</td>\n",
       "      <td>0.644381</td>\n",
       "      <td>0.291395</td>\n",
       "      <td>0.465207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.473928</td>\n",
       "      <td>0.351776</td>\n",
       "      <td>0.537199</td>\n",
       "      <td>0.686489</td>\n",
       "      <td>0.556354</td>\n",
       "      <td>0.457168</td>\n",
       "      <td>0.416810</td>\n",
       "      <td>0.553847</td>\n",
       "      <td>0.495582</td>\n",
       "      <td>0.389449</td>\n",
       "      <td>...</td>\n",
       "      <td>0.293910</td>\n",
       "      <td>0.541848</td>\n",
       "      <td>0.495582</td>\n",
       "      <td>0.390825</td>\n",
       "      <td>0.486192</td>\n",
       "      <td>0.387577</td>\n",
       "      <td>0.500825</td>\n",
       "      <td>0.630560</td>\n",
       "      <td>0.287197</td>\n",
       "      <td>0.452587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.476124</td>\n",
       "      <td>0.353191</td>\n",
       "      <td>0.542635</td>\n",
       "      <td>0.694782</td>\n",
       "      <td>0.556399</td>\n",
       "      <td>0.468404</td>\n",
       "      <td>0.414412</td>\n",
       "      <td>0.560307</td>\n",
       "      <td>0.506311</td>\n",
       "      <td>0.398568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.290420</td>\n",
       "      <td>0.539187</td>\n",
       "      <td>0.502692</td>\n",
       "      <td>0.390664</td>\n",
       "      <td>0.490658</td>\n",
       "      <td>0.393450</td>\n",
       "      <td>0.498113</td>\n",
       "      <td>0.640028</td>\n",
       "      <td>0.287259</td>\n",
       "      <td>0.462387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.483929</td>\n",
       "      <td>0.361752</td>\n",
       "      <td>0.554790</td>\n",
       "      <td>0.711324</td>\n",
       "      <td>0.568600</td>\n",
       "      <td>0.481153</td>\n",
       "      <td>0.423831</td>\n",
       "      <td>0.574579</td>\n",
       "      <td>0.517956</td>\n",
       "      <td>0.410251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.294822</td>\n",
       "      <td>0.550308</td>\n",
       "      <td>0.516133</td>\n",
       "      <td>0.398841</td>\n",
       "      <td>0.504143</td>\n",
       "      <td>0.405589</td>\n",
       "      <td>0.509152</td>\n",
       "      <td>0.654384</td>\n",
       "      <td>0.295114</td>\n",
       "      <td>0.474410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 651 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6    \\\n",
       "0   0.483645  0.361467  0.554238  0.710514  0.568284  0.480299  0.423669   \n",
       "1   0.395633  0.276065  0.413185  0.520224  0.456285  0.312599  0.358712   \n",
       "2   0.483616  0.361465  0.554362  0.710801  0.568189  0.480785  0.423600   \n",
       "3   0.483518  0.361396  0.554118  0.710424  0.568176  0.480281  0.423687   \n",
       "4   0.472673  0.350223  0.537816  0.688879  0.552320  0.463823  0.412580   \n",
       "5   0.480985  0.359481  0.551479  0.707980  0.565124  0.479652  0.422500   \n",
       "6   0.482609  0.360434  0.552554  0.708216  0.566925  0.478271  0.422831   \n",
       "7   0.482930  0.360841  0.553217  0.709241  0.567430  0.479284  0.423271   \n",
       "8   0.482557  0.360505  0.552735  0.708676  0.566934  0.478934  0.422991   \n",
       "9   0.483447  0.361238  0.554222  0.710671  0.567757  0.480956  0.423151   \n",
       "10  0.483553  0.361372  0.554163  0.710456  0.568108  0.480367  0.423525   \n",
       "11  0.441631  0.317474  0.482237  0.609172  0.511285  0.387845  0.385501   \n",
       "12  0.481878  0.359708  0.551501  0.706866  0.565882  0.477289  0.422139   \n",
       "13  0.479733  0.357172  0.548211  0.702351  0.562118  0.474129  0.418883   \n",
       "14  0.472955  0.350811  0.536363  0.685815  0.554592  0.457734  0.415426   \n",
       "15  0.469970  0.347475  0.530301  0.676653  0.550739  0.448226  0.412776   \n",
       "16  0.479624  0.357290  0.547223  0.700579  0.562980  0.471004  0.420233   \n",
       "17  0.473928  0.351776  0.537199  0.686489  0.556354  0.457168  0.416810   \n",
       "18  0.476124  0.353191  0.542635  0.694782  0.556399  0.468404  0.414412   \n",
       "19  0.483929  0.361752  0.554790  0.711324  0.568600  0.481153  0.423831   \n",
       "\n",
       "         7         8         9    ...       641       642       643       644  \\\n",
       "0   0.573898  0.517194  0.409517  ...  0.294879  0.550146  0.515443  0.398634   \n",
       "1   0.417652  0.348625  0.263294  ...  0.267948  0.460260  0.363751  0.325871   \n",
       "2   0.574170  0.517495  0.409927  ...  0.294631  0.549918  0.515754  0.398578   \n",
       "3   0.573871  0.517027  0.409515  ...  0.294782  0.550025  0.515427  0.398576   \n",
       "4   0.555803  0.500534  0.394656  ...  0.288762  0.535541  0.498433  0.388106   \n",
       "5   0.572389  0.514375  0.409019  ...  0.292483  0.546619  0.514282  0.396723   \n",
       "6   0.571989  0.515217  0.407739  ...  0.294561  0.549054  0.513590  0.397744   \n",
       "7   0.572914  0.515966  0.408648  ...  0.294556  0.549398  0.514507  0.398096   \n",
       "8   0.572480  0.515476  0.408337  ...  0.294291  0.548903  0.514115  0.397779   \n",
       "9   0.574028  0.517653  0.410023  ...  0.294275  0.549374  0.515677  0.398280   \n",
       "10  0.573854  0.517208  0.409562  ...  0.294726  0.549926  0.515424  0.398518   \n",
       "11  0.487909  0.433042  0.327988  ...  0.284926  0.506139  0.431435  0.360816   \n",
       "12  0.570880  0.514152  0.406855  ...  0.294120  0.548078  0.512557  0.397062   \n",
       "13  0.566854  0.511527  0.403841  ...  0.292562  0.544461  0.508834  0.394501   \n",
       "14  0.553344  0.495567  0.389815  ...  0.292406  0.539659  0.495334  0.389671   \n",
       "15  0.545385  0.487751  0.381503  ...  0.292932  0.537474  0.487328  0.387064   \n",
       "16  0.565488  0.508748  0.401362  ...  0.294178  0.546219  0.507177  0.395122   \n",
       "17  0.553847  0.495582  0.389449  ...  0.293910  0.541848  0.495582  0.390825   \n",
       "18  0.560307  0.506311  0.398568  ...  0.290420  0.539187  0.502692  0.390664   \n",
       "19  0.574579  0.517956  0.410251  ...  0.294822  0.550308  0.516133  0.398841   \n",
       "\n",
       "         645       646       647       648       649       650  \n",
       "0   0.503561  0.404975  0.508967  0.653606  0.294895  0.473652  \n",
       "1   0.368478  0.275772  0.429126  0.467670  0.227519  0.315546  \n",
       "2   0.503796  0.405294  0.508851  0.653842  0.294897  0.474024  \n",
       "3   0.503580  0.405018  0.508951  0.653425  0.294882  0.473588  \n",
       "4   0.487059  0.390251  0.495574  0.633590  0.285212  0.457666  \n",
       "5   0.502650  0.404733  0.507044  0.650095  0.293729  0.472227  \n",
       "6   0.501884  0.403372  0.507954  0.651410  0.294054  0.471757  \n",
       "7   0.502763  0.404255  0.508427  0.652242  0.294454  0.472625  \n",
       "8   0.502397  0.403964  0.508056  0.651646  0.294200  0.472235  \n",
       "9   0.503581  0.405205  0.508323  0.653836  0.294629  0.474135  \n",
       "10  0.503504  0.404970  0.508786  0.653561  0.294804  0.473683  \n",
       "11  0.426484  0.330049  0.464359  0.560301  0.257484  0.389249  \n",
       "12  0.500887  0.402502  0.507102  0.650124  0.293447  0.470777  \n",
       "13  0.496881  0.399010  0.503351  0.646544  0.290941  0.467803  \n",
       "14  0.485609  0.387488  0.499068  0.629963  0.286302  0.452779  \n",
       "15  0.478467  0.380086  0.496186  0.621768  0.283401  0.444497  \n",
       "16  0.496087  0.397614  0.504957  0.644381  0.291395  0.465207  \n",
       "17  0.486192  0.387577  0.500825  0.630560  0.287197  0.452587  \n",
       "18  0.490658  0.393450  0.498113  0.640028  0.287259  0.462387  \n",
       "19  0.504143  0.405589  0.509152  0.654384  0.295114  0.474410  \n",
       "\n",
       "[20 rows x 651 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(decoded_stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    ax = plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history[\"loss\"])\n",
    "    plt.title(\"Train loss\")\n",
    "    ax = plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history[\"val_loss\"])\n",
    "    plt.title(\"Test loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loss vs Epoch\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D convolutional autoencoder\n",
    "(Kernel size = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 651, 1)]          0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 651, 16)           64        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 326, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 326, 1)            49        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 163, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 163, 1)            4         \n",
      "_________________________________________________________________\n",
      "up_sampling1d_8 (UpSampling1 (None, 326, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 325, 16)           48        \n",
      "_________________________________________________________________\n",
      "up_sampling1d_9 (UpSampling1 (None, 650, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 650, 1)            49        \n",
      "=================================================================\n",
      "Total params: 214\n",
      "Trainable params: 214\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/250\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /Users/anjalichauhan/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py:830 train_function  *\n        return step_function(self, iterator)\n    /Users/anjalichauhan/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py:813 run_step  *\n        outputs = model.train_step(data)\n    /Users/anjalichauhan/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py:771 train_step  *\n        loss = self.compiled_loss(\n    /Users/anjalichauhan/opt/anaconda3/lib/python3.8/site-packages/keras/engine/compile_utils.py:201 __call__  *\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /Users/anjalichauhan/opt/anaconda3/lib/python3.8/site-packages/keras/losses.py:142 __call__  *\n        losses = call_fn(y_true, y_pred)\n    /Users/anjalichauhan/opt/anaconda3/lib/python3.8/site-packages/keras/losses.py:246 call  *\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /Users/anjalichauhan/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206 wrapper  **\n        return target(*args, **kwargs)\n    /Users/anjalichauhan/opt/anaconda3/lib/python3.8/site-packages/keras/losses.py:1202 mean_squared_error\n        return backend.mean(tf.math.squared_difference(y_pred, y_true), axis=-1)\n    /Users/anjalichauhan/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py:10422 squared_difference\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n    /Users/anjalichauhan/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py:748 _apply_op_helper\n        op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n    /Users/anjalichauhan/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:599 _create_op_internal\n        return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n    /Users/anjalichauhan/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:3557 _create_op_internal\n        ret = Operation(\n    /Users/anjalichauhan/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:2041 __init__\n        self._c_op = _create_c_op(self._graph, node_def, inputs,\n    /Users/anjalichauhan/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1883 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Dimensions must be equal, but are 650 and 651 for '{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT](mean_squared_error/remove_squeezable_dimensions/Squeeze, IteratorGetNext:1)' with input shapes: [?,650], [?,651].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-6b46e44caee2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'MeanSquaredError'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m history = autoencoder.fit(training_set_scaled, training_set_scaled,\n\u001b[0m\u001b[1;32m     27\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1156\u001b[0m                 _r=1):\n\u001b[1;32m   1157\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 763\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    764\u001b[0m             *args, **kwds))\n\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3048\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3049\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3050\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3051\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3444\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3445\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3277\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3278\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3279\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3280\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3281\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /Users/anjalichauhan/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py:830 train_function  *\n        return step_function(self, iterator)\n    /Users/anjalichauhan/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py:813 run_step  *\n        outputs = model.train_step(data)\n    /Users/anjalichauhan/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py:771 train_step  *\n        loss = self.compiled_loss(\n    /Users/anjalichauhan/opt/anaconda3/lib/python3.8/site-packages/keras/engine/compile_utils.py:201 __call__  *\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /Users/anjalichauhan/opt/anaconda3/lib/python3.8/site-packages/keras/losses.py:142 __call__  *\n        losses = call_fn(y_true, y_pred)\n    /Users/anjalichauhan/opt/anaconda3/lib/python3.8/site-packages/keras/losses.py:246 call  *\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /Users/anjalichauhan/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206 wrapper  **\n        return target(*args, **kwargs)\n    /Users/anjalichauhan/opt/anaconda3/lib/python3.8/site-packages/keras/losses.py:1202 mean_squared_error\n        return backend.mean(tf.math.squared_difference(y_pred, y_true), axis=-1)\n    /Users/anjalichauhan/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py:10422 squared_difference\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n    /Users/anjalichauhan/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py:748 _apply_op_helper\n        op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n    /Users/anjalichauhan/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:599 _create_op_internal\n        return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n    /Users/anjalichauhan/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:3557 _create_op_internal\n        ret = Operation(\n    /Users/anjalichauhan/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:2041 __init__\n        self._c_op = _create_c_op(self._graph, node_def, inputs,\n    /Users/anjalichauhan/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1883 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Dimensions must be equal, but are 650 and 651 for '{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT](mean_squared_error/remove_squeezable_dimensions/Squeeze, IteratorGetNext:1)' with input shapes: [?,650], [?,651].\n"
     ]
    }
   ],
   "source": [
    "# main “event” very well represented while the overall reconstruction is very smooth \n",
    "\n",
    "input_window = Input(shape=(window_length,1))\n",
    "x = Conv1D(16, 3, activation=\"tanh\", padding=\"same\")(input_window) # 10 dims\n",
    "#x = BatchNormalization()(x)\n",
    "x = MaxPooling1D(2, padding=\"same\")(x) # 5 dims\n",
    "x = Conv1D(1, 3, activation=\"tanh\", padding=\"same\")(x) # 5 dims\n",
    "#x = BatchNormalization()(x)\n",
    "encoded = MaxPooling1D(2, padding=\"same\")(x) # 3 dims\n",
    "\n",
    "encoder = Model(input_window, encoded)\n",
    "\n",
    "# 3 dimensions in the encoded layer\n",
    "\n",
    "x = Conv1D(1, 3, activation=\"tanh\", padding=\"same\")(encoded) # 3 dims\n",
    "#x = BatchNormalization()(x)\n",
    "x = UpSampling1D(2)(x) # 6 dims\n",
    "x = Conv1D(16, 2, activation='tanh')(x) # 5 dims\n",
    "#x = BatchNormalization()(x)\n",
    "x = UpSampling1D(2)(x) # 10 dims\n",
    "decoded = Conv1D(1, 3, activation='linear', padding='same')(x) # 10 dims\n",
    "autoencoder = Model(input_window, decoded)\n",
    "autoencoder.summary()\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='MeanSquaredError')\n",
    "history = autoencoder.fit(train, train,\n",
    "                epochs=epochs,\n",
    "                batch_size=1024,\n",
    "                shuffle=True,\n",
    "                validation_data=(test, test))\n",
    "\n",
    "decoded_stocks = autoencoder.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(encoder.predict(test_set_scaled))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
