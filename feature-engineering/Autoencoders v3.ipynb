{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Conv1D, MaxPooling1D, UpSampling1D, BatchNormalization, LSTM, RepeatVector\n",
    "from keras.models import Sequential, Model\n",
    "from keras.models import model_from_json\n",
    "from tensorflow import keras\n",
    "from keras import regularizers\n",
    "import datetime\n",
    "import time\n",
    "import requests as req\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from tqdm import tqdm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.compat.v1\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## USE: full log(returns)/returns dataframe\n",
    "## Risk Adjusted Returns\n",
    "\n",
    "df = pd.read_pickle(\"../Data/risk_adj_returns.pkl\").iloc[1:]\n",
    "df1 = pd.read_pickle(\"../Data/returns.pkl\").iloc[1:]\n",
    "\n",
    "drop_columns = []\n",
    "for col in df1.columns:\n",
    "    if df1[col].isnull().all() == True:\n",
    "        drop_columns.append(col)\n",
    "        \n",
    "df1.drop(columns=drop_columns, inplace=True)\n",
    "\n",
    "# df['pct_change'] = df.close.pct_change()\n",
    "# df['log_ret'] = np.log(df.close) - np.log(df.close.shift(1))\n",
    "# df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.dropna(how='any',axis=0) #All rows have NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>905270</th>\n",
       "      <th>921795</th>\n",
       "      <th>904261</th>\n",
       "      <th>905261</th>\n",
       "      <th>916328</th>\n",
       "      <th>923024</th>\n",
       "      <th>936365</th>\n",
       "      <th>902355</th>\n",
       "      <th>912215</th>\n",
       "      <th>929813</th>\n",
       "      <th>...</th>\n",
       "      <th>9660J1</th>\n",
       "      <th>69568X</th>\n",
       "      <th>543755</th>\n",
       "      <th>77463M</th>\n",
       "      <th>29235J</th>\n",
       "      <th>131745</th>\n",
       "      <th>69487D</th>\n",
       "      <th>68157P</th>\n",
       "      <th>9110RA</th>\n",
       "      <th>292703</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-05-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.136600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.349877</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.376702</td>\n",
       "      <td>4.400221</td>\n",
       "      <td>1.590198</td>\n",
       "      <td>-0.991004</td>\n",
       "      <td>1.682900</td>\n",
       "      <td>3.364703</td>\n",
       "      <td>2.271817</td>\n",
       "      <td>0.634962</td>\n",
       "      <td>4.859794</td>\n",
       "      <td>1.204016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.946372</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.542355</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.230947</td>\n",
       "      <td>-0.288569</td>\n",
       "      <td>0.449063</td>\n",
       "      <td>-1.661106</td>\n",
       "      <td>0.439588</td>\n",
       "      <td>-0.548765</td>\n",
       "      <td>0.185928</td>\n",
       "      <td>0.727530</td>\n",
       "      <td>1.884279</td>\n",
       "      <td>-0.498203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.393581</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.616491</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.296083</td>\n",
       "      <td>2.388001</td>\n",
       "      <td>-0.166049</td>\n",
       "      <td>3.255613</td>\n",
       "      <td>2.317760</td>\n",
       "      <td>4.702784</td>\n",
       "      <td>0.673960</td>\n",
       "      <td>2.336242</td>\n",
       "      <td>-0.429666</td>\n",
       "      <td>-1.520393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.907923</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.102119</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056867</td>\n",
       "      <td>1.893011</td>\n",
       "      <td>-0.063971</td>\n",
       "      <td>-0.510347</td>\n",
       "      <td>1.044185</td>\n",
       "      <td>-0.275482</td>\n",
       "      <td>1.979237</td>\n",
       "      <td>1.876893</td>\n",
       "      <td>1.247655</td>\n",
       "      <td>2.369727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.829804</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.116807</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.312589</td>\n",
       "      <td>-0.892462</td>\n",
       "      <td>-0.409679</td>\n",
       "      <td>0.519992</td>\n",
       "      <td>0.160551</td>\n",
       "      <td>-1.549363</td>\n",
       "      <td>0.570831</td>\n",
       "      <td>0.766354</td>\n",
       "      <td>-0.444733</td>\n",
       "      <td>1.742926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1237 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            905270  921795  904261  905261    916328  923024    936365  \\\n",
       "date                                                                     \n",
       "2021-05-24     NaN     NaN     NaN     NaN  0.136600     NaN  0.349877   \n",
       "2021-05-25     NaN     NaN     NaN     NaN  0.946372     NaN  0.542355   \n",
       "2021-05-26     NaN     NaN     NaN     NaN -1.393581     NaN  0.616491   \n",
       "2021-05-27     NaN     NaN     NaN     NaN -0.907923     NaN  0.102119   \n",
       "2021-05-28     NaN     NaN     NaN     NaN  0.829804     NaN  2.116807   \n",
       "\n",
       "            902355  912215  929813  ...    9660J1    69568X    543755  \\\n",
       "date                                ...                                 \n",
       "2021-05-24     NaN     NaN     NaN  ...  0.376702  4.400221  1.590198   \n",
       "2021-05-25     NaN     NaN     NaN  ...  0.230947 -0.288569  0.449063   \n",
       "2021-05-26     NaN     NaN     NaN  ...  1.296083  2.388001 -0.166049   \n",
       "2021-05-27     NaN     NaN     NaN  ...  0.056867  1.893011 -0.063971   \n",
       "2021-05-28     NaN     NaN     NaN  ... -0.312589 -0.892462 -0.409679   \n",
       "\n",
       "              77463M    29235J    131745    69487D    68157P    9110RA  \\\n",
       "date                                                                     \n",
       "2021-05-24 -0.991004  1.682900  3.364703  2.271817  0.634962  4.859794   \n",
       "2021-05-25 -1.661106  0.439588 -0.548765  0.185928  0.727530  1.884279   \n",
       "2021-05-26  3.255613  2.317760  4.702784  0.673960  2.336242 -0.429666   \n",
       "2021-05-27 -0.510347  1.044185 -0.275482  1.979237  1.876893  1.247655   \n",
       "2021-05-28  0.519992  0.160551 -1.549363  0.570831  0.766354 -0.444733   \n",
       "\n",
       "              292703  \n",
       "date                  \n",
       "2021-05-24  1.204016  \n",
       "2021-05-25 -0.498203  \n",
       "2021-05-26 -1.520393  \n",
       "2021-05-27  2.369727  \n",
       "2021-05-28  1.742926  \n",
       "\n",
       "[5 rows x 1237 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_investable(t, n_rows):\n",
    "    \"Find stocks in investable universe at time t\\\n",
    "    (stocks in the S&P500 that have prices recorded for the last n_rows days)\"\n",
    "    \n",
    "    df_investable = df1.copy(deep = True).sort_index(ascending = False)\n",
    "    \n",
    "    #add 1 date to get the test features in investable\n",
    "    t = t + pd.DateOffset(1)\n",
    "    \n",
    "    #if t is now a non-trading day, advance until we reach a valid trading day\n",
    "    while t not in df_investable.index:\n",
    "        t = t + pd.DateOffset(1)\n",
    "    \n",
    "    t_index = df_investable.index.get_loc(t)\n",
    "    \n",
    "    #take n_rows worth of data upto time specified\n",
    "    df_investable = df_investable.iloc[t_index + 1:t_index + n_rows + 1]\n",
    "    \n",
    "    #find all stocks that exist in the S&P at this time period\n",
    "    investable_universe = []\n",
    "    for col in df_investable.columns:\n",
    "        if ~df_investable[col].iloc[:n_rows].isna().any():\n",
    "            investable_universe.append(col)\n",
    "        \n",
    "    df_investable = df_investable[investable_universe]\n",
    "    \n",
    "    return df_investable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>916328</th>\n",
       "      <th>936365</th>\n",
       "      <th>905271</th>\n",
       "      <th>905113</th>\n",
       "      <th>905802</th>\n",
       "      <th>905425</th>\n",
       "      <th>906156</th>\n",
       "      <th>916305</th>\n",
       "      <th>992816</th>\n",
       "      <th>921093</th>\n",
       "      <th>...</th>\n",
       "      <th>311917</th>\n",
       "      <th>69568X</th>\n",
       "      <th>543755</th>\n",
       "      <th>77463M</th>\n",
       "      <th>29235J</th>\n",
       "      <th>131745</th>\n",
       "      <th>69487D</th>\n",
       "      <th>68157P</th>\n",
       "      <th>9110RA</th>\n",
       "      <th>292703</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-05-11</th>\n",
       "      <td>1.155878</td>\n",
       "      <td>-1.483924</td>\n",
       "      <td>0.422731</td>\n",
       "      <td>0.440871</td>\n",
       "      <td>-0.916272</td>\n",
       "      <td>0.208893</td>\n",
       "      <td>0.039456</td>\n",
       "      <td>0.189573</td>\n",
       "      <td>-0.762997</td>\n",
       "      <td>0.182232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.910010</td>\n",
       "      <td>-1.298276</td>\n",
       "      <td>0.524017</td>\n",
       "      <td>-2.303263</td>\n",
       "      <td>-1.639854</td>\n",
       "      <td>0.687071</td>\n",
       "      <td>-0.282002</td>\n",
       "      <td>1.212495</td>\n",
       "      <td>-2.540416</td>\n",
       "      <td>0.918045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-10</th>\n",
       "      <td>1.952862</td>\n",
       "      <td>1.506276</td>\n",
       "      <td>0.424525</td>\n",
       "      <td>0.665344</td>\n",
       "      <td>1.523657</td>\n",
       "      <td>1.131734</td>\n",
       "      <td>0.875622</td>\n",
       "      <td>-0.132526</td>\n",
       "      <td>1.430401</td>\n",
       "      <td>0.457666</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.047163</td>\n",
       "      <td>-0.596383</td>\n",
       "      <td>1.208014</td>\n",
       "      <td>11.324786</td>\n",
       "      <td>1.981454</td>\n",
       "      <td>1.009464</td>\n",
       "      <td>0.658962</td>\n",
       "      <td>0.433437</td>\n",
       "      <td>2.485207</td>\n",
       "      <td>-0.570505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-09</th>\n",
       "      <td>1.175268</td>\n",
       "      <td>2.928510</td>\n",
       "      <td>0.843985</td>\n",
       "      <td>1.236468</td>\n",
       "      <td>2.364144</td>\n",
       "      <td>-1.457249</td>\n",
       "      <td>0.761981</td>\n",
       "      <td>-0.489827</td>\n",
       "      <td>0.704112</td>\n",
       "      <td>-0.568828</td>\n",
       "      <td>...</td>\n",
       "      <td>3.119572</td>\n",
       "      <td>1.616055</td>\n",
       "      <td>-3.125446</td>\n",
       "      <td>4.231626</td>\n",
       "      <td>1.659818</td>\n",
       "      <td>-0.220334</td>\n",
       "      <td>-0.151837</td>\n",
       "      <td>1.978531</td>\n",
       "      <td>-0.236128</td>\n",
       "      <td>0.219173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-08</th>\n",
       "      <td>-1.028321</td>\n",
       "      <td>0.172563</td>\n",
       "      <td>0.036708</td>\n",
       "      <td>1.366291</td>\n",
       "      <td>2.853766</td>\n",
       "      <td>-2.224484</td>\n",
       "      <td>1.176709</td>\n",
       "      <td>-0.375375</td>\n",
       "      <td>0.480665</td>\n",
       "      <td>0.595102</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146117</td>\n",
       "      <td>-0.264227</td>\n",
       "      <td>-1.725105</td>\n",
       "      <td>-2.178649</td>\n",
       "      <td>0.218023</td>\n",
       "      <td>2.915452</td>\n",
       "      <td>0.335161</td>\n",
       "      <td>1.020625</td>\n",
       "      <td>0.474496</td>\n",
       "      <td>-0.398633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-07</th>\n",
       "      <td>0.987402</td>\n",
       "      <td>2.748227</td>\n",
       "      <td>0.423937</td>\n",
       "      <td>1.974362</td>\n",
       "      <td>0.868677</td>\n",
       "      <td>-0.879089</td>\n",
       "      <td>0.233859</td>\n",
       "      <td>0.832702</td>\n",
       "      <td>0.723495</td>\n",
       "      <td>-0.387597</td>\n",
       "      <td>...</td>\n",
       "      <td>0.328098</td>\n",
       "      <td>2.951477</td>\n",
       "      <td>2.178275</td>\n",
       "      <td>6.744186</td>\n",
       "      <td>-0.024219</td>\n",
       "      <td>0.652103</td>\n",
       "      <td>-1.814918</td>\n",
       "      <td>2.172496</td>\n",
       "      <td>-0.823529</td>\n",
       "      <td>-0.538091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 644 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              916328    936365    905271    905113    905802    905425  \\\n",
       "date                                                                     \n",
       "2018-05-11  1.155878 -1.483924  0.422731  0.440871 -0.916272  0.208893   \n",
       "2018-05-10  1.952862  1.506276  0.424525  0.665344  1.523657  1.131734   \n",
       "2018-05-09  1.175268  2.928510  0.843985  1.236468  2.364144 -1.457249   \n",
       "2018-05-08 -1.028321  0.172563  0.036708  1.366291  2.853766 -2.224484   \n",
       "2018-05-07  0.987402  2.748227  0.423937  1.974362  0.868677 -0.879089   \n",
       "\n",
       "              906156    916305    992816    921093  ...    311917    69568X  \\\n",
       "date                                                ...                       \n",
       "2018-05-11  0.039456  0.189573 -0.762997  0.182232  ...  0.910010 -1.298276   \n",
       "2018-05-10  0.875622 -0.132526  1.430401  0.457666  ... -0.047163 -0.596383   \n",
       "2018-05-09  0.761981 -0.489827  0.704112 -0.568828  ...  3.119572  1.616055   \n",
       "2018-05-08  1.176709 -0.375375  0.480665  0.595102  ...  0.146117 -0.264227   \n",
       "2018-05-07  0.233859  0.832702  0.723495 -0.387597  ...  0.328098  2.951477   \n",
       "\n",
       "              543755     77463M    29235J    131745    69487D    68157P  \\\n",
       "date                                                                      \n",
       "2018-05-11  0.524017  -2.303263 -1.639854  0.687071 -0.282002  1.212495   \n",
       "2018-05-10  1.208014  11.324786  1.981454  1.009464  0.658962  0.433437   \n",
       "2018-05-09 -3.125446   4.231626  1.659818 -0.220334 -0.151837  1.978531   \n",
       "2018-05-08 -1.725105  -2.178649  0.218023  2.915452  0.335161  1.020625   \n",
       "2018-05-07  2.178275   6.744186 -0.024219  0.652103 -1.814918  2.172496   \n",
       "\n",
       "              9110RA    292703  \n",
       "date                            \n",
       "2018-05-11 -2.540416  0.918045  \n",
       "2018-05-10  2.485207 -0.570505  \n",
       "2018-05-09 -0.236128  0.219173  \n",
       "2018-05-08  0.474496 -0.398633  \n",
       "2018-05-07 -0.823529 -0.538091  \n",
       "\n",
       "[5 rows x 644 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = get_investable(pd.to_datetime('2018-05-11'),500)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tts = train_test_split(df1, test_size=0.2, shuffle=False)\n",
    "train = tts[0]\n",
    "test = tts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>916328</th>\n",
       "      <th>936365</th>\n",
       "      <th>905271</th>\n",
       "      <th>905113</th>\n",
       "      <th>905802</th>\n",
       "      <th>905425</th>\n",
       "      <th>906156</th>\n",
       "      <th>916305</th>\n",
       "      <th>992816</th>\n",
       "      <th>921093</th>\n",
       "      <th>...</th>\n",
       "      <th>311917</th>\n",
       "      <th>69568X</th>\n",
       "      <th>543755</th>\n",
       "      <th>77463M</th>\n",
       "      <th>29235J</th>\n",
       "      <th>131745</th>\n",
       "      <th>69487D</th>\n",
       "      <th>68157P</th>\n",
       "      <th>9110RA</th>\n",
       "      <th>292703</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-10-14</th>\n",
       "      <td>-0.463641</td>\n",
       "      <td>4.006163</td>\n",
       "      <td>0.294874</td>\n",
       "      <td>-1.158691</td>\n",
       "      <td>0.349379</td>\n",
       "      <td>-0.367823</td>\n",
       "      <td>-0.430392</td>\n",
       "      <td>0.453096</td>\n",
       "      <td>0.555651</td>\n",
       "      <td>-0.284024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.409130</td>\n",
       "      <td>-1.862765</td>\n",
       "      <td>0.351865</td>\n",
       "      <td>3.636364</td>\n",
       "      <td>0.351105</td>\n",
       "      <td>-1.812908</td>\n",
       "      <td>-0.432050</td>\n",
       "      <td>0.223839</td>\n",
       "      <td>-1.865672</td>\n",
       "      <td>-0.444939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.963746</td>\n",
       "      <td>-0.414125</td>\n",
       "      <td>-1.328181</td>\n",
       "      <td>-0.846805</td>\n",
       "      <td>0.854839</td>\n",
       "      <td>-0.461361</td>\n",
       "      <td>-1.406353</td>\n",
       "      <td>-0.306801</td>\n",
       "      <td>-0.330267</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.684346</td>\n",
       "      <td>-0.630242</td>\n",
       "      <td>0.708717</td>\n",
       "      <td>0.917431</td>\n",
       "      <td>-1.549097</td>\n",
       "      <td>-3.092059</td>\n",
       "      <td>-0.875998</td>\n",
       "      <td>-0.334635</td>\n",
       "      <td>-3.249097</td>\n",
       "      <td>0.272648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-12</th>\n",
       "      <td>-0.437318</td>\n",
       "      <td>1.846154</td>\n",
       "      <td>0.120618</td>\n",
       "      <td>-2.865819</td>\n",
       "      <td>-1.160358</td>\n",
       "      <td>1.075970</td>\n",
       "      <td>-0.279330</td>\n",
       "      <td>0.968927</td>\n",
       "      <td>0.894239</td>\n",
       "      <td>0.118092</td>\n",
       "      <td>...</td>\n",
       "      <td>0.699903</td>\n",
       "      <td>0.704648</td>\n",
       "      <td>0.248668</td>\n",
       "      <td>1.869159</td>\n",
       "      <td>-1.276542</td>\n",
       "      <td>-1.454294</td>\n",
       "      <td>0.794663</td>\n",
       "      <td>0.223589</td>\n",
       "      <td>-0.359712</td>\n",
       "      <td>-1.139427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-11</th>\n",
       "      <td>-5.379310</td>\n",
       "      <td>-4.970760</td>\n",
       "      <td>-2.031019</td>\n",
       "      <td>-11.425005</td>\n",
       "      <td>-2.557924</td>\n",
       "      <td>-1.398489</td>\n",
       "      <td>-1.680129</td>\n",
       "      <td>-0.959629</td>\n",
       "      <td>0.215424</td>\n",
       "      <td>-2.149295</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.314084</td>\n",
       "      <td>-0.422991</td>\n",
       "      <td>-1.573427</td>\n",
       "      <td>0.943396</td>\n",
       "      <td>-1.702075</td>\n",
       "      <td>-0.413793</td>\n",
       "      <td>-0.875231</td>\n",
       "      <td>-0.666297</td>\n",
       "      <td>-2.797203</td>\n",
       "      <td>-2.228079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-10</th>\n",
       "      <td>1.233419</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.236897</td>\n",
       "      <td>0.446035</td>\n",
       "      <td>1.658187</td>\n",
       "      <td>0.679722</td>\n",
       "      <td>0.226684</td>\n",
       "      <td>0.632701</td>\n",
       "      <td>1.744696</td>\n",
       "      <td>1.453693</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084219</td>\n",
       "      <td>2.207416</td>\n",
       "      <td>0.456621</td>\n",
       "      <td>-8.620690</td>\n",
       "      <td>0.274075</td>\n",
       "      <td>1.045296</td>\n",
       "      <td>-0.155355</td>\n",
       "      <td>-0.524717</td>\n",
       "      <td>1.490419</td>\n",
       "      <td>0.529865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 644 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              916328    936365    905271     905113    905802    905425  \\\n",
       "date                                                                      \n",
       "2016-10-14 -0.463641  4.006163  0.294874  -1.158691  0.349379 -0.367823   \n",
       "2016-10-13  0.000000 -1.963746 -0.414125  -1.328181 -0.846805  0.854839   \n",
       "2016-10-12 -0.437318  1.846154  0.120618  -2.865819 -1.160358  1.075970   \n",
       "2016-10-11 -5.379310 -4.970760 -2.031019 -11.425005 -2.557924 -1.398489   \n",
       "2016-10-10  1.233419  1.333333  0.236897   0.446035  1.658187  0.679722   \n",
       "\n",
       "              906156    916305    992816    921093  ...    311917    69568X  \\\n",
       "date                                                ...                       \n",
       "2016-10-14 -0.430392  0.453096  0.555651 -0.284024  ...  0.409130 -1.862765   \n",
       "2016-10-13 -0.461361 -1.406353 -0.306801 -0.330267  ... -0.684346 -0.630242   \n",
       "2016-10-12 -0.279330  0.968927  0.894239  0.118092  ...  0.699903  0.704648   \n",
       "2016-10-11 -1.680129 -0.959629  0.215424 -2.149295  ... -2.314084 -0.422991   \n",
       "2016-10-10  0.226684  0.632701  1.744696  1.453693  ...  0.084219  2.207416   \n",
       "\n",
       "              543755    77463M    29235J    131745    69487D    68157P  \\\n",
       "date                                                                     \n",
       "2016-10-14  0.351865  3.636364  0.351105 -1.812908 -0.432050  0.223839   \n",
       "2016-10-13  0.708717  0.917431 -1.549097 -3.092059 -0.875998 -0.334635   \n",
       "2016-10-12  0.248668  1.869159 -1.276542 -1.454294  0.794663  0.223589   \n",
       "2016-10-11 -1.573427  0.943396 -1.702075 -0.413793 -0.875231 -0.666297   \n",
       "2016-10-10  0.456621 -8.620690  0.274075  1.045296 -0.155355 -0.524717   \n",
       "\n",
       "              9110RA    292703  \n",
       "date                            \n",
       "2016-10-14 -1.865672 -0.444939  \n",
       "2016-10-13 -3.249097  0.272648  \n",
       "2016-10-12 -0.359712 -1.139427  \n",
       "2016-10-11 -2.797203 -2.228079  \n",
       "2016-10-10  1.490419  0.529865  \n",
       "\n",
       "[5 rows x 644 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>916328</th>\n",
       "      <th>936365</th>\n",
       "      <th>905271</th>\n",
       "      <th>905113</th>\n",
       "      <th>905802</th>\n",
       "      <th>905425</th>\n",
       "      <th>906156</th>\n",
       "      <th>916305</th>\n",
       "      <th>992816</th>\n",
       "      <th>921093</th>\n",
       "      <th>...</th>\n",
       "      <th>311917</th>\n",
       "      <th>69568X</th>\n",
       "      <th>543755</th>\n",
       "      <th>77463M</th>\n",
       "      <th>29235J</th>\n",
       "      <th>131745</th>\n",
       "      <th>69487D</th>\n",
       "      <th>68157P</th>\n",
       "      <th>9110RA</th>\n",
       "      <th>292703</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-05-24</th>\n",
       "      <td>1.632325</td>\n",
       "      <td>3.960396</td>\n",
       "      <td>1.150612</td>\n",
       "      <td>1.508398</td>\n",
       "      <td>-0.665643</td>\n",
       "      <td>1.080222</td>\n",
       "      <td>2.012895</td>\n",
       "      <td>1.405975</td>\n",
       "      <td>1.524422</td>\n",
       "      <td>3.981207</td>\n",
       "      <td>...</td>\n",
       "      <td>2.560616</td>\n",
       "      <td>0.781611</td>\n",
       "      <td>2.232519</td>\n",
       "      <td>-2.020202</td>\n",
       "      <td>2.325226</td>\n",
       "      <td>2.200825</td>\n",
       "      <td>2.945144</td>\n",
       "      <td>2.523060</td>\n",
       "      <td>3.903904</td>\n",
       "      <td>1.088886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-23</th>\n",
       "      <td>-0.611702</td>\n",
       "      <td>4.392765</td>\n",
       "      <td>0.162586</td>\n",
       "      <td>1.865755</td>\n",
       "      <td>1.612903</td>\n",
       "      <td>-1.332288</td>\n",
       "      <td>-0.516270</td>\n",
       "      <td>-0.122872</td>\n",
       "      <td>1.270741</td>\n",
       "      <td>1.531509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045341</td>\n",
       "      <td>-1.843109</td>\n",
       "      <td>0.211060</td>\n",
       "      <td>7.027027</td>\n",
       "      <td>1.949470</td>\n",
       "      <td>-0.068729</td>\n",
       "      <td>0.644869</td>\n",
       "      <td>2.360455</td>\n",
       "      <td>-2.489019</td>\n",
       "      <td>-0.118217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-20</th>\n",
       "      <td>0.966702</td>\n",
       "      <td>2.652520</td>\n",
       "      <td>-0.183509</td>\n",
       "      <td>-0.109406</td>\n",
       "      <td>0.698568</td>\n",
       "      <td>-0.078309</td>\n",
       "      <td>1.880778</td>\n",
       "      <td>1.082328</td>\n",
       "      <td>1.082803</td>\n",
       "      <td>3.805056</td>\n",
       "      <td>...</td>\n",
       "      <td>0.569995</td>\n",
       "      <td>2.355838</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.543478</td>\n",
       "      <td>3.219575</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.267731</td>\n",
       "      <td>2.768265</td>\n",
       "      <td>-0.582242</td>\n",
       "      <td>1.075397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-19</th>\n",
       "      <td>-1.350993</td>\n",
       "      <td>-1.822917</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.108760</td>\n",
       "      <td>-1.207729</td>\n",
       "      <td>0.868878</td>\n",
       "      <td>-1.227960</td>\n",
       "      <td>-0.617175</td>\n",
       "      <td>-0.380711</td>\n",
       "      <td>2.292722</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079863</td>\n",
       "      <td>1.913151</td>\n",
       "      <td>-0.339847</td>\n",
       "      <td>-0.540541</td>\n",
       "      <td>-1.412474</td>\n",
       "      <td>-2.348993</td>\n",
       "      <td>-0.689417</td>\n",
       "      <td>-1.072840</td>\n",
       "      <td>-1.079914</td>\n",
       "      <td>-0.274071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-18</th>\n",
       "      <td>0.106073</td>\n",
       "      <td>1.319261</td>\n",
       "      <td>-0.825861</td>\n",
       "      <td>-3.010928</td>\n",
       "      <td>0.520291</td>\n",
       "      <td>-1.875678</td>\n",
       "      <td>1.210962</td>\n",
       "      <td>2.364621</td>\n",
       "      <td>1.144507</td>\n",
       "      <td>0.053348</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034215</td>\n",
       "      <td>3.180885</td>\n",
       "      <td>-0.211954</td>\n",
       "      <td>3.351955</td>\n",
       "      <td>1.481720</td>\n",
       "      <td>-0.996678</td>\n",
       "      <td>2.690265</td>\n",
       "      <td>0.084770</td>\n",
       "      <td>3.889304</td>\n",
       "      <td>0.226920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 644 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              916328    936365    905271    905113    905802    905425  \\\n",
       "date                                                                     \n",
       "2016-05-24  1.632325  3.960396  1.150612  1.508398 -0.665643  1.080222   \n",
       "2016-05-23 -0.611702  4.392765  0.162586  1.865755  1.612903 -1.332288   \n",
       "2016-05-20  0.966702  2.652520 -0.183509 -0.109406  0.698568 -0.078309   \n",
       "2016-05-19 -1.350993 -1.822917  0.000000  1.108760 -1.207729  0.868878   \n",
       "2016-05-18  0.106073  1.319261 -0.825861 -3.010928  0.520291 -1.875678   \n",
       "\n",
       "              906156    916305    992816    921093  ...    311917    69568X  \\\n",
       "date                                                ...                       \n",
       "2016-05-24  2.012895  1.405975  1.524422  3.981207  ...  2.560616  0.781611   \n",
       "2016-05-23 -0.516270 -0.122872  1.270741  1.531509  ...  0.045341 -1.843109   \n",
       "2016-05-20  1.880778  1.082328  1.082803  3.805056  ...  0.569995  2.355838   \n",
       "2016-05-19 -1.227960 -0.617175 -0.380711  2.292722  ...  0.079863  1.913151   \n",
       "2016-05-18  1.210962  2.364621  1.144507  0.053348  ... -0.034215  3.180885   \n",
       "\n",
       "              543755    77463M    29235J    131745    69487D    68157P  \\\n",
       "date                                                                     \n",
       "2016-05-24  2.232519 -2.020202  2.325226  2.200825  2.945144  2.523060   \n",
       "2016-05-23  0.211060  7.027027  1.949470 -0.068729  0.644869  2.360455   \n",
       "2016-05-20  0.980392  0.543478  3.219575  0.000000  2.267731  2.768265   \n",
       "2016-05-19 -0.339847 -0.540541 -1.412474 -2.348993 -0.689417 -1.072840   \n",
       "2016-05-18 -0.211954  3.351955  1.481720 -0.996678  2.690265  0.084770   \n",
       "\n",
       "              9110RA    292703  \n",
       "date                            \n",
       "2016-05-24  3.903904  1.088886  \n",
       "2016-05-23 -2.489019 -0.118217  \n",
       "2016-05-20 -0.582242  1.075397  \n",
       "2016-05-19 -1.079914 -0.274071  \n",
       "2016-05-18  3.889304  0.226920  \n",
       "\n",
       "[5 rows x 644 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>634</th>\n",
       "      <th>635</th>\n",
       "      <th>636</th>\n",
       "      <th>637</th>\n",
       "      <th>638</th>\n",
       "      <th>639</th>\n",
       "      <th>640</th>\n",
       "      <th>641</th>\n",
       "      <th>642</th>\n",
       "      <th>643</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.681938</td>\n",
       "      <td>0.561247</td>\n",
       "      <td>0.534958</td>\n",
       "      <td>0.667773</td>\n",
       "      <td>0.285284</td>\n",
       "      <td>0.618457</td>\n",
       "      <td>0.386756</td>\n",
       "      <td>0.721853</td>\n",
       "      <td>0.342626</td>\n",
       "      <td>0.560088</td>\n",
       "      <td>...</td>\n",
       "      <td>0.546874</td>\n",
       "      <td>0.461215</td>\n",
       "      <td>0.451240</td>\n",
       "      <td>0.266433</td>\n",
       "      <td>0.399997</td>\n",
       "      <td>0.413940</td>\n",
       "      <td>0.525361</td>\n",
       "      <td>0.642595</td>\n",
       "      <td>0.145999</td>\n",
       "      <td>0.662292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.765102</td>\n",
       "      <td>0.635032</td>\n",
       "      <td>0.535151</td>\n",
       "      <td>0.674886</td>\n",
       "      <td>0.401511</td>\n",
       "      <td>0.767389</td>\n",
       "      <td>0.443800</td>\n",
       "      <td>0.696633</td>\n",
       "      <td>0.552781</td>\n",
       "      <td>0.577114</td>\n",
       "      <td>...</td>\n",
       "      <td>0.496270</td>\n",
       "      <td>0.505375</td>\n",
       "      <td>0.497635</td>\n",
       "      <td>0.602618</td>\n",
       "      <td>0.683300</td>\n",
       "      <td>0.429399</td>\n",
       "      <td>0.575786</td>\n",
       "      <td>0.582642</td>\n",
       "      <td>0.374450</td>\n",
       "      <td>0.589851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.683961</td>\n",
       "      <td>0.670126</td>\n",
       "      <td>0.580098</td>\n",
       "      <td>0.692984</td>\n",
       "      <td>0.441548</td>\n",
       "      <td>0.349568</td>\n",
       "      <td>0.436048</td>\n",
       "      <td>0.668657</td>\n",
       "      <td>0.483193</td>\n",
       "      <td>0.513660</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663689</td>\n",
       "      <td>0.644571</td>\n",
       "      <td>0.203700</td>\n",
       "      <td>0.427640</td>\n",
       "      <td>0.658137</td>\n",
       "      <td>0.370430</td>\n",
       "      <td>0.532337</td>\n",
       "      <td>0.701547</td>\n",
       "      <td>0.250746</td>\n",
       "      <td>0.628281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.454020</td>\n",
       "      <td>0.602122</td>\n",
       "      <td>0.493594</td>\n",
       "      <td>0.697097</td>\n",
       "      <td>0.464871</td>\n",
       "      <td>0.225748</td>\n",
       "      <td>0.464341</td>\n",
       "      <td>0.677619</td>\n",
       "      <td>0.461784</td>\n",
       "      <td>0.585610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.506489</td>\n",
       "      <td>0.526273</td>\n",
       "      <td>0.298684</td>\n",
       "      <td>0.269507</td>\n",
       "      <td>0.545343</td>\n",
       "      <td>0.520791</td>\n",
       "      <td>0.558434</td>\n",
       "      <td>0.627830</td>\n",
       "      <td>0.283049</td>\n",
       "      <td>0.598215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.664358</td>\n",
       "      <td>0.665677</td>\n",
       "      <td>0.535088</td>\n",
       "      <td>0.716366</td>\n",
       "      <td>0.370311</td>\n",
       "      <td>0.442874</td>\n",
       "      <td>0.400019</td>\n",
       "      <td>0.772208</td>\n",
       "      <td>0.485050</td>\n",
       "      <td>0.524863</td>\n",
       "      <td>...</td>\n",
       "      <td>0.516109</td>\n",
       "      <td>0.728590</td>\n",
       "      <td>0.563447</td>\n",
       "      <td>0.489621</td>\n",
       "      <td>0.526392</td>\n",
       "      <td>0.412264</td>\n",
       "      <td>0.443215</td>\n",
       "      <td>0.716474</td>\n",
       "      <td>0.224044</td>\n",
       "      <td>0.591428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 644 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.681938  0.561247  0.534958  0.667773  0.285284  0.618457  0.386756   \n",
       "1  0.765102  0.635032  0.535151  0.674886  0.401511  0.767389  0.443800   \n",
       "2  0.683961  0.670126  0.580098  0.692984  0.441548  0.349568  0.436048   \n",
       "3  0.454020  0.602122  0.493594  0.697097  0.464871  0.225748  0.464341   \n",
       "4  0.664358  0.665677  0.535088  0.716366  0.370311  0.442874  0.400019   \n",
       "\n",
       "        7         8         9    ...       634       635       636       637  \\\n",
       "0  0.721853  0.342626  0.560088  ...  0.546874  0.461215  0.451240  0.266433   \n",
       "1  0.696633  0.552781  0.577114  ...  0.496270  0.505375  0.497635  0.602618   \n",
       "2  0.668657  0.483193  0.513660  ...  0.663689  0.644571  0.203700  0.427640   \n",
       "3  0.677619  0.461784  0.585610  ...  0.506489  0.526273  0.298684  0.269507   \n",
       "4  0.772208  0.485050  0.524863  ...  0.516109  0.728590  0.563447  0.489621   \n",
       "\n",
       "        638       639       640       641       642       643  \n",
       "0  0.399997  0.413940  0.525361  0.642595  0.145999  0.662292  \n",
       "1  0.683300  0.429399  0.575786  0.582642  0.374450  0.589851  \n",
       "2  0.658137  0.370430  0.532337  0.701547  0.250746  0.628281  \n",
       "3  0.545343  0.520791  0.558434  0.627830  0.283049  0.598215  \n",
       "4  0.526392  0.412264  0.443215  0.716474  0.224044  0.591428  \n",
       "\n",
       "[5 rows x 644 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Scaling\n",
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "training_set_scaled = sc.fit_transform(train)\n",
    "test_set_scaled = sc.fit_transform(test)\n",
    "pd.DataFrame(training_set_scaled).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simple multi-layer percepetron (MLP) autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_38\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_32 (InputLayer)        [(None, 644)]             0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 20)                12900     \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 644)               13524     \n",
      "=================================================================\n",
      "Total params: 26,424\n",
      "Trainable params: 26,424\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2820 - acc: 0.0031 - mae: 0.4959 - val_loss: 0.2733 - val_acc: 0.0000e+00 - val_mae: 0.4859\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.2686 - acc: 0.0000e+00 - mae: 0.4830 - val_loss: 0.2630 - val_acc: 0.0000e+00 - val_mae: 0.4746\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.2584 - acc: 0.0000e+00 - mae: 0.4717 - val_loss: 0.2550 - val_acc: 0.0000e+00 - val_mae: 0.4635\n",
      "Epoch 4/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.2504 - acc: 0.0000e+00 - mae: 0.4607 - val_loss: 0.2465 - val_acc: 0.0000e+00 - val_mae: 0.4513\n",
      "Epoch 5/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.2420 - acc: 0.0000e+00 - mae: 0.4485 - val_loss: 0.2363 - val_acc: 0.0000e+00 - val_mae: 0.4374\n",
      "Epoch 6/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.2319 - acc: 0.0000e+00 - mae: 0.4345 - val_loss: 0.2249 - val_acc: 0.0000e+00 - val_mae: 0.4222\n",
      "Epoch 7/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.2206 - acc: 0.0000e+00 - mae: 0.4194 - val_loss: 0.2132 - val_acc: 0.0000e+00 - val_mae: 0.4066\n",
      "Epoch 8/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.2089 - acc: 0.0000e+00 - mae: 0.4036 - val_loss: 0.2014 - val_acc: 0.0000e+00 - val_mae: 0.3907\n",
      "Epoch 9/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1973 - acc: 0.0000e+00 - mae: 0.3877 - val_loss: 0.1897 - val_acc: 0.0000e+00 - val_mae: 0.3750\n",
      "Epoch 10/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.1857 - acc: 0.0000e+00 - mae: 0.3719 - val_loss: 0.1783 - val_acc: 0.0000e+00 - val_mae: 0.3596\n",
      "Epoch 11/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1744 - acc: 0.0000e+00 - mae: 0.3565 - val_loss: 0.1672 - val_acc: 0.0000e+00 - val_mae: 0.3445\n",
      "Epoch 12/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1634 - acc: 0.0000e+00 - mae: 0.3414 - val_loss: 0.1565 - val_acc: 0.0000e+00 - val_mae: 0.3297\n",
      "Epoch 13/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1528 - acc: 0.0000e+00 - mae: 0.3266 - val_loss: 0.1462 - val_acc: 0.0000e+00 - val_mae: 0.3150\n",
      "Epoch 14/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1426 - acc: 0.0000e+00 - mae: 0.3119 - val_loss: 0.1363 - val_acc: 0.0000e+00 - val_mae: 0.3006\n",
      "Epoch 15/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1328 - acc: 0.0000e+00 - mae: 0.2975 - val_loss: 0.1267 - val_acc: 0.0000e+00 - val_mae: 0.2864\n",
      "Epoch 16/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1232 - acc: 0.0000e+00 - mae: 0.2833 - val_loss: 0.1174 - val_acc: 0.0000e+00 - val_mae: 0.2725\n",
      "Epoch 17/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1140 - acc: 0.0000e+00 - mae: 0.2694 - val_loss: 0.1085 - val_acc: 0.0000e+00 - val_mae: 0.2590\n",
      "Epoch 18/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1052 - acc: 0.0000e+00 - mae: 0.2558 - val_loss: 0.1001 - val_acc: 0.0000e+00 - val_mae: 0.2460\n",
      "Epoch 19/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0969 - acc: 0.0000e+00 - mae: 0.2428 - val_loss: 0.0922 - val_acc: 0.0000e+00 - val_mae: 0.2337\n",
      "Epoch 20/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0892 - acc: 0.0000e+00 - mae: 0.2305 - val_loss: 0.0850 - val_acc: 0.0000e+00 - val_mae: 0.2223\n",
      "Epoch 21/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0820 - acc: 0.0000e+00 - mae: 0.2190 - val_loss: 0.0783 - val_acc: 0.0000e+00 - val_mae: 0.2115\n",
      "Epoch 22/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0754 - acc: 0.0000e+00 - mae: 0.2081 - val_loss: 0.0720 - val_acc: 0.0000e+00 - val_mae: 0.2012\n",
      "Epoch 23/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0692 - acc: 0.0000e+00 - mae: 0.1978 - val_loss: 0.0663 - val_acc: 0.0000e+00 - val_mae: 0.1915\n",
      "Epoch 24/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0635 - acc: 0.0000e+00 - mae: 0.1880 - val_loss: 0.0609 - val_acc: 0.0000e+00 - val_mae: 0.1822\n",
      "Epoch 25/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0582 - acc: 0.0000e+00 - mae: 0.1787 - val_loss: 0.0559 - val_acc: 0.0000e+00 - val_mae: 0.1734\n",
      "Epoch 26/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0533 - acc: 0.0000e+00 - mae: 0.1698 - val_loss: 0.0514 - val_acc: 0.0000e+00 - val_mae: 0.1652\n",
      "Epoch 27/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0488 - acc: 0.0000e+00 - mae: 0.1614 - val_loss: 0.0472 - val_acc: 0.0125 - val_mae: 0.1575\n",
      "Epoch 28/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0447 - acc: 0.0000e+00 - mae: 0.1537 - val_loss: 0.0435 - val_acc: 0.0125 - val_mae: 0.1505\n",
      "Epoch 29/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0410 - acc: 0.0000e+00 - mae: 0.1465 - val_loss: 0.0401 - val_acc: 0.0000e+00 - val_mae: 0.1440\n",
      "Epoch 30/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0377 - acc: 0.0063 - mae: 0.1399 - val_loss: 0.0371 - val_acc: 0.0000e+00 - val_mae: 0.1381\n",
      "Epoch 31/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0347 - acc: 0.0063 - mae: 0.1338 - val_loss: 0.0343 - val_acc: 0.0000e+00 - val_mae: 0.1325\n",
      "Epoch 32/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0320 - acc: 0.0063 - mae: 0.1281 - val_loss: 0.0319 - val_acc: 0.0000e+00 - val_mae: 0.1274\n",
      "Epoch 33/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0296 - acc: 0.0063 - mae: 0.1228 - val_loss: 0.0296 - val_acc: 0.0000e+00 - val_mae: 0.1226\n",
      "Epoch 34/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0274 - acc: 0.0063 - mae: 0.1179 - val_loss: 0.0276 - val_acc: 0.0000e+00 - val_mae: 0.1181\n",
      "Epoch 35/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0254 - acc: 0.0063 - mae: 0.1134 - val_loss: 0.0258 - val_acc: 0.0000e+00 - val_mae: 0.1141\n",
      "Epoch 36/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0237 - acc: 0.0063 - mae: 0.1092 - val_loss: 0.0243 - val_acc: 0.0000e+00 - val_mae: 0.1103\n",
      "Epoch 37/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0221 - acc: 0.0063 - mae: 0.1054 - val_loss: 0.0229 - val_acc: 0.0000e+00 - val_mae: 0.1070\n",
      "Epoch 38/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0207 - acc: 0.0063 - mae: 0.1019 - val_loss: 0.0216 - val_acc: 0.0000e+00 - val_mae: 0.1039\n",
      "Epoch 39/500\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0195 - acc: 0.0000e+00 - mae: 0.0988 - val_loss: 0.0206 - val_acc: 0.0000e+00 - val_mae: 0.1012\n",
      "Epoch 40/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0185 - acc: 0.0000e+00 - mae: 0.0961 - val_loss: 0.0196 - val_acc: 0.0000e+00 - val_mae: 0.0988\n",
      "Epoch 41/500\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0175 - acc: 0.0000e+00 - mae: 0.0936 - val_loss: 0.0188 - val_acc: 0.0000e+00 - val_mae: 0.0966\n",
      "Epoch 42/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0167 - acc: 0.0000e+00 - mae: 0.0914 - val_loss: 0.0180 - val_acc: 0.0000e+00 - val_mae: 0.0947\n",
      "Epoch 43/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0160 - acc: 0.0000e+00 - mae: 0.0894 - val_loss: 0.0174 - val_acc: 0.0000e+00 - val_mae: 0.0929\n",
      "Epoch 44/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0153 - acc: 0.0000e+00 - mae: 0.0876 - val_loss: 0.0168 - val_acc: 0.0000e+00 - val_mae: 0.0913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0148 - acc: 0.0000e+00 - mae: 0.0859 - val_loss: 0.0163 - val_acc: 0.0000e+00 - val_mae: 0.0899\n",
      "Epoch 46/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0143 - acc: 0.0000e+00 - mae: 0.0845 - val_loss: 0.0158 - val_acc: 0.0000e+00 - val_mae: 0.0886\n",
      "Epoch 47/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0138 - acc: 0.0063 - mae: 0.0831 - val_loss: 0.0154 - val_acc: 0.0000e+00 - val_mae: 0.0874\n",
      "Epoch 48/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0134 - acc: 0.0063 - mae: 0.0820 - val_loss: 0.0151 - val_acc: 0.0000e+00 - val_mae: 0.0863\n",
      "Epoch 49/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0131 - acc: 0.0094 - mae: 0.0809 - val_loss: 0.0148 - val_acc: 0.0000e+00 - val_mae: 0.0853\n",
      "Epoch 50/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0128 - acc: 0.0094 - mae: 0.0799 - val_loss: 0.0145 - val_acc: 0.0000e+00 - val_mae: 0.0844\n",
      "Epoch 51/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0125 - acc: 0.0094 - mae: 0.0789 - val_loss: 0.0143 - val_acc: 0.0000e+00 - val_mae: 0.0835\n",
      "Epoch 52/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0123 - acc: 0.0094 - mae: 0.0781 - val_loss: 0.0140 - val_acc: 0.0000e+00 - val_mae: 0.0827\n",
      "Epoch 53/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0120 - acc: 0.0094 - mae: 0.0773 - val_loss: 0.0138 - val_acc: 0.0000e+00 - val_mae: 0.0820\n",
      "Epoch 54/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0118 - acc: 0.0094 - mae: 0.0766 - val_loss: 0.0137 - val_acc: 0.0000e+00 - val_mae: 0.0813\n",
      "Epoch 55/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0117 - acc: 0.0094 - mae: 0.0759 - val_loss: 0.0135 - val_acc: 0.0000e+00 - val_mae: 0.0807\n",
      "Epoch 56/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0115 - acc: 0.0094 - mae: 0.0753 - val_loss: 0.0133 - val_acc: 0.0000e+00 - val_mae: 0.0801\n",
      "Epoch 57/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0114 - acc: 0.0094 - mae: 0.0747 - val_loss: 0.0132 - val_acc: 0.0000e+00 - val_mae: 0.0796\n",
      "Epoch 58/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0112 - acc: 0.0094 - mae: 0.0742 - val_loss: 0.0131 - val_acc: 0.0000e+00 - val_mae: 0.0791\n",
      "Epoch 59/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0111 - acc: 0.0094 - mae: 0.0738 - val_loss: 0.0130 - val_acc: 0.0000e+00 - val_mae: 0.0786\n",
      "Epoch 60/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0110 - acc: 0.0094 - mae: 0.0733 - val_loss: 0.0129 - val_acc: 0.0000e+00 - val_mae: 0.0782\n",
      "Epoch 61/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0109 - acc: 0.0094 - mae: 0.0729 - val_loss: 0.0128 - val_acc: 0.0000e+00 - val_mae: 0.0778\n",
      "Epoch 62/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0108 - acc: 0.0094 - mae: 0.0726 - val_loss: 0.0127 - val_acc: 0.0000e+00 - val_mae: 0.0775\n",
      "Epoch 63/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0107 - acc: 0.0094 - mae: 0.0722 - val_loss: 0.0127 - val_acc: 0.0000e+00 - val_mae: 0.0771\n",
      "Epoch 64/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0107 - acc: 0.0031 - mae: 0.0719 - val_loss: 0.0126 - val_acc: 0.0000e+00 - val_mae: 0.0768\n",
      "Epoch 65/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0106 - acc: 0.0063 - mae: 0.0716 - val_loss: 0.0125 - val_acc: 0.0000e+00 - val_mae: 0.0766\n",
      "Epoch 66/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0105 - acc: 0.0063 - mae: 0.0714 - val_loss: 0.0125 - val_acc: 0.0000e+00 - val_mae: 0.0763\n",
      "Epoch 67/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0105 - acc: 0.0156 - mae: 0.0711 - val_loss: 0.0124 - val_acc: 0.0250 - val_mae: 0.0761\n",
      "Epoch 68/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0104 - acc: 0.0375 - mae: 0.0709 - val_loss: 0.0124 - val_acc: 0.1000 - val_mae: 0.0758\n",
      "Epoch 69/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0104 - acc: 0.0938 - mae: 0.0707 - val_loss: 0.0123 - val_acc: 0.1250 - val_mae: 0.0756\n",
      "Epoch 70/500\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0103 - acc: 0.1375 - mae: 0.070 - 0s 28ms/step - loss: 0.0103 - acc: 0.1375 - mae: 0.0705 - val_loss: 0.0123 - val_acc: 0.1250 - val_mae: 0.0755\n",
      "Epoch 71/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0103 - acc: 0.1500 - mae: 0.0703 - val_loss: 0.0123 - val_acc: 0.1375 - val_mae: 0.0753\n",
      "Epoch 72/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0102 - acc: 0.1656 - mae: 0.0701 - val_loss: 0.0122 - val_acc: 0.1500 - val_mae: 0.0752\n",
      "Epoch 73/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0102 - acc: 0.1813 - mae: 0.0699 - val_loss: 0.0122 - val_acc: 0.1500 - val_mae: 0.0750\n",
      "Epoch 74/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0102 - acc: 0.1813 - mae: 0.0698 - val_loss: 0.0122 - val_acc: 0.1500 - val_mae: 0.0749\n",
      "Epoch 75/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0102 - acc: 0.1813 - mae: 0.0697 - val_loss: 0.0122 - val_acc: 0.1500 - val_mae: 0.0748\n",
      "Epoch 76/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0101 - acc: 0.1844 - mae: 0.0696 - val_loss: 0.0122 - val_acc: 0.1500 - val_mae: 0.0747\n",
      "Epoch 77/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0101 - acc: 0.1875 - mae: 0.0695 - val_loss: 0.0121 - val_acc: 0.1500 - val_mae: 0.0747\n",
      "Epoch 78/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0101 - acc: 0.1875 - mae: 0.0694 - val_loss: 0.0121 - val_acc: 0.1500 - val_mae: 0.0746\n",
      "Epoch 79/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0101 - acc: 0.1875 - mae: 0.0693 - val_loss: 0.0121 - val_acc: 0.1500 - val_mae: 0.0745\n",
      "Epoch 80/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0101 - acc: 0.1875 - mae: 0.0692 - val_loss: 0.0121 - val_acc: 0.1500 - val_mae: 0.0745\n",
      "Epoch 81/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0100 - acc: 0.1875 - mae: 0.0692 - val_loss: 0.0121 - val_acc: 0.1500 - val_mae: 0.0744\n",
      "Epoch 82/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0100 - acc: 0.1875 - mae: 0.0691 - val_loss: 0.0121 - val_acc: 0.1500 - val_mae: 0.0744\n",
      "Epoch 83/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0100 - acc: 0.1875 - mae: 0.0690 - val_loss: 0.0121 - val_acc: 0.1500 - val_mae: 0.0743\n",
      "Epoch 84/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0100 - acc: 0.1875 - mae: 0.0690 - val_loss: 0.0120 - val_acc: 0.1500 - val_mae: 0.0743\n",
      "Epoch 85/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0100 - acc: 0.1875 - mae: 0.0689 - val_loss: 0.0120 - val_acc: 0.1500 - val_mae: 0.0742\n",
      "Epoch 86/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0100 - acc: 0.1875 - mae: 0.0689 - val_loss: 0.0120 - val_acc: 0.1500 - val_mae: 0.0742\n",
      "Epoch 87/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0100 - acc: 0.1875 - mae: 0.0688 - val_loss: 0.0120 - val_acc: 0.1500 - val_mae: 0.0742\n",
      "Epoch 88/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0099 - acc: 0.1875 - mae: 0.0688 - val_loss: 0.0120 - val_acc: 0.1500 - val_mae: 0.0741\n",
      "Epoch 89/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0099 - acc: 0.1875 - mae: 0.0687 - val_loss: 0.0120 - val_acc: 0.1500 - val_mae: 0.0741\n",
      "Epoch 90/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0099 - acc: 0.1875 - mae: 0.0687 - val_loss: 0.0120 - val_acc: 0.1500 - val_mae: 0.0741\n",
      "Epoch 91/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0099 - acc: 0.1875 - mae: 0.0687 - val_loss: 0.0120 - val_acc: 0.1500 - val_mae: 0.0740\n",
      "Epoch 92/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0099 - acc: 0.1875 - mae: 0.0686 - val_loss: 0.0120 - val_acc: 0.1500 - val_mae: 0.0740\n",
      "Epoch 93/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0099 - acc: 0.1875 - mae: 0.0686 - val_loss: 0.0120 - val_acc: 0.1500 - val_mae: 0.0740\n",
      "Epoch 94/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0099 - acc: 0.1875 - mae: 0.0685 - val_loss: 0.0120 - val_acc: 0.1500 - val_mae: 0.0739\n",
      "Epoch 95/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0099 - acc: 0.1875 - mae: 0.0685 - val_loss: 0.0120 - val_acc: 0.1500 - val_mae: 0.0739\n",
      "Epoch 96/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0099 - acc: 0.1875 - mae: 0.0685 - val_loss: 0.0120 - val_acc: 0.1500 - val_mae: 0.0739\n",
      "Epoch 97/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0099 - acc: 0.1875 - mae: 0.0684 - val_loss: 0.0119 - val_acc: 0.1500 - val_mae: 0.0739\n",
      "Epoch 98/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0099 - acc: 0.1875 - mae: 0.0684 - val_loss: 0.0119 - val_acc: 0.1500 - val_mae: 0.0738\n",
      "Epoch 99/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0098 - acc: 0.1875 - mae: 0.0684 - val_loss: 0.0119 - val_acc: 0.1500 - val_mae: 0.0738\n",
      "Epoch 100/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0098 - acc: 0.1875 - mae: 0.0683 - val_loss: 0.0119 - val_acc: 0.1500 - val_mae: 0.0738\n",
      "Epoch 101/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0098 - acc: 0.1875 - mae: 0.0683 - val_loss: 0.0119 - val_acc: 0.1500 - val_mae: 0.0737\n",
      "Epoch 102/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0098 - acc: 0.1875 - mae: 0.0683 - val_loss: 0.0119 - val_acc: 0.1500 - val_mae: 0.0737\n",
      "Epoch 103/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0098 - acc: 0.1875 - mae: 0.0682 - val_loss: 0.0119 - val_acc: 0.1500 - val_mae: 0.0737\n",
      "Epoch 104/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0098 - acc: 0.1875 - mae: 0.0682 - val_loss: 0.0119 - val_acc: 0.1500 - val_mae: 0.0736\n",
      "Epoch 105/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0098 - acc: 0.1875 - mae: 0.0682 - val_loss: 0.0119 - val_acc: 0.1500 - val_mae: 0.0736\n",
      "Epoch 106/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0098 - acc: 0.1875 - mae: 0.0681 - val_loss: 0.0119 - val_acc: 0.1500 - val_mae: 0.0736\n",
      "Epoch 107/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0098 - acc: 0.1875 - mae: 0.0681 - val_loss: 0.0119 - val_acc: 0.1500 - val_mae: 0.0735\n",
      "Epoch 108/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0098 - acc: 0.1844 - mae: 0.0680 - val_loss: 0.0119 - val_acc: 0.1500 - val_mae: 0.0735\n",
      "Epoch 109/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0097 - acc: 0.1844 - mae: 0.0680 - val_loss: 0.0119 - val_acc: 0.1500 - val_mae: 0.0735\n",
      "Epoch 110/500\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0097 - acc: 0.1844 - mae: 0.0679 - val_loss: 0.0119 - val_acc: 0.1500 - val_mae: 0.0734\n",
      "Epoch 111/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0097 - acc: 0.1844 - mae: 0.0679 - val_loss: 0.0118 - val_acc: 0.1500 - val_mae: 0.0734\n",
      "Epoch 112/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0097 - acc: 0.1844 - mae: 0.0678 - val_loss: 0.0118 - val_acc: 0.1500 - val_mae: 0.0733\n",
      "Epoch 113/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0097 - acc: 0.1844 - mae: 0.0678 - val_loss: 0.0118 - val_acc: 0.1500 - val_mae: 0.0733\n",
      "Epoch 114/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0097 - acc: 0.1844 - mae: 0.0677 - val_loss: 0.0118 - val_acc: 0.1500 - val_mae: 0.0733\n",
      "Epoch 115/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0097 - acc: 0.1844 - mae: 0.0677 - val_loss: 0.0118 - val_acc: 0.1500 - val_mae: 0.0733\n",
      "Epoch 116/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0096 - acc: 0.1844 - mae: 0.0677 - val_loss: 0.0118 - val_acc: 0.1500 - val_mae: 0.0733\n",
      "Epoch 117/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0096 - acc: 0.1844 - mae: 0.0677 - val_loss: 0.0118 - val_acc: 0.1500 - val_mae: 0.0732\n",
      "Epoch 118/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0096 - acc: 0.1844 - mae: 0.0676 - val_loss: 0.0118 - val_acc: 0.1500 - val_mae: 0.0731\n",
      "Epoch 119/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0096 - acc: 0.1844 - mae: 0.0675 - val_loss: 0.0118 - val_acc: 0.1500 - val_mae: 0.0731\n",
      "Epoch 120/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0096 - acc: 0.1844 - mae: 0.0674 - val_loss: 0.0118 - val_acc: 0.1500 - val_mae: 0.0730\n",
      "Epoch 121/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0096 - acc: 0.1844 - mae: 0.0673 - val_loss: 0.0118 - val_acc: 0.1500 - val_mae: 0.0730\n",
      "Epoch 122/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0095 - acc: 0.1844 - mae: 0.0673 - val_loss: 0.0117 - val_acc: 0.1500 - val_mae: 0.0729\n",
      "Epoch 123/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0095 - acc: 0.1844 - mae: 0.0672 - val_loss: 0.0117 - val_acc: 0.1500 - val_mae: 0.0729\n",
      "Epoch 124/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0095 - acc: 0.1844 - mae: 0.0671 - val_loss: 0.0117 - val_acc: 0.1500 - val_mae: 0.0728\n",
      "Epoch 125/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0095 - acc: 0.1844 - mae: 0.0670 - val_loss: 0.0117 - val_acc: 0.1500 - val_mae: 0.0728\n",
      "Epoch 126/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0095 - acc: 0.1844 - mae: 0.0670 - val_loss: 0.0117 - val_acc: 0.1500 - val_mae: 0.0727\n",
      "Epoch 127/500\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0094 - acc: 0.1844 - mae: 0.0669 - val_loss: 0.0117 - val_acc: 0.1500 - val_mae: 0.0727\n",
      "Epoch 128/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0094 - acc: 0.1844 - mae: 0.0669 - val_loss: 0.0117 - val_acc: 0.1500 - val_mae: 0.0727\n",
      "Epoch 129/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0094 - acc: 0.1844 - mae: 0.0668 - val_loss: 0.0117 - val_acc: 0.1500 - val_mae: 0.0727\n",
      "Epoch 130/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0094 - acc: 0.1844 - mae: 0.0668 - val_loss: 0.0117 - val_acc: 0.1500 - val_mae: 0.0727\n",
      "Epoch 131/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0094 - acc: 0.1844 - mae: 0.0667 - val_loss: 0.0117 - val_acc: 0.1500 - val_mae: 0.0726\n",
      "Epoch 132/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0094 - acc: 0.1844 - mae: 0.0667 - val_loss: 0.0117 - val_acc: 0.1500 - val_mae: 0.0725\n",
      "Epoch 133/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0093 - acc: 0.1844 - mae: 0.0666 - val_loss: 0.0116 - val_acc: 0.1500 - val_mae: 0.0725\n",
      "Epoch 134/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0093 - acc: 0.1844 - mae: 0.0664 - val_loss: 0.0116 - val_acc: 0.1500 - val_mae: 0.0724\n",
      "Epoch 135/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0093 - acc: 0.1844 - mae: 0.0664 - val_loss: 0.0116 - val_acc: 0.1500 - val_mae: 0.0723\n",
      "Epoch 136/500\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0093 - acc: 0.1844 - mae: 0.0663 - val_loss: 0.0116 - val_acc: 0.1500 - val_mae: 0.0722\n",
      "Epoch 137/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0093 - acc: 0.1844 - mae: 0.0662 - val_loss: 0.0116 - val_acc: 0.1500 - val_mae: 0.0722\n",
      "Epoch 138/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0093 - acc: 0.1844 - mae: 0.0662 - val_loss: 0.0116 - val_acc: 0.1500 - val_mae: 0.0722\n",
      "Epoch 139/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0093 - acc: 0.1844 - mae: 0.0662 - val_loss: 0.0116 - val_acc: 0.1500 - val_mae: 0.0721\n",
      "Epoch 140/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0092 - acc: 0.1844 - mae: 0.0661 - val_loss: 0.0115 - val_acc: 0.1500 - val_mae: 0.0720\n",
      "Epoch 141/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0092 - acc: 0.1844 - mae: 0.0660 - val_loss: 0.0115 - val_acc: 0.1500 - val_mae: 0.0720\n",
      "Epoch 142/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0092 - acc: 0.1844 - mae: 0.0660 - val_loss: 0.0115 - val_acc: 0.1500 - val_mae: 0.0720\n",
      "Epoch 143/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0092 - acc: 0.1844 - mae: 0.0659 - val_loss: 0.0115 - val_acc: 0.1500 - val_mae: 0.0720\n",
      "Epoch 144/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0092 - acc: 0.1844 - mae: 0.0659 - val_loss: 0.0115 - val_acc: 0.1500 - val_mae: 0.0719\n",
      "Epoch 145/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0092 - acc: 0.1844 - mae: 0.0658 - val_loss: 0.0115 - val_acc: 0.1500 - val_mae: 0.0718\n",
      "Epoch 146/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0092 - acc: 0.1813 - mae: 0.0657 - val_loss: 0.0115 - val_acc: 0.1500 - val_mae: 0.0718\n",
      "Epoch 147/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0091 - acc: 0.1813 - mae: 0.0657 - val_loss: 0.0115 - val_acc: 0.1500 - val_mae: 0.0718\n",
      "Epoch 148/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0091 - acc: 0.1813 - mae: 0.0656 - val_loss: 0.0115 - val_acc: 0.1500 - val_mae: 0.0717\n",
      "Epoch 149/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0091 - acc: 0.1813 - mae: 0.0656 - val_loss: 0.0115 - val_acc: 0.1500 - val_mae: 0.0717\n",
      "Epoch 150/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0091 - acc: 0.1813 - mae: 0.0655 - val_loss: 0.0115 - val_acc: 0.1500 - val_mae: 0.0716\n",
      "Epoch 151/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0091 - acc: 0.1813 - mae: 0.0655 - val_loss: 0.0114 - val_acc: 0.1500 - val_mae: 0.0716\n",
      "Epoch 152/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0091 - acc: 0.1813 - mae: 0.0654 - val_loss: 0.0114 - val_acc: 0.1500 - val_mae: 0.0716\n",
      "Epoch 153/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0091 - acc: 0.1813 - mae: 0.0654 - val_loss: 0.0114 - val_acc: 0.1500 - val_mae: 0.0715\n",
      "Epoch 154/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0091 - acc: 0.1813 - mae: 0.0653 - val_loss: 0.0114 - val_acc: 0.1500 - val_mae: 0.0715\n",
      "Epoch 155/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0091 - acc: 0.1813 - mae: 0.0653 - val_loss: 0.0114 - val_acc: 0.1500 - val_mae: 0.0714\n",
      "Epoch 156/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0090 - acc: 0.1813 - mae: 0.0653 - val_loss: 0.0114 - val_acc: 0.1500 - val_mae: 0.0714\n",
      "Epoch 157/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0090 - acc: 0.1813 - mae: 0.0652 - val_loss: 0.0114 - val_acc: 0.1500 - val_mae: 0.0714\n",
      "Epoch 158/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0090 - acc: 0.1813 - mae: 0.0652 - val_loss: 0.0114 - val_acc: 0.1500 - val_mae: 0.0713\n",
      "Epoch 159/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0090 - acc: 0.1813 - mae: 0.0651 - val_loss: 0.0114 - val_acc: 0.1500 - val_mae: 0.0713\n",
      "Epoch 160/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0090 - acc: 0.1813 - mae: 0.0651 - val_loss: 0.0114 - val_acc: 0.1500 - val_mae: 0.0713\n",
      "Epoch 161/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0090 - acc: 0.1813 - mae: 0.0651 - val_loss: 0.0114 - val_acc: 0.1500 - val_mae: 0.0713\n",
      "Epoch 162/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0090 - acc: 0.1813 - mae: 0.0650 - val_loss: 0.0114 - val_acc: 0.1500 - val_mae: 0.0712\n",
      "Epoch 163/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0090 - acc: 0.1813 - mae: 0.0650 - val_loss: 0.0113 - val_acc: 0.1500 - val_mae: 0.0712\n",
      "Epoch 164/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0090 - acc: 0.1813 - mae: 0.0649 - val_loss: 0.0113 - val_acc: 0.1500 - val_mae: 0.0712\n",
      "Epoch 165/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0090 - acc: 0.1813 - mae: 0.0649 - val_loss: 0.0113 - val_acc: 0.1500 - val_mae: 0.0711\n",
      "Epoch 166/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0090 - acc: 0.1813 - mae: 0.0649 - val_loss: 0.0113 - val_acc: 0.1500 - val_mae: 0.0711\n",
      "Epoch 167/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0089 - acc: 0.1813 - mae: 0.0648 - val_loss: 0.0113 - val_acc: 0.1500 - val_mae: 0.0711\n",
      "Epoch 168/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0089 - acc: 0.1813 - mae: 0.0648 - val_loss: 0.0113 - val_acc: 0.1500 - val_mae: 0.0710\n",
      "Epoch 169/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0089 - acc: 0.1813 - mae: 0.0648 - val_loss: 0.0113 - val_acc: 0.1500 - val_mae: 0.0710\n",
      "Epoch 170/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0089 - acc: 0.1813 - mae: 0.0647 - val_loss: 0.0113 - val_acc: 0.1500 - val_mae: 0.0710\n",
      "Epoch 171/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0089 - acc: 0.1813 - mae: 0.0647 - val_loss: 0.0113 - val_acc: 0.1500 - val_mae: 0.0710\n",
      "Epoch 172/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0089 - acc: 0.1813 - mae: 0.0647 - val_loss: 0.0113 - val_acc: 0.1500 - val_mae: 0.0709\n",
      "Epoch 173/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0089 - acc: 0.1781 - mae: 0.0646 - val_loss: 0.0113 - val_acc: 0.1500 - val_mae: 0.0709\n",
      "Epoch 174/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0089 - acc: 0.1781 - mae: 0.0646 - val_loss: 0.0113 - val_acc: 0.1500 - val_mae: 0.0709\n",
      "Epoch 175/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0089 - acc: 0.1781 - mae: 0.0646 - val_loss: 0.0113 - val_acc: 0.1500 - val_mae: 0.0708\n",
      "Epoch 176/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0089 - acc: 0.1781 - mae: 0.0645 - val_loss: 0.0113 - val_acc: 0.1500 - val_mae: 0.0708\n",
      "Epoch 177/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0089 - acc: 0.1781 - mae: 0.0645 - val_loss: 0.0113 - val_acc: 0.1500 - val_mae: 0.0708\n",
      "Epoch 178/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0089 - acc: 0.1781 - mae: 0.0644 - val_loss: 0.0112 - val_acc: 0.1500 - val_mae: 0.0708\n",
      "Epoch 179/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0088 - acc: 0.1781 - mae: 0.0644 - val_loss: 0.0112 - val_acc: 0.1500 - val_mae: 0.0707\n",
      "Epoch 180/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0088 - acc: 0.1781 - mae: 0.0644 - val_loss: 0.0112 - val_acc: 0.1500 - val_mae: 0.0707\n",
      "Epoch 181/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0088 - acc: 0.1781 - mae: 0.0643 - val_loss: 0.0112 - val_acc: 0.1500 - val_mae: 0.0707\n",
      "Epoch 182/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0088 - acc: 0.1781 - mae: 0.0643 - val_loss: 0.0112 - val_acc: 0.1500 - val_mae: 0.0707\n",
      "Epoch 183/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0088 - acc: 0.1781 - mae: 0.0643 - val_loss: 0.0112 - val_acc: 0.1500 - val_mae: 0.0706\n",
      "Epoch 184/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0088 - acc: 0.1750 - mae: 0.0642 - val_loss: 0.0112 - val_acc: 0.1500 - val_mae: 0.0706\n",
      "Epoch 185/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0088 - acc: 0.1750 - mae: 0.0642 - val_loss: 0.0112 - val_acc: 0.1500 - val_mae: 0.0706\n",
      "Epoch 186/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0088 - acc: 0.1750 - mae: 0.0642 - val_loss: 0.0112 - val_acc: 0.1500 - val_mae: 0.0705\n",
      "Epoch 187/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0088 - acc: 0.1750 - mae: 0.0641 - val_loss: 0.0112 - val_acc: 0.1500 - val_mae: 0.0705\n",
      "Epoch 188/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0088 - acc: 0.1750 - mae: 0.0641 - val_loss: 0.0112 - val_acc: 0.1500 - val_mae: 0.0705\n",
      "Epoch 189/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0088 - acc: 0.1750 - mae: 0.0641 - val_loss: 0.0112 - val_acc: 0.1500 - val_mae: 0.0705\n",
      "Epoch 190/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0087 - acc: 0.1750 - mae: 0.0640 - val_loss: 0.0112 - val_acc: 0.1500 - val_mae: 0.0704\n",
      "Epoch 191/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0087 - acc: 0.1750 - mae: 0.0640 - val_loss: 0.0112 - val_acc: 0.1500 - val_mae: 0.0704\n",
      "Epoch 192/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0087 - acc: 0.1750 - mae: 0.0640 - val_loss: 0.0111 - val_acc: 0.1500 - val_mae: 0.0704\n",
      "Epoch 193/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0087 - acc: 0.1750 - mae: 0.0639 - val_loss: 0.0111 - val_acc: 0.1500 - val_mae: 0.0704\n",
      "Epoch 194/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0087 - acc: 0.1750 - mae: 0.0639 - val_loss: 0.0111 - val_acc: 0.1500 - val_mae: 0.0703\n",
      "Epoch 195/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0087 - acc: 0.1750 - mae: 0.0639 - val_loss: 0.0111 - val_acc: 0.1500 - val_mae: 0.0703\n",
      "Epoch 196/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0087 - acc: 0.1750 - mae: 0.0638 - val_loss: 0.0111 - val_acc: 0.1500 - val_mae: 0.0703\n",
      "Epoch 197/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0087 - acc: 0.1750 - mae: 0.0638 - val_loss: 0.0111 - val_acc: 0.1500 - val_mae: 0.0703\n",
      "Epoch 198/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0087 - acc: 0.1750 - mae: 0.0638 - val_loss: 0.0111 - val_acc: 0.1500 - val_mae: 0.0703\n",
      "Epoch 199/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0087 - acc: 0.1750 - mae: 0.0638 - val_loss: 0.0111 - val_acc: 0.1500 - val_mae: 0.0702\n",
      "Epoch 200/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0087 - acc: 0.1750 - mae: 0.0637 - val_loss: 0.0111 - val_acc: 0.1500 - val_mae: 0.0702\n",
      "Epoch 201/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0087 - acc: 0.1750 - mae: 0.0637 - val_loss: 0.0111 - val_acc: 0.1500 - val_mae: 0.0702\n",
      "Epoch 202/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0087 - acc: 0.1750 - mae: 0.0637 - val_loss: 0.0111 - val_acc: 0.1500 - val_mae: 0.0702\n",
      "Epoch 203/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0087 - acc: 0.1750 - mae: 0.0636 - val_loss: 0.0111 - val_acc: 0.1500 - val_mae: 0.0701\n",
      "Epoch 204/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0086 - acc: 0.1750 - mae: 0.0636 - val_loss: 0.0111 - val_acc: 0.1500 - val_mae: 0.0701\n",
      "Epoch 205/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0086 - acc: 0.1750 - mae: 0.0636 - val_loss: 0.0111 - val_acc: 0.1500 - val_mae: 0.0701\n",
      "Epoch 206/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0086 - acc: 0.1750 - mae: 0.0636 - val_loss: 0.0111 - val_acc: 0.1500 - val_mae: 0.0701\n",
      "Epoch 207/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0086 - acc: 0.1750 - mae: 0.0635 - val_loss: 0.0111 - val_acc: 0.1500 - val_mae: 0.0700\n",
      "Epoch 208/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0086 - acc: 0.1750 - mae: 0.0635 - val_loss: 0.0111 - val_acc: 0.1500 - val_mae: 0.0700\n",
      "Epoch 209/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0086 - acc: 0.1750 - mae: 0.0635 - val_loss: 0.0111 - val_acc: 0.1500 - val_mae: 0.0700\n",
      "Epoch 210/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0086 - acc: 0.1750 - mae: 0.0635 - val_loss: 0.0110 - val_acc: 0.1500 - val_mae: 0.0700\n",
      "Epoch 211/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0086 - acc: 0.1750 - mae: 0.0634 - val_loss: 0.0110 - val_acc: 0.1500 - val_mae: 0.0700\n",
      "Epoch 212/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0086 - acc: 0.1750 - mae: 0.0634 - val_loss: 0.0110 - val_acc: 0.1500 - val_mae: 0.0699\n",
      "Epoch 213/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0086 - acc: 0.1750 - mae: 0.0634 - val_loss: 0.0110 - val_acc: 0.1500 - val_mae: 0.0699\n",
      "Epoch 214/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0086 - acc: 0.1750 - mae: 0.0634 - val_loss: 0.0110 - val_acc: 0.1500 - val_mae: 0.0699\n",
      "Epoch 215/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0086 - acc: 0.1750 - mae: 0.0633 - val_loss: 0.0110 - val_acc: 0.1500 - val_mae: 0.0699\n",
      "Epoch 216/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0086 - acc: 0.1750 - mae: 0.0633 - val_loss: 0.0110 - val_acc: 0.1500 - val_mae: 0.0699\n",
      "Epoch 217/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0086 - acc: 0.1750 - mae: 0.0633 - val_loss: 0.0110 - val_acc: 0.1500 - val_mae: 0.0699\n",
      "Epoch 218/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0086 - acc: 0.1750 - mae: 0.0633 - val_loss: 0.0110 - val_acc: 0.1500 - val_mae: 0.0698\n",
      "Epoch 219/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0086 - acc: 0.1750 - mae: 0.0632 - val_loss: 0.0110 - val_acc: 0.1500 - val_mae: 0.0698\n",
      "Epoch 220/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0085 - acc: 0.1750 - mae: 0.0632 - val_loss: 0.0110 - val_acc: 0.1500 - val_mae: 0.0698\n",
      "Epoch 221/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0085 - acc: 0.1750 - mae: 0.0632 - val_loss: 0.0110 - val_acc: 0.1500 - val_mae: 0.0698\n",
      "Epoch 222/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0085 - acc: 0.1750 - mae: 0.0632 - val_loss: 0.0110 - val_acc: 0.1500 - val_mae: 0.0698\n",
      "Epoch 223/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0085 - acc: 0.1750 - mae: 0.0632 - val_loss: 0.0110 - val_acc: 0.1500 - val_mae: 0.0697\n",
      "Epoch 224/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0085 - acc: 0.1750 - mae: 0.0631 - val_loss: 0.0110 - val_acc: 0.1500 - val_mae: 0.0697\n",
      "Epoch 225/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0085 - acc: 0.1750 - mae: 0.0631 - val_loss: 0.0110 - val_acc: 0.1500 - val_mae: 0.0697\n",
      "Epoch 226/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0085 - acc: 0.1750 - mae: 0.0631 - val_loss: 0.0110 - val_acc: 0.1500 - val_mae: 0.0697\n",
      "Epoch 227/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0085 - acc: 0.1750 - mae: 0.0631 - val_loss: 0.0110 - val_acc: 0.1500 - val_mae: 0.0697\n",
      "Epoch 228/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0085 - acc: 0.1750 - mae: 0.0631 - val_loss: 0.0110 - val_acc: 0.1500 - val_mae: 0.0697\n",
      "Epoch 229/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0085 - acc: 0.1750 - mae: 0.0630 - val_loss: 0.0110 - val_acc: 0.1500 - val_mae: 0.0696\n",
      "Epoch 230/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0085 - acc: 0.1750 - mae: 0.0630 - val_loss: 0.0110 - val_acc: 0.1500 - val_mae: 0.0696\n",
      "Epoch 231/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0085 - acc: 0.1750 - mae: 0.0630 - val_loss: 0.0109 - val_acc: 0.1500 - val_mae: 0.0696\n",
      "Epoch 232/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0085 - acc: 0.1750 - mae: 0.0630 - val_loss: 0.0109 - val_acc: 0.1500 - val_mae: 0.0696\n",
      "Epoch 233/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0085 - acc: 0.1750 - mae: 0.0630 - val_loss: 0.0109 - val_acc: 0.1500 - val_mae: 0.0696\n",
      "Epoch 234/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0085 - acc: 0.1750 - mae: 0.0629 - val_loss: 0.0109 - val_acc: 0.1500 - val_mae: 0.0696\n",
      "Epoch 235/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0085 - acc: 0.1750 - mae: 0.0629 - val_loss: 0.0109 - val_acc: 0.1500 - val_mae: 0.0696\n",
      "Epoch 236/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0085 - acc: 0.1781 - mae: 0.0629 - val_loss: 0.0109 - val_acc: 0.1500 - val_mae: 0.0695\n",
      "Epoch 237/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0085 - acc: 0.1781 - mae: 0.0629 - val_loss: 0.0109 - val_acc: 0.1500 - val_mae: 0.0695\n",
      "Epoch 238/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0085 - acc: 0.1781 - mae: 0.0629 - val_loss: 0.0109 - val_acc: 0.1500 - val_mae: 0.0695\n",
      "Epoch 239/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0085 - acc: 0.1781 - mae: 0.0628 - val_loss: 0.0109 - val_acc: 0.1500 - val_mae: 0.0695\n",
      "Epoch 240/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0084 - acc: 0.1781 - mae: 0.0628 - val_loss: 0.0109 - val_acc: 0.1500 - val_mae: 0.0695\n",
      "Epoch 241/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0084 - acc: 0.1781 - mae: 0.0628 - val_loss: 0.0109 - val_acc: 0.1500 - val_mae: 0.0695\n",
      "Epoch 242/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0084 - acc: 0.1781 - mae: 0.0628 - val_loss: 0.0109 - val_acc: 0.1500 - val_mae: 0.0694\n",
      "Epoch 243/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0084 - acc: 0.1781 - mae: 0.0628 - val_loss: 0.0109 - val_acc: 0.1500 - val_mae: 0.0694\n",
      "Epoch 244/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0084 - acc: 0.1781 - mae: 0.0627 - val_loss: 0.0109 - val_acc: 0.1500 - val_mae: 0.0694\n",
      "Epoch 245/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0084 - acc: 0.1781 - mae: 0.0627 - val_loss: 0.0109 - val_acc: 0.1500 - val_mae: 0.0694\n",
      "Epoch 246/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0084 - acc: 0.1781 - mae: 0.0627 - val_loss: 0.0109 - val_acc: 0.1500 - val_mae: 0.0694\n",
      "Epoch 247/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0084 - acc: 0.1781 - mae: 0.0627 - val_loss: 0.0109 - val_acc: 0.1500 - val_mae: 0.0694\n",
      "Epoch 248/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0084 - acc: 0.1781 - mae: 0.0627 - val_loss: 0.0109 - val_acc: 0.1500 - val_mae: 0.0694\n",
      "Epoch 249/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0084 - acc: 0.1781 - mae: 0.0626 - val_loss: 0.0109 - val_acc: 0.1500 - val_mae: 0.0693\n",
      "Epoch 250/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0084 - acc: 0.1781 - mae: 0.0626 - val_loss: 0.0109 - val_acc: 0.1500 - val_mae: 0.0693\n",
      "Epoch 251/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0084 - acc: 0.1781 - mae: 0.0626 - val_loss: 0.0109 - val_acc: 0.1500 - val_mae: 0.0693\n",
      "Epoch 252/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0084 - acc: 0.1781 - mae: 0.0626 - val_loss: 0.0109 - val_acc: 0.1500 - val_mae: 0.0693\n",
      "Epoch 253/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0084 - acc: 0.1781 - mae: 0.0626 - val_loss: 0.0109 - val_acc: 0.1500 - val_mae: 0.0693\n",
      "Epoch 254/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0084 - acc: 0.1781 - mae: 0.0626 - val_loss: 0.0109 - val_acc: 0.1500 - val_mae: 0.0693\n",
      "Epoch 255/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0084 - acc: 0.1781 - mae: 0.0625 - val_loss: 0.0108 - val_acc: 0.1500 - val_mae: 0.0692\n",
      "Epoch 256/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0084 - acc: 0.1781 - mae: 0.0625 - val_loss: 0.0108 - val_acc: 0.1500 - val_mae: 0.0692\n",
      "Epoch 257/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0084 - acc: 0.1781 - mae: 0.0625 - val_loss: 0.0108 - val_acc: 0.1500 - val_mae: 0.0692\n",
      "Epoch 258/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0084 - acc: 0.1781 - mae: 0.0625 - val_loss: 0.0108 - val_acc: 0.1500 - val_mae: 0.0692\n",
      "Epoch 259/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0084 - acc: 0.1781 - mae: 0.0625 - val_loss: 0.0108 - val_acc: 0.1500 - val_mae: 0.0692\n",
      "Epoch 260/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0084 - acc: 0.1781 - mae: 0.0625 - val_loss: 0.0108 - val_acc: 0.1500 - val_mae: 0.0692\n",
      "Epoch 261/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0084 - acc: 0.1781 - mae: 0.0624 - val_loss: 0.0108 - val_acc: 0.1500 - val_mae: 0.0692\n",
      "Epoch 262/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0083 - acc: 0.1781 - mae: 0.0624 - val_loss: 0.0108 - val_acc: 0.1500 - val_mae: 0.0691\n",
      "Epoch 263/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0083 - acc: 0.1781 - mae: 0.0624 - val_loss: 0.0108 - val_acc: 0.1500 - val_mae: 0.0691\n",
      "Epoch 264/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0083 - acc: 0.1781 - mae: 0.0624 - val_loss: 0.0108 - val_acc: 0.1500 - val_mae: 0.0691\n",
      "Epoch 265/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0083 - acc: 0.1781 - mae: 0.0624 - val_loss: 0.0108 - val_acc: 0.1500 - val_mae: 0.0691\n",
      "Epoch 266/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0083 - acc: 0.1781 - mae: 0.0624 - val_loss: 0.0108 - val_acc: 0.1500 - val_mae: 0.0691\n",
      "Epoch 267/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0083 - acc: 0.1781 - mae: 0.0623 - val_loss: 0.0108 - val_acc: 0.1500 - val_mae: 0.0691\n",
      "Epoch 268/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0083 - acc: 0.1781 - mae: 0.0623 - val_loss: 0.0108 - val_acc: 0.1500 - val_mae: 0.0691\n",
      "Epoch 269/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0083 - acc: 0.1781 - mae: 0.0623 - val_loss: 0.0108 - val_acc: 0.1500 - val_mae: 0.0690\n",
      "Epoch 270/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0083 - acc: 0.1781 - mae: 0.0623 - val_loss: 0.0108 - val_acc: 0.1500 - val_mae: 0.0690\n",
      "Epoch 271/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0083 - acc: 0.1781 - mae: 0.0623 - val_loss: 0.0108 - val_acc: 0.1500 - val_mae: 0.0690\n",
      "Epoch 272/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0083 - acc: 0.1781 - mae: 0.0623 - val_loss: 0.0108 - val_acc: 0.1500 - val_mae: 0.0690\n",
      "Epoch 273/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0083 - acc: 0.1781 - mae: 0.0622 - val_loss: 0.0108 - val_acc: 0.1500 - val_mae: 0.0690\n",
      "Epoch 274/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0083 - acc: 0.1781 - mae: 0.0622 - val_loss: 0.0108 - val_acc: 0.1500 - val_mae: 0.0690\n",
      "Epoch 275/500\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0083 - acc: 0.1781 - mae: 0.0622 - val_loss: 0.0108 - val_acc: 0.1500 - val_mae: 0.0690\n",
      "Epoch 276/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0083 - acc: 0.1781 - mae: 0.0622 - val_loss: 0.0108 - val_acc: 0.1500 - val_mae: 0.0690\n",
      "Epoch 277/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0083 - acc: 0.1781 - mae: 0.0622 - val_loss: 0.0108 - val_acc: 0.1500 - val_mae: 0.0690\n",
      "Epoch 278/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0083 - acc: 0.1781 - mae: 0.0622 - val_loss: 0.0108 - val_acc: 0.1500 - val_mae: 0.0689\n",
      "Epoch 279/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0083 - acc: 0.1781 - mae: 0.0622 - val_loss: 0.0108 - val_acc: 0.1500 - val_mae: 0.0689\n",
      "Epoch 280/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0083 - acc: 0.1781 - mae: 0.0621 - val_loss: 0.0108 - val_acc: 0.1500 - val_mae: 0.0689\n",
      "Epoch 281/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0083 - acc: 0.1781 - mae: 0.0621 - val_loss: 0.0108 - val_acc: 0.1500 - val_mae: 0.0689\n",
      "Epoch 282/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0083 - acc: 0.1781 - mae: 0.0621 - val_loss: 0.0108 - val_acc: 0.1500 - val_mae: 0.0689\n",
      "Epoch 283/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0083 - acc: 0.1781 - mae: 0.0621 - val_loss: 0.0107 - val_acc: 0.1500 - val_mae: 0.0689\n",
      "Epoch 284/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0083 - acc: 0.1781 - mae: 0.0621 - val_loss: 0.0107 - val_acc: 0.1500 - val_mae: 0.0689\n",
      "Epoch 285/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0083 - acc: 0.1781 - mae: 0.0621 - val_loss: 0.0107 - val_acc: 0.1500 - val_mae: 0.0689\n",
      "Epoch 286/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0083 - acc: 0.1781 - mae: 0.0621 - val_loss: 0.0107 - val_acc: 0.1500 - val_mae: 0.0688\n",
      "Epoch 287/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0083 - acc: 0.1813 - mae: 0.0620 - val_loss: 0.0107 - val_acc: 0.1500 - val_mae: 0.0688\n",
      "Epoch 288/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0083 - acc: 0.1813 - mae: 0.0620 - val_loss: 0.0107 - val_acc: 0.1500 - val_mae: 0.0688\n",
      "Epoch 289/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0082 - acc: 0.1813 - mae: 0.0620 - val_loss: 0.0107 - val_acc: 0.1500 - val_mae: 0.0688\n",
      "Epoch 290/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0082 - acc: 0.1813 - mae: 0.0620 - val_loss: 0.0107 - val_acc: 0.1500 - val_mae: 0.0688\n",
      "Epoch 291/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0082 - acc: 0.1813 - mae: 0.0620 - val_loss: 0.0107 - val_acc: 0.1500 - val_mae: 0.0688\n",
      "Epoch 292/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0082 - acc: 0.1813 - mae: 0.0620 - val_loss: 0.0107 - val_acc: 0.1500 - val_mae: 0.0688\n",
      "Epoch 293/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0082 - acc: 0.1813 - mae: 0.0620 - val_loss: 0.0107 - val_acc: 0.1500 - val_mae: 0.0688\n",
      "Epoch 294/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0082 - acc: 0.1813 - mae: 0.0620 - val_loss: 0.0107 - val_acc: 0.1500 - val_mae: 0.0688\n",
      "Epoch 295/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0082 - acc: 0.1813 - mae: 0.0619 - val_loss: 0.0107 - val_acc: 0.1500 - val_mae: 0.0688\n",
      "Epoch 296/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0082 - acc: 0.1813 - mae: 0.0619 - val_loss: 0.0107 - val_acc: 0.1500 - val_mae: 0.0687\n",
      "Epoch 297/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0082 - acc: 0.1813 - mae: 0.0619 - val_loss: 0.0107 - val_acc: 0.1500 - val_mae: 0.0687\n",
      "Epoch 298/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0082 - acc: 0.1813 - mae: 0.0619 - val_loss: 0.0107 - val_acc: 0.1500 - val_mae: 0.0687\n",
      "Epoch 299/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0082 - acc: 0.1813 - mae: 0.0619 - val_loss: 0.0107 - val_acc: 0.1500 - val_mae: 0.0687\n",
      "Epoch 300/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0082 - acc: 0.1813 - mae: 0.0619 - val_loss: 0.0107 - val_acc: 0.1500 - val_mae: 0.0687\n",
      "Epoch 301/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0082 - acc: 0.1813 - mae: 0.0619 - val_loss: 0.0107 - val_acc: 0.1500 - val_mae: 0.0687\n",
      "Epoch 302/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0082 - acc: 0.1813 - mae: 0.0618 - val_loss: 0.0107 - val_acc: 0.1500 - val_mae: 0.0687\n",
      "Epoch 303/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0082 - acc: 0.1813 - mae: 0.0618 - val_loss: 0.0107 - val_acc: 0.1500 - val_mae: 0.0687\n",
      "Epoch 304/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0082 - acc: 0.1813 - mae: 0.0618 - val_loss: 0.0107 - val_acc: 0.1500 - val_mae: 0.0687\n",
      "Epoch 305/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0082 - acc: 0.1813 - mae: 0.0618 - val_loss: 0.0107 - val_acc: 0.1500 - val_mae: 0.0687\n",
      "Epoch 306/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0082 - acc: 0.1813 - mae: 0.0618 - val_loss: 0.0107 - val_acc: 0.1500 - val_mae: 0.0686\n",
      "Epoch 307/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0082 - acc: 0.1813 - mae: 0.0618 - val_loss: 0.0107 - val_acc: 0.1500 - val_mae: 0.0686\n",
      "Epoch 308/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0082 - acc: 0.1813 - mae: 0.0618 - val_loss: 0.0107 - val_acc: 0.1500 - val_mae: 0.0686\n",
      "Epoch 309/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0082 - acc: 0.1813 - mae: 0.0618 - val_loss: 0.0107 - val_acc: 0.1500 - val_mae: 0.0686\n",
      "Epoch 310/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0082 - acc: 0.1813 - mae: 0.0617 - val_loss: 0.0107 - val_acc: 0.1500 - val_mae: 0.0686\n",
      "Epoch 311/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0082 - acc: 0.1813 - mae: 0.0617 - val_loss: 0.0107 - val_acc: 0.1500 - val_mae: 0.0686\n",
      "Epoch 312/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0082 - acc: 0.1813 - mae: 0.0617 - val_loss: 0.0107 - val_acc: 0.1500 - val_mae: 0.0686\n",
      "Epoch 313/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0082 - acc: 0.1813 - mae: 0.0617 - val_loss: 0.0107 - val_acc: 0.1500 - val_mae: 0.0686\n",
      "Epoch 314/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0082 - acc: 0.1813 - mae: 0.0617 - val_loss: 0.0107 - val_acc: 0.1500 - val_mae: 0.0686\n",
      "Epoch 315/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0082 - acc: 0.1813 - mae: 0.0617 - val_loss: 0.0106 - val_acc: 0.1500 - val_mae: 0.0686\n",
      "Epoch 316/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0082 - acc: 0.1813 - mae: 0.0617 - val_loss: 0.0106 - val_acc: 0.1500 - val_mae: 0.0685\n",
      "Epoch 317/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0082 - acc: 0.1813 - mae: 0.0617 - val_loss: 0.0106 - val_acc: 0.1500 - val_mae: 0.0685\n",
      "Epoch 318/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0082 - acc: 0.1813 - mae: 0.0616 - val_loss: 0.0106 - val_acc: 0.1500 - val_mae: 0.0685\n",
      "Epoch 319/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0082 - acc: 0.1813 - mae: 0.0616 - val_loss: 0.0106 - val_acc: 0.1500 - val_mae: 0.0685\n",
      "Epoch 320/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0081 - acc: 0.1813 - mae: 0.0616 - val_loss: 0.0106 - val_acc: 0.1500 - val_mae: 0.0685\n",
      "Epoch 321/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0081 - acc: 0.1813 - mae: 0.0616 - val_loss: 0.0106 - val_acc: 0.1500 - val_mae: 0.0685\n",
      "Epoch 322/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0081 - acc: 0.1813 - mae: 0.0616 - val_loss: 0.0106 - val_acc: 0.1500 - val_mae: 0.0685\n",
      "Epoch 323/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0081 - acc: 0.1813 - mae: 0.0616 - val_loss: 0.0106 - val_acc: 0.1500 - val_mae: 0.0685\n",
      "Epoch 324/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0081 - acc: 0.1813 - mae: 0.0616 - val_loss: 0.0106 - val_acc: 0.1500 - val_mae: 0.0685\n",
      "Epoch 325/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0081 - acc: 0.1813 - mae: 0.0616 - val_loss: 0.0106 - val_acc: 0.1500 - val_mae: 0.0685\n",
      "Epoch 326/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0081 - acc: 0.1813 - mae: 0.0615 - val_loss: 0.0106 - val_acc: 0.1500 - val_mae: 0.0684\n",
      "Epoch 327/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0081 - acc: 0.1813 - mae: 0.0615 - val_loss: 0.0106 - val_acc: 0.1500 - val_mae: 0.0684\n",
      "Epoch 328/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0081 - acc: 0.1813 - mae: 0.0615 - val_loss: 0.0106 - val_acc: 0.1500 - val_mae: 0.0684\n",
      "Epoch 329/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0081 - acc: 0.1813 - mae: 0.0615 - val_loss: 0.0106 - val_acc: 0.1500 - val_mae: 0.0684\n",
      "Epoch 330/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0081 - acc: 0.1813 - mae: 0.0615 - val_loss: 0.0106 - val_acc: 0.1500 - val_mae: 0.0684\n",
      "Epoch 331/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0081 - acc: 0.1813 - mae: 0.0615 - val_loss: 0.0106 - val_acc: 0.1500 - val_mae: 0.0684\n",
      "Epoch 332/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0081 - acc: 0.1813 - mae: 0.0615 - val_loss: 0.0106 - val_acc: 0.1500 - val_mae: 0.0684\n",
      "Epoch 333/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0081 - acc: 0.1813 - mae: 0.0614 - val_loss: 0.0106 - val_acc: 0.1500 - val_mae: 0.0684\n",
      "Epoch 334/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0081 - acc: 0.1813 - mae: 0.0614 - val_loss: 0.0106 - val_acc: 0.1500 - val_mae: 0.0684\n",
      "Epoch 335/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0081 - acc: 0.1813 - mae: 0.0614 - val_loss: 0.0106 - val_acc: 0.1500 - val_mae: 0.0684\n",
      "Epoch 336/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0081 - acc: 0.1813 - mae: 0.0614 - val_loss: 0.0106 - val_acc: 0.1500 - val_mae: 0.0683\n",
      "Epoch 337/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0081 - acc: 0.1813 - mae: 0.0614 - val_loss: 0.0106 - val_acc: 0.1500 - val_mae: 0.0683\n",
      "Epoch 338/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0081 - acc: 0.1813 - mae: 0.0614 - val_loss: 0.0106 - val_acc: 0.1500 - val_mae: 0.0683\n",
      "Epoch 339/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0081 - acc: 0.1813 - mae: 0.0614 - val_loss: 0.0106 - val_acc: 0.1500 - val_mae: 0.0683\n",
      "Epoch 340/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0081 - acc: 0.1813 - mae: 0.0614 - val_loss: 0.0106 - val_acc: 0.1500 - val_mae: 0.0683\n",
      "Epoch 341/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0081 - acc: 0.1813 - mae: 0.0613 - val_loss: 0.0106 - val_acc: 0.1500 - val_mae: 0.0683\n",
      "Epoch 342/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0081 - acc: 0.1813 - mae: 0.0613 - val_loss: 0.0106 - val_acc: 0.1500 - val_mae: 0.0683\n",
      "Epoch 343/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0081 - acc: 0.1813 - mae: 0.0613 - val_loss: 0.0106 - val_acc: 0.1500 - val_mae: 0.0683\n",
      "Epoch 344/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0081 - acc: 0.1813 - mae: 0.0613 - val_loss: 0.0106 - val_acc: 0.1500 - val_mae: 0.0683\n",
      "Epoch 345/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0081 - acc: 0.1813 - mae: 0.0613 - val_loss: 0.0106 - val_acc: 0.1500 - val_mae: 0.0683\n",
      "Epoch 346/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0081 - acc: 0.1813 - mae: 0.0613 - val_loss: 0.0105 - val_acc: 0.1500 - val_mae: 0.0682\n",
      "Epoch 347/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0081 - acc: 0.1813 - mae: 0.0613 - val_loss: 0.0105 - val_acc: 0.1500 - val_mae: 0.0682\n",
      "Epoch 348/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0081 - acc: 0.1813 - mae: 0.0613 - val_loss: 0.0105 - val_acc: 0.1500 - val_mae: 0.0682\n",
      "Epoch 349/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0081 - acc: 0.1813 - mae: 0.0612 - val_loss: 0.0105 - val_acc: 0.1500 - val_mae: 0.0682\n",
      "Epoch 350/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0080 - acc: 0.1813 - mae: 0.0612 - val_loss: 0.0105 - val_acc: 0.1500 - val_mae: 0.0682\n",
      "Epoch 351/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0080 - acc: 0.1813 - mae: 0.0612 - val_loss: 0.0105 - val_acc: 0.1500 - val_mae: 0.0682\n",
      "Epoch 352/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0080 - acc: 0.1813 - mae: 0.0612 - val_loss: 0.0105 - val_acc: 0.1500 - val_mae: 0.0682\n",
      "Epoch 353/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0080 - acc: 0.1813 - mae: 0.0612 - val_loss: 0.0105 - val_acc: 0.1500 - val_mae: 0.0682\n",
      "Epoch 354/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0080 - acc: 0.1813 - mae: 0.0612 - val_loss: 0.0105 - val_acc: 0.1500 - val_mae: 0.0682\n",
      "Epoch 355/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0080 - acc: 0.1813 - mae: 0.0612 - val_loss: 0.0105 - val_acc: 0.1500 - val_mae: 0.0682\n",
      "Epoch 356/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0080 - acc: 0.1813 - mae: 0.0611 - val_loss: 0.0105 - val_acc: 0.1500 - val_mae: 0.0681\n",
      "Epoch 357/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0080 - acc: 0.1813 - mae: 0.0611 - val_loss: 0.0105 - val_acc: 0.1500 - val_mae: 0.0681\n",
      "Epoch 358/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0080 - acc: 0.1813 - mae: 0.0611 - val_loss: 0.0105 - val_acc: 0.1500 - val_mae: 0.0681\n",
      "Epoch 359/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0080 - acc: 0.1813 - mae: 0.0611 - val_loss: 0.0105 - val_acc: 0.1500 - val_mae: 0.0681\n",
      "Epoch 360/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0080 - acc: 0.1813 - mae: 0.0611 - val_loss: 0.0105 - val_acc: 0.1500 - val_mae: 0.0681\n",
      "Epoch 361/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0080 - acc: 0.1813 - mae: 0.0611 - val_loss: 0.0105 - val_acc: 0.1500 - val_mae: 0.0681\n",
      "Epoch 362/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0080 - acc: 0.1813 - mae: 0.0611 - val_loss: 0.0105 - val_acc: 0.1500 - val_mae: 0.0681\n",
      "Epoch 363/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0080 - acc: 0.1813 - mae: 0.0611 - val_loss: 0.0105 - val_acc: 0.1500 - val_mae: 0.0681\n",
      "Epoch 364/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0080 - acc: 0.1813 - mae: 0.0610 - val_loss: 0.0105 - val_acc: 0.1500 - val_mae: 0.0681\n",
      "Epoch 365/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0080 - acc: 0.1813 - mae: 0.0610 - val_loss: 0.0105 - val_acc: 0.1500 - val_mae: 0.0681\n",
      "Epoch 366/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0080 - acc: 0.1813 - mae: 0.0610 - val_loss: 0.0105 - val_acc: 0.1500 - val_mae: 0.0680\n",
      "Epoch 367/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0080 - acc: 0.1813 - mae: 0.0610 - val_loss: 0.0105 - val_acc: 0.1500 - val_mae: 0.0680\n",
      "Epoch 368/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0080 - acc: 0.1813 - mae: 0.0610 - val_loss: 0.0105 - val_acc: 0.1500 - val_mae: 0.0680\n",
      "Epoch 369/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0080 - acc: 0.1813 - mae: 0.0610 - val_loss: 0.0105 - val_acc: 0.1500 - val_mae: 0.0680\n",
      "Epoch 370/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0080 - acc: 0.1813 - mae: 0.0610 - val_loss: 0.0105 - val_acc: 0.1500 - val_mae: 0.0680\n",
      "Epoch 371/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0080 - acc: 0.1813 - mae: 0.0609 - val_loss: 0.0105 - val_acc: 0.1500 - val_mae: 0.0680\n",
      "Epoch 372/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0080 - acc: 0.1813 - mae: 0.0609 - val_loss: 0.0105 - val_acc: 0.1500 - val_mae: 0.0680\n",
      "Epoch 373/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0080 - acc: 0.1813 - mae: 0.0609 - val_loss: 0.0105 - val_acc: 0.1500 - val_mae: 0.0680\n",
      "Epoch 374/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0080 - acc: 0.1813 - mae: 0.0609 - val_loss: 0.0105 - val_acc: 0.1500 - val_mae: 0.0680\n",
      "Epoch 375/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0080 - acc: 0.1813 - mae: 0.0609 - val_loss: 0.0105 - val_acc: 0.1500 - val_mae: 0.0680\n",
      "Epoch 376/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0080 - acc: 0.1813 - mae: 0.0609 - val_loss: 0.0105 - val_acc: 0.1500 - val_mae: 0.0679\n",
      "Epoch 377/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0080 - acc: 0.1813 - mae: 0.0609 - val_loss: 0.0104 - val_acc: 0.1500 - val_mae: 0.0679\n",
      "Epoch 378/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0080 - acc: 0.1813 - mae: 0.0608 - val_loss: 0.0104 - val_acc: 0.1500 - val_mae: 0.0679\n",
      "Epoch 379/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0080 - acc: 0.1813 - mae: 0.0608 - val_loss: 0.0104 - val_acc: 0.1500 - val_mae: 0.0679\n",
      "Epoch 380/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0079 - acc: 0.1813 - mae: 0.0608 - val_loss: 0.0104 - val_acc: 0.1500 - val_mae: 0.0679\n",
      "Epoch 381/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0079 - acc: 0.1813 - mae: 0.0608 - val_loss: 0.0104 - val_acc: 0.1500 - val_mae: 0.0679\n",
      "Epoch 382/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0079 - acc: 0.1813 - mae: 0.0608 - val_loss: 0.0104 - val_acc: 0.1500 - val_mae: 0.0679\n",
      "Epoch 383/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0079 - acc: 0.1813 - mae: 0.0608 - val_loss: 0.0104 - val_acc: 0.1500 - val_mae: 0.0679\n",
      "Epoch 384/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0079 - acc: 0.1813 - mae: 0.0608 - val_loss: 0.0104 - val_acc: 0.1500 - val_mae: 0.0679\n",
      "Epoch 385/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0079 - acc: 0.1813 - mae: 0.0608 - val_loss: 0.0104 - val_acc: 0.1500 - val_mae: 0.0678\n",
      "Epoch 386/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0079 - acc: 0.1813 - mae: 0.0607 - val_loss: 0.0104 - val_acc: 0.1500 - val_mae: 0.0678\n",
      "Epoch 387/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0079 - acc: 0.1813 - mae: 0.0607 - val_loss: 0.0104 - val_acc: 0.1500 - val_mae: 0.0678\n",
      "Epoch 388/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0079 - acc: 0.1813 - mae: 0.0607 - val_loss: 0.0104 - val_acc: 0.1500 - val_mae: 0.0678\n",
      "Epoch 389/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0079 - acc: 0.1813 - mae: 0.0607 - val_loss: 0.0104 - val_acc: 0.1500 - val_mae: 0.0678\n",
      "Epoch 390/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0079 - acc: 0.1813 - mae: 0.0607 - val_loss: 0.0104 - val_acc: 0.1500 - val_mae: 0.0678\n",
      "Epoch 391/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0079 - acc: 0.1813 - mae: 0.0607 - val_loss: 0.0104 - val_acc: 0.1500 - val_mae: 0.0678\n",
      "Epoch 392/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0079 - acc: 0.1813 - mae: 0.0607 - val_loss: 0.0104 - val_acc: 0.1500 - val_mae: 0.0678\n",
      "Epoch 393/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0079 - acc: 0.1813 - mae: 0.0606 - val_loss: 0.0104 - val_acc: 0.1500 - val_mae: 0.0678\n",
      "Epoch 394/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0079 - acc: 0.1813 - mae: 0.0606 - val_loss: 0.0104 - val_acc: 0.1500 - val_mae: 0.0678\n",
      "Epoch 395/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0079 - acc: 0.1813 - mae: 0.0606 - val_loss: 0.0104 - val_acc: 0.1500 - val_mae: 0.0677\n",
      "Epoch 396/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0079 - acc: 0.1813 - mae: 0.0606 - val_loss: 0.0104 - val_acc: 0.1500 - val_mae: 0.0677\n",
      "Epoch 397/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0079 - acc: 0.1813 - mae: 0.0606 - val_loss: 0.0104 - val_acc: 0.1500 - val_mae: 0.0677\n",
      "Epoch 398/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0079 - acc: 0.1813 - mae: 0.0606 - val_loss: 0.0104 - val_acc: 0.1500 - val_mae: 0.0677\n",
      "Epoch 399/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0079 - acc: 0.1813 - mae: 0.0606 - val_loss: 0.0104 - val_acc: 0.1500 - val_mae: 0.0677\n",
      "Epoch 400/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0079 - acc: 0.1813 - mae: 0.0605 - val_loss: 0.0104 - val_acc: 0.1500 - val_mae: 0.0677\n",
      "Epoch 401/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0079 - acc: 0.1813 - mae: 0.0605 - val_loss: 0.0104 - val_acc: 0.1500 - val_mae: 0.0677\n",
      "Epoch 402/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0079 - acc: 0.1813 - mae: 0.0605 - val_loss: 0.0104 - val_acc: 0.1500 - val_mae: 0.0677\n",
      "Epoch 403/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0079 - acc: 0.1813 - mae: 0.0605 - val_loss: 0.0104 - val_acc: 0.1500 - val_mae: 0.0677\n",
      "Epoch 404/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0079 - acc: 0.1813 - mae: 0.0605 - val_loss: 0.0104 - val_acc: 0.1500 - val_mae: 0.0676\n",
      "Epoch 405/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0079 - acc: 0.1813 - mae: 0.0605 - val_loss: 0.0104 - val_acc: 0.1500 - val_mae: 0.0676\n",
      "Epoch 406/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0079 - acc: 0.1813 - mae: 0.0605 - val_loss: 0.0103 - val_acc: 0.1500 - val_mae: 0.0676\n",
      "Epoch 407/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0079 - acc: 0.1813 - mae: 0.0604 - val_loss: 0.0103 - val_acc: 0.1500 - val_mae: 0.0676\n",
      "Epoch 408/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0079 - acc: 0.1813 - mae: 0.0604 - val_loss: 0.0103 - val_acc: 0.1500 - val_mae: 0.0676\n",
      "Epoch 409/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0078 - acc: 0.1813 - mae: 0.0604 - val_loss: 0.0103 - val_acc: 0.1500 - val_mae: 0.0676\n",
      "Epoch 410/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0078 - acc: 0.1813 - mae: 0.0604 - val_loss: 0.0103 - val_acc: 0.1500 - val_mae: 0.0676\n",
      "Epoch 411/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0078 - acc: 0.1813 - mae: 0.0604 - val_loss: 0.0103 - val_acc: 0.1500 - val_mae: 0.0676\n",
      "Epoch 412/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0078 - acc: 0.1813 - mae: 0.0604 - val_loss: 0.0103 - val_acc: 0.1500 - val_mae: 0.0676\n",
      "Epoch 413/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0078 - acc: 0.1813 - mae: 0.0604 - val_loss: 0.0103 - val_acc: 0.1500 - val_mae: 0.0676\n",
      "Epoch 414/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0078 - acc: 0.1813 - mae: 0.0604 - val_loss: 0.0103 - val_acc: 0.1500 - val_mae: 0.0675\n",
      "Epoch 415/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0078 - acc: 0.1813 - mae: 0.0603 - val_loss: 0.0103 - val_acc: 0.1500 - val_mae: 0.0675\n",
      "Epoch 416/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0078 - acc: 0.1813 - mae: 0.0603 - val_loss: 0.0103 - val_acc: 0.1500 - val_mae: 0.0675\n",
      "Epoch 417/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0078 - acc: 0.1813 - mae: 0.0603 - val_loss: 0.0103 - val_acc: 0.1500 - val_mae: 0.0675\n",
      "Epoch 418/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0078 - acc: 0.1813 - mae: 0.0603 - val_loss: 0.0103 - val_acc: 0.1500 - val_mae: 0.0675\n",
      "Epoch 419/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0078 - acc: 0.1813 - mae: 0.0603 - val_loss: 0.0103 - val_acc: 0.1500 - val_mae: 0.0675\n",
      "Epoch 420/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0078 - acc: 0.1813 - mae: 0.0603 - val_loss: 0.0103 - val_acc: 0.1500 - val_mae: 0.0675\n",
      "Epoch 421/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0078 - acc: 0.1813 - mae: 0.0603 - val_loss: 0.0103 - val_acc: 0.1500 - val_mae: 0.0675\n",
      "Epoch 422/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0078 - acc: 0.1813 - mae: 0.0602 - val_loss: 0.0103 - val_acc: 0.1500 - val_mae: 0.0675\n",
      "Epoch 423/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0078 - acc: 0.1813 - mae: 0.0602 - val_loss: 0.0103 - val_acc: 0.1500 - val_mae: 0.0674\n",
      "Epoch 424/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0078 - acc: 0.1813 - mae: 0.0602 - val_loss: 0.0103 - val_acc: 0.1500 - val_mae: 0.0674\n",
      "Epoch 425/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0078 - acc: 0.1813 - mae: 0.0602 - val_loss: 0.0103 - val_acc: 0.1500 - val_mae: 0.0674\n",
      "Epoch 426/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0078 - acc: 0.1813 - mae: 0.0602 - val_loss: 0.0103 - val_acc: 0.1500 - val_mae: 0.0674\n",
      "Epoch 427/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0078 - acc: 0.1813 - mae: 0.0602 - val_loss: 0.0103 - val_acc: 0.1500 - val_mae: 0.0674\n",
      "Epoch 428/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0078 - acc: 0.1813 - mae: 0.0602 - val_loss: 0.0103 - val_acc: 0.1500 - val_mae: 0.0674\n",
      "Epoch 429/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0078 - acc: 0.1813 - mae: 0.0601 - val_loss: 0.0103 - val_acc: 0.1500 - val_mae: 0.0674\n",
      "Epoch 430/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0078 - acc: 0.1813 - mae: 0.0601 - val_loss: 0.0103 - val_acc: 0.1500 - val_mae: 0.0674\n",
      "Epoch 431/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0078 - acc: 0.1813 - mae: 0.0601 - val_loss: 0.0103 - val_acc: 0.1500 - val_mae: 0.0674\n",
      "Epoch 432/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0078 - acc: 0.1813 - mae: 0.0601 - val_loss: 0.0103 - val_acc: 0.1500 - val_mae: 0.0673\n",
      "Epoch 433/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0078 - acc: 0.1813 - mae: 0.0601 - val_loss: 0.0103 - val_acc: 0.1500 - val_mae: 0.0673\n",
      "Epoch 434/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0078 - acc: 0.1813 - mae: 0.0601 - val_loss: 0.0102 - val_acc: 0.1500 - val_mae: 0.0673\n",
      "Epoch 435/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0078 - acc: 0.1813 - mae: 0.0601 - val_loss: 0.0102 - val_acc: 0.1500 - val_mae: 0.0673\n",
      "Epoch 436/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0078 - acc: 0.1813 - mae: 0.0601 - val_loss: 0.0102 - val_acc: 0.1500 - val_mae: 0.0673\n",
      "Epoch 437/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0078 - acc: 0.1813 - mae: 0.0600 - val_loss: 0.0102 - val_acc: 0.1500 - val_mae: 0.0673\n",
      "Epoch 438/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0078 - acc: 0.1813 - mae: 0.0600 - val_loss: 0.0102 - val_acc: 0.1500 - val_mae: 0.0673\n",
      "Epoch 439/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0077 - acc: 0.1813 - mae: 0.0600 - val_loss: 0.0102 - val_acc: 0.1500 - val_mae: 0.0673\n",
      "Epoch 440/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0077 - acc: 0.1813 - mae: 0.0600 - val_loss: 0.0102 - val_acc: 0.1500 - val_mae: 0.0673\n",
      "Epoch 441/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0077 - acc: 0.1813 - mae: 0.0600 - val_loss: 0.0102 - val_acc: 0.1500 - val_mae: 0.0673\n",
      "Epoch 442/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0077 - acc: 0.1813 - mae: 0.0600 - val_loss: 0.0102 - val_acc: 0.1500 - val_mae: 0.0672\n",
      "Epoch 443/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0077 - acc: 0.1813 - mae: 0.0600 - val_loss: 0.0102 - val_acc: 0.1500 - val_mae: 0.0672\n",
      "Epoch 444/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0077 - acc: 0.1813 - mae: 0.0599 - val_loss: 0.0102 - val_acc: 0.1500 - val_mae: 0.0672\n",
      "Epoch 445/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0077 - acc: 0.1813 - mae: 0.0599 - val_loss: 0.0102 - val_acc: 0.1500 - val_mae: 0.0672\n",
      "Epoch 446/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0077 - acc: 0.1813 - mae: 0.0599 - val_loss: 0.0102 - val_acc: 0.1500 - val_mae: 0.0672\n",
      "Epoch 447/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0077 - acc: 0.1813 - mae: 0.0599 - val_loss: 0.0102 - val_acc: 0.1500 - val_mae: 0.0672\n",
      "Epoch 448/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0077 - acc: 0.1813 - mae: 0.0599 - val_loss: 0.0102 - val_acc: 0.1500 - val_mae: 0.0672\n",
      "Epoch 449/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0077 - acc: 0.1813 - mae: 0.0599 - val_loss: 0.0102 - val_acc: 0.1500 - val_mae: 0.0672\n",
      "Epoch 450/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0077 - acc: 0.1813 - mae: 0.0599 - val_loss: 0.0102 - val_acc: 0.1500 - val_mae: 0.0672\n",
      "Epoch 451/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0077 - acc: 0.1813 - mae: 0.0598 - val_loss: 0.0102 - val_acc: 0.1500 - val_mae: 0.0671\n",
      "Epoch 452/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0077 - acc: 0.1813 - mae: 0.0598 - val_loss: 0.0102 - val_acc: 0.1500 - val_mae: 0.0671\n",
      "Epoch 453/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0077 - acc: 0.1813 - mae: 0.0598 - val_loss: 0.0102 - val_acc: 0.1500 - val_mae: 0.0671\n",
      "Epoch 454/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0077 - acc: 0.1813 - mae: 0.0598 - val_loss: 0.0102 - val_acc: 0.1500 - val_mae: 0.0671\n",
      "Epoch 455/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0077 - acc: 0.1813 - mae: 0.0598 - val_loss: 0.0102 - val_acc: 0.1500 - val_mae: 0.0671\n",
      "Epoch 456/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0077 - acc: 0.1813 - mae: 0.0598 - val_loss: 0.0102 - val_acc: 0.1500 - val_mae: 0.0671\n",
      "Epoch 457/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0077 - acc: 0.1781 - mae: 0.0598 - val_loss: 0.0102 - val_acc: 0.1500 - val_mae: 0.0671\n",
      "Epoch 458/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0077 - acc: 0.1813 - mae: 0.0598 - val_loss: 0.0102 - val_acc: 0.1500 - val_mae: 0.0672\n",
      "Epoch 459/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0077 - acc: 0.1781 - mae: 0.0599 - val_loss: 0.0102 - val_acc: 0.1500 - val_mae: 0.0672\n",
      "Epoch 460/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0077 - acc: 0.1781 - mae: 0.0599 - val_loss: 0.0102 - val_acc: 0.1500 - val_mae: 0.0673\n",
      "Epoch 461/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0077 - acc: 0.1781 - mae: 0.0599 - val_loss: 0.0102 - val_acc: 0.1500 - val_mae: 0.0671\n",
      "Epoch 00461: early stopping\n",
      "Model: \"model_39\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_33 (InputLayer)        [(None, 644)]             0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 20)                12900     \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 644)               13524     \n",
      "=================================================================\n",
      "Total params: 26,424\n",
      "Trainable params: 26,424\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# calculated log returns (i.e. the log of the difference between the price x+1 and price x)\n",
    "# windows of train.shape[1] consecutive returns will be produced. \n",
    "# normalized with a MinMaxScaler to the range [0,1].\n",
    "\n",
    "epochs = 500\n",
    "\n",
    "class simple_autoencoder():\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def reduced_dim(self):\n",
    "        encoding_dim = 20\n",
    "        window_length = training_set_scaled.shape[1]\n",
    "        input_window = Input(shape=(window_length,))\n",
    "        # encoded representation of the input\n",
    "        encoded = Dense(encoding_dim, activation='tanh')(input_window) #tanh, linear, leakyrelu\n",
    "        # model mapping an input to its encoded representation\n",
    "        encoder = Model(input_window, encoded)\n",
    "        return pd.DataFrame(encoder.predict(test_set_scaled)).head()\n",
    "\n",
    "    def model(self):\n",
    "        encoding_dim = 20\n",
    "        window_length = training_set_scaled.shape[1]\n",
    "        # input placeholder\n",
    "        input_window = Input(shape=(window_length,))\n",
    "        # encoded representation of the input\n",
    "        encoded = Dense(encoding_dim, activation='tanh')(input_window) #tanh, linear, leakyrelu\n",
    "        # lossy reconstruction of the input\n",
    "        decoded = Dense(window_length, activation='linear')(encoded) #linear\n",
    "        # model mapping an input to its reconstruction\n",
    "        simple_autoencoder = Model(input_window, decoded)\n",
    "        simple_autoencoder.summary()\n",
    "        sae = simple_autoencoder.compile(optimizer='adam', loss='mean_squared_error', metrics=['acc','mae']) #MSE\n",
    "        return simple_autoencoder\n",
    "\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-5, patience=5, mode='auto', verbose = 1)\n",
    "checkpointer = ModelCheckpoint(filepath=\"best_weights.hdf5\",save_best_only=True)\n",
    "\n",
    "\n",
    "model = simple_autoencoder()\n",
    "history = model.model().fit(training_set_scaled, training_set_scaled,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=1024,\n",
    "                    shuffle=True,\n",
    "                    validation_split = 0.2,\n",
    "                    callbacks = [monitor, checkpointer])       \n",
    "#                   validation_data=(test_set_scaled, test_set_scaled))\n",
    "\n",
    "decoded_stocks = simple_autoencoder().model().predict(test_set_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.253874</td>\n",
       "      <td>0.108061</td>\n",
       "      <td>-0.701503</td>\n",
       "      <td>-0.730775</td>\n",
       "      <td>0.362164</td>\n",
       "      <td>0.202069</td>\n",
       "      <td>0.217054</td>\n",
       "      <td>0.323640</td>\n",
       "      <td>0.798893</td>\n",
       "      <td>0.086669</td>\n",
       "      <td>-0.029927</td>\n",
       "      <td>0.870718</td>\n",
       "      <td>-0.483008</td>\n",
       "      <td>-0.361751</td>\n",
       "      <td>0.144097</td>\n",
       "      <td>-0.165063</td>\n",
       "      <td>-0.431446</td>\n",
       "      <td>-0.401243</td>\n",
       "      <td>-0.305907</td>\n",
       "      <td>-0.370328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.108600</td>\n",
       "      <td>0.183218</td>\n",
       "      <td>-0.714460</td>\n",
       "      <td>-0.712744</td>\n",
       "      <td>0.619875</td>\n",
       "      <td>0.250855</td>\n",
       "      <td>0.470326</td>\n",
       "      <td>0.410083</td>\n",
       "      <td>0.717201</td>\n",
       "      <td>0.020788</td>\n",
       "      <td>0.134166</td>\n",
       "      <td>0.847102</td>\n",
       "      <td>-0.425439</td>\n",
       "      <td>-0.416016</td>\n",
       "      <td>0.139242</td>\n",
       "      <td>0.128938</td>\n",
       "      <td>-0.581184</td>\n",
       "      <td>-0.308913</td>\n",
       "      <td>-0.526256</td>\n",
       "      <td>-0.565992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.017777</td>\n",
       "      <td>0.266416</td>\n",
       "      <td>-0.630921</td>\n",
       "      <td>-0.649905</td>\n",
       "      <td>0.426936</td>\n",
       "      <td>0.234227</td>\n",
       "      <td>0.527329</td>\n",
       "      <td>0.525547</td>\n",
       "      <td>0.891732</td>\n",
       "      <td>-0.216279</td>\n",
       "      <td>0.117406</td>\n",
       "      <td>0.931062</td>\n",
       "      <td>-0.513410</td>\n",
       "      <td>-0.262765</td>\n",
       "      <td>-0.063280</td>\n",
       "      <td>0.151675</td>\n",
       "      <td>-0.440844</td>\n",
       "      <td>-0.511183</td>\n",
       "      <td>-0.584432</td>\n",
       "      <td>-0.580721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.080044</td>\n",
       "      <td>0.221360</td>\n",
       "      <td>-0.651152</td>\n",
       "      <td>-0.613587</td>\n",
       "      <td>0.418594</td>\n",
       "      <td>-0.023749</td>\n",
       "      <td>0.044255</td>\n",
       "      <td>0.305970</td>\n",
       "      <td>0.552357</td>\n",
       "      <td>0.182876</td>\n",
       "      <td>-0.099578</td>\n",
       "      <td>0.898218</td>\n",
       "      <td>-0.480421</td>\n",
       "      <td>-0.399982</td>\n",
       "      <td>0.220741</td>\n",
       "      <td>-0.018310</td>\n",
       "      <td>-0.378346</td>\n",
       "      <td>-0.393864</td>\n",
       "      <td>-0.229432</td>\n",
       "      <td>-0.586607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.098448</td>\n",
       "      <td>0.187826</td>\n",
       "      <td>-0.655765</td>\n",
       "      <td>-0.721442</td>\n",
       "      <td>0.126462</td>\n",
       "      <td>0.178148</td>\n",
       "      <td>0.099366</td>\n",
       "      <td>0.234990</td>\n",
       "      <td>0.629338</td>\n",
       "      <td>-0.261646</td>\n",
       "      <td>0.309820</td>\n",
       "      <td>0.846419</td>\n",
       "      <td>-0.277255</td>\n",
       "      <td>-0.290999</td>\n",
       "      <td>-0.180330</td>\n",
       "      <td>0.347799</td>\n",
       "      <td>-0.470017</td>\n",
       "      <td>-0.195855</td>\n",
       "      <td>-0.524071</td>\n",
       "      <td>-0.578214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.253874  0.108061 -0.701503 -0.730775  0.362164  0.202069  0.217054   \n",
       "1 -0.108600  0.183218 -0.714460 -0.712744  0.619875  0.250855  0.470326   \n",
       "2  0.017777  0.266416 -0.630921 -0.649905  0.426936  0.234227  0.527329   \n",
       "3  0.080044  0.221360 -0.651152 -0.613587  0.418594 -0.023749  0.044255   \n",
       "4  0.098448  0.187826 -0.655765 -0.721442  0.126462  0.178148  0.099366   \n",
       "\n",
       "          7         8         9        10        11        12        13  \\\n",
       "0  0.323640  0.798893  0.086669 -0.029927  0.870718 -0.483008 -0.361751   \n",
       "1  0.410083  0.717201  0.020788  0.134166  0.847102 -0.425439 -0.416016   \n",
       "2  0.525547  0.891732 -0.216279  0.117406  0.931062 -0.513410 -0.262765   \n",
       "3  0.305970  0.552357  0.182876 -0.099578  0.898218 -0.480421 -0.399982   \n",
       "4  0.234990  0.629338 -0.261646  0.309820  0.846419 -0.277255 -0.290999   \n",
       "\n",
       "         14        15        16        17        18        19  \n",
       "0  0.144097 -0.165063 -0.431446 -0.401243 -0.305907 -0.370328  \n",
       "1  0.139242  0.128938 -0.581184 -0.308913 -0.526256 -0.565992  \n",
       "2 -0.063280  0.151675 -0.440844 -0.511183 -0.584432 -0.580721  \n",
       "3  0.220741 -0.018310 -0.378346 -0.393864 -0.229432 -0.586607  \n",
       "4 -0.180330  0.347799 -0.470017 -0.195855 -0.524071 -0.578214  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_autoencoder().reduced_dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # calculated log returns (i.e. the log of the difference between the price x+1 and price x)\n",
    "# # windows of train.shape[1] consecutive returns will be produced. \n",
    "# # Can be normalized with a MinMaxScaler to the range [0,1]??\n",
    "\n",
    "# window_length = training_set_scaled.shape[1]\n",
    "# encoding_dim = 20\n",
    "# epochs = 500\n",
    "\n",
    "# # compress the input to a 3-dimensional latent space. \n",
    "\n",
    "# # input placeholder\n",
    "# input_window = Input(shape=(window_length,))\n",
    "# # encoded representation of the input\n",
    "# encoded = Dense(encoding_dim, activation='tanh')(input_window) #tanh, linear, leakyrelu\n",
    "# # lossy reconstruction of the input\n",
    "# decoded = Dense(window_length, activation='linear')(encoded) #linear\n",
    "\n",
    "# # model mapping an input to its reconstruction\n",
    "# simple_autoencoder = Model(input_window, decoded)\n",
    "\n",
    "# # model mapping an input to its encoded representation\n",
    "# encoder = Model(input_window, encoded)\n",
    "\n",
    "# monitor = EarlyStopping(monitor='val_loss', min_delta=1e-5, patience=5, mode='auto', verbose = 1)\n",
    "# checkpointer = ModelCheckpoint(filepath=\"best_weights.hdf5\",save_best_only=True)\n",
    "\n",
    "# simple_autoencoder.summary()\n",
    "# sae = simple_autoencoder.compile(optimizer='adam', loss='mean_squared_error', metrics=['acc','mae']) #MSE\n",
    "# sae\n",
    "# history = simple_autoencoder.fit(training_set_scaled, training_set_scaled,\n",
    "#                 epochs=epochs,\n",
    "#                 batch_size=1024,\n",
    "#                 shuffle=True,\n",
    "#                 validation_split = 0.2,\n",
    "#                 callbacks = [monitor, checkpointer])       \n",
    "# #               validation_data=(test_set_scaled, test_set_scaled))\n",
    "\n",
    "# decoded_stocks = simple_autoencoder.predict(test_set_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(encoder.predict(test_set_scaled)).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(decoded_stocks).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    ax = plt.subplot(1, 4, 1)\n",
    "    plt.plot(history.history[\"loss\"])\n",
    "    plt.title(\"Train loss\")\n",
    "    ax = plt.subplot(1, 4, 2)\n",
    "    plt.plot(history.history[\"val_loss\"])\n",
    "    plt.title(\"Test loss\")\n",
    "    ax = plt.subplot(1, 4, 3)\n",
    "    plt.plot(history.history[\"val_acc\"])\n",
    "    plt.title(\"Accuracy\")\n",
    "    ax = plt.subplot(1, 4, 4)\n",
    "    plt.plot(history.history[\"val_mae\"])\n",
    "    plt.title(\"Mean Absolute Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loss vs Epoch\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 1D convolutional autoencoder\n",
    "(Kernel size = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main “event” very well represented while the overall reconstruction is very smooth \n",
    "\n",
    "input_window = Input(shape=(window_length,1))\n",
    "x = Conv1D(16, 3, activation=\"tanh\", padding=\"same\")(input_window) # 10 dims\n",
    "#x = BatchNormalization()(x)\n",
    "x = MaxPooling1D(2, padding=\"same\")(x) # 5 dims\n",
    "x = Conv1D(1, 3, activation=\"tanh\", padding=\"same\")(x) # 5 dims\n",
    "#x = BatchNormalization()(x)\n",
    "encoded = MaxPooling1D(2, padding=\"same\")(x) # 3 dims\n",
    "encoder = Model(input_window, encoded)\n",
    "\n",
    "# 3 dimensions in the encoded layer\n",
    "\n",
    "x = Conv1D(1, 3, activation=\"tanh\", padding=\"same\")(encoded) # 3 dims\n",
    "#x = BatchNormalization()(x)\n",
    "x = UpSampling1D(2)(x) # 6 dims\n",
    "x = Conv1D(16, 2, activation='tanh')(x) # 5 dims\n",
    "#x = BatchNormalization()(x)\n",
    "x = UpSampling1D(2)(x) # 10 dims\n",
    "decoded = Conv1D(1, 3, activation='linear', padding='same')(x) # 10 dims\n",
    "conv_autoencoder = Model(input_window, decoded)\n",
    "conv_autoencoder.summary()\n",
    "\n",
    "conv_autoencoder.compile(optimizer='adam', loss='mean_squared_error',metrics=['acc','mae'])\n",
    "history = conv_autoencoder.fit(training_set_scaled, training_set_scaled,\n",
    "                epochs=epochs,\n",
    "                batch_size=1024,\n",
    "                shuffle=True,\n",
    "                validation_split = 0.2)\n",
    "\n",
    "decoded_stocks = conv_autoencoder.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(encoder.predict(test_set_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multiple Layers w/ L1 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def model(optimizer = \"Adam\", score = \"acc\"):\n",
    "    #Input Layer\n",
    "    input_layer = Input(shape =(df.shape[1], ))\n",
    "  \n",
    "    #Encode\n",
    "    encoded = Dense(100, activation ='tanh',\n",
    "                    activity_regularizer = regularizers.l1(10e-5))(input_layer)\n",
    "    encoded = Dense(50, activation ='tanh',\n",
    "                    activity_regularizer = regularizers.l1(10e-5))(encoded)\n",
    "    encoded = Dense(25, activation ='tanh',\n",
    "                    activity_regularizer = regularizers.l1(10e-5))(encoded)\n",
    "    encoded = Dense(12, activation ='tanh',\n",
    "                    activity_regularizer = regularizers.l1(10e-5))(encoded)\n",
    "    encoded = Dense(6, activation ='relu')(encoded)\n",
    "\n",
    "    #Decoder\n",
    "    decoded = Dense(12, activation ='linear')(encoded)\n",
    "    decoded = Dense(25, activation ='linear')(decoded)\n",
    "    decoded = Dense(50, activation ='linear')(decoded)\n",
    "    decoded = Dense(100, activation ='linear')(decoded)\n",
    "\n",
    "    #Output\n",
    "    output_layer = Dense(df.shape[1], activation ='linear')(decoded)\n",
    "\n",
    "    #Parameters\n",
    "    autoencoder = Model(input_layer, output_layer)\n",
    "    autoencoder.compile(optimizer =optimizer, loss =tf.keras.losses.MeanSquaredError(), metrics = ['acc','mae'])\n",
    "    \n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Auto-encoder network\n",
    "output = model().fit(training_set_scaled, training_set_scaled,\n",
    "                batch_size = 16, epochs = 100, \n",
    "                shuffle = True, validation_split = 0.2)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "optimizer is not a legal parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-8c2128de517e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m## Error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mgrid_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcheckpointer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# X, y?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mgrid_best_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    706\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[1;32m    709\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1027\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1030\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    845\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    253\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    253\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mcloned_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m         \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcloned_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mset_params\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \"\"\"\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msk_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mcheck_params\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    101\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparams_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'nb_epoch'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{} is not a legal parameter'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=unused-argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: optimizer is not a legal parameter"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model_regressor = KerasRegressor(simple_autoencoder().model, verbose=1, batch_size=10, epochs=10)\n",
    "#define the grid search parameters\n",
    "#dimensions = []\n",
    "#dropout = []\n",
    "batch_size = [10,20]\n",
    "loss = ['mean_squared_error']\n",
    "optimizer = ['Adam', 'SGD', 'RMSprop']\n",
    "epochs = [10, 15]\n",
    "scoring = ['acc']\n",
    "\n",
    "param_grid = dict(optimizer=optimizer,score = scoring)\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-5, patience=5, mode='auto')\n",
    "checkpointer = ModelCheckpoint(filepath=\"best_weights.hdf6\",save_best_only=True)\n",
    "\n",
    "grid = GridSearchCV(estimator=model_regressor, param_grid=param_grid, \n",
    "                    n_jobs=1, cv = 3)\n",
    "\n",
    "## Error\n",
    "grid_result = grid.fit(df,df, callbacks=[monitor,checkpointer]) # X, y?\n",
    "\n",
    "grid_best_parameters = grid_result.best_params_\n",
    "grid_best_accuracy = grid_result.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "estimator should be an estimator implementing 'fit' method, <bound method simple_autoencoder.model of <__main__.simple_autoencoder object at 0x7fd1cdeb3c10>> was passed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-b4459fde0ef8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrandom_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msimple_autoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_distributions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_set_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrandom_best_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrandom_best_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    652\u001b[0m         \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m         scorers, self.multimetric_ = _check_multimetric_scoring(\n\u001b[0m\u001b[1;32m    655\u001b[0m             self.estimator, scoring=self.scoring)\n\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36m_check_multimetric_scoring\u001b[0;34m(estimator, scoring)\u001b[0m\n\u001b[1;32m    473\u001b[0m     if callable(scoring) or scoring is None or isinstance(scoring,\n\u001b[1;32m    474\u001b[0m                                                           str):\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mscorers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"score\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscorers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36mcheck_scoring\u001b[0;34m(estimator, scoring, allow_none)\u001b[0m\n\u001b[1;32m    400\u001b[0m     \"\"\"\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         raise TypeError(\"estimator should be an estimator implementing \"\n\u001b[0m\u001b[1;32m    403\u001b[0m                         \"'fit' method, %r was passed\" % estimator)\n\u001b[1;32m    404\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: estimator should be an estimator implementing 'fit' method, <bound method simple_autoencoder.model of <__main__.simple_autoencoder object at 0x7fd1cdeb3c10>> was passed"
     ]
    }
   ],
   "source": [
    "random_search = RandomizedSearchCV(estimator=simple_autoencoder().model, param_distributions=param_grid, n_iter=100)\n",
    "random_search.fit(training_set_scaled, training_set_scaled)\n",
    "\n",
    "random_best_parameters = grid_result.best_params_\n",
    "random_best_accuracy = grid_result.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_best_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- Code's working fine now, there are about 3 different autoencoders here, Simple, Convolutional and one with multiple layers and regularization.\n",
    "- Performed hyp tuning on the last autoencoder model with multiple layers\n",
    "- The only thing we need to do is add the parameters in model() and param_grid. At the moment, I've only tried optimizer and score. \n",
    "- Makes those changes here: \n",
    "    - def model(optimizer = \"Adam\", score = \"acc\", epoch = '', batch_size= '',....)\n",
    "    - param_grid = dict(optimizer=optimizer,score = scoring, epoch = ....)\n",
    "- Parameters we need to add: no. of Dimensions of our dataframe, loss function, epochs, batch_size, dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
