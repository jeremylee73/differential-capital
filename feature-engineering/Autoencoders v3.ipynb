{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Conv1D, MaxPooling1D, UpSampling1D, BatchNormalization, LSTM, RepeatVector\n",
    "from keras.models import Model\n",
    "from keras.models import model_from_json\n",
    "from keras import regularizers\n",
    "import datetime\n",
    "import time\n",
    "import requests as req\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## USE: full log(returns)/returns dataframe\n",
    "## Risk Adjusted Returns\n",
    "\n",
    "df = pd.read_pickle(\"../Data/risk_adj_returns.pkl\").iloc[1:]\n",
    "\n",
    "drop_columns = []\n",
    "for col in df.columns:\n",
    "    if df[col].isnull().all() == True:\n",
    "        drop_columns.append(col)\n",
    "        \n",
    "df.drop(columns=drop_columns, inplace=True)\n",
    "\n",
    "# df['pct_change'] = df.close.pct_change()\n",
    "# df['log_ret'] = np.log(df.close) - np.log(df.close.shift(1))\n",
    "# df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.dropna(how='any',axis=0) #All rows have NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>905270</th>\n",
       "      <th>921795</th>\n",
       "      <th>904261</th>\n",
       "      <th>905261</th>\n",
       "      <th>916328</th>\n",
       "      <th>923024</th>\n",
       "      <th>936365</th>\n",
       "      <th>902355</th>\n",
       "      <th>912215</th>\n",
       "      <th>929813</th>\n",
       "      <th>...</th>\n",
       "      <th>9660J1</th>\n",
       "      <th>69568X</th>\n",
       "      <th>543755</th>\n",
       "      <th>77463M</th>\n",
       "      <th>29235J</th>\n",
       "      <th>131745</th>\n",
       "      <th>69487D</th>\n",
       "      <th>68157P</th>\n",
       "      <th>9110RA</th>\n",
       "      <th>292703</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-05-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.073424</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.026594</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042002</td>\n",
       "      <td>0.042352</td>\n",
       "      <td>-0.007893</td>\n",
       "      <td>-0.034946</td>\n",
       "      <td>-0.006981</td>\n",
       "      <td>0.025437</td>\n",
       "      <td>0.006211</td>\n",
       "      <td>-0.034570</td>\n",
       "      <td>0.045914</td>\n",
       "      <td>-0.023507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.041516</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.023492</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045774</td>\n",
       "      <td>-0.040381</td>\n",
       "      <td>-0.029872</td>\n",
       "      <td>-0.042776</td>\n",
       "      <td>-0.034353</td>\n",
       "      <td>-0.047507</td>\n",
       "      <td>-0.041449</td>\n",
       "      <td>-0.032225</td>\n",
       "      <td>-0.001858</td>\n",
       "      <td>-0.073776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.133718</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.022297</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018214</td>\n",
       "      <td>0.006846</td>\n",
       "      <td>-0.041720</td>\n",
       "      <td>0.014670</td>\n",
       "      <td>0.006996</td>\n",
       "      <td>0.050378</td>\n",
       "      <td>-0.030298</td>\n",
       "      <td>0.008515</td>\n",
       "      <td>-0.039008</td>\n",
       "      <td>-0.103963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.114582</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.030586</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050278</td>\n",
       "      <td>-0.001888</td>\n",
       "      <td>-0.039754</td>\n",
       "      <td>-0.029330</td>\n",
       "      <td>-0.021043</td>\n",
       "      <td>-0.042413</td>\n",
       "      <td>-0.000474</td>\n",
       "      <td>-0.003118</td>\n",
       "      <td>-0.012079</td>\n",
       "      <td>0.010919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.046110</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001882</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059837</td>\n",
       "      <td>-0.051037</td>\n",
       "      <td>-0.046412</td>\n",
       "      <td>-0.017292</td>\n",
       "      <td>-0.040496</td>\n",
       "      <td>-0.066158</td>\n",
       "      <td>-0.032654</td>\n",
       "      <td>-0.031242</td>\n",
       "      <td>-0.039250</td>\n",
       "      <td>-0.007592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1237 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            905270  921795  904261  905261    916328  923024    936365  \\\n",
       "date                                                                     \n",
       "2021-05-24     NaN     NaN     NaN     NaN -0.073424     NaN -0.026594   \n",
       "2021-05-25     NaN     NaN     NaN     NaN -0.041516     NaN -0.023492   \n",
       "2021-05-26     NaN     NaN     NaN     NaN -0.133718     NaN -0.022297   \n",
       "2021-05-27     NaN     NaN     NaN     NaN -0.114582     NaN -0.030586   \n",
       "2021-05-28     NaN     NaN     NaN     NaN -0.046110     NaN  0.001882   \n",
       "\n",
       "            902355  912215  929813  ...    9660J1    69568X    543755  \\\n",
       "date                                ...                                 \n",
       "2021-05-24     NaN     NaN     NaN  ... -0.042002  0.042352 -0.007893   \n",
       "2021-05-25     NaN     NaN     NaN  ... -0.045774 -0.040381 -0.029872   \n",
       "2021-05-26     NaN     NaN     NaN  ... -0.018214  0.006846 -0.041720   \n",
       "2021-05-27     NaN     NaN     NaN  ... -0.050278 -0.001888 -0.039754   \n",
       "2021-05-28     NaN     NaN     NaN  ... -0.059837 -0.051037 -0.046412   \n",
       "\n",
       "              77463M    29235J    131745    69487D    68157P    9110RA  \\\n",
       "date                                                                     \n",
       "2021-05-24 -0.034946 -0.006981  0.025437  0.006211 -0.034570  0.045914   \n",
       "2021-05-25 -0.042776 -0.034353 -0.047507 -0.041449 -0.032225 -0.001858   \n",
       "2021-05-26  0.014670  0.006996  0.050378 -0.030298  0.008515 -0.039008   \n",
       "2021-05-27 -0.029330 -0.021043 -0.042413 -0.000474 -0.003118 -0.012079   \n",
       "2021-05-28 -0.017292 -0.040496 -0.066158 -0.032654 -0.031242 -0.039250   \n",
       "\n",
       "              292703  \n",
       "date                  \n",
       "2021-05-24 -0.023507  \n",
       "2021-05-25 -0.073776  \n",
       "2021-05-26 -0.103963  \n",
       "2021-05-27  0.010919  \n",
       "2021-05-28 -0.007592  \n",
       "\n",
       "[5 rows x 1237 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_investable(t, n_rows):\n",
    "    \"Find stocks in investable universe at time t\\\n",
    "    (stocks in the S&P500 that have prices recorded for the last n_rows days)\"\n",
    "    \n",
    "    df_investable = df.copy(deep = True).sort_index(ascending = False)\n",
    "    \n",
    "    #add 1 date to get the test features in investable\n",
    "    t = t + pd.DateOffset(1)\n",
    "    \n",
    "    #if t is now a non-trading day, advance until we reach a valid trading day\n",
    "    while t not in df_investable.index:\n",
    "        t = t + pd.DateOffset(1)\n",
    "    \n",
    "    t_index = df_investable.index.get_loc(t)\n",
    "    \n",
    "    #take n_rows worth of data upto time specified\n",
    "    df_investable = df_investable.iloc[t_index + 1:t_index + n_rows + 1]\n",
    "    \n",
    "    #find all stocks that exist in the S&P at this time period\n",
    "    investable_universe = []\n",
    "    for col in df_investable.columns:\n",
    "        if ~df_investable[col].iloc[:n_rows].isna().any():\n",
    "            investable_universe.append(col)\n",
    "        \n",
    "    df_investable = df_investable[investable_universe]\n",
    "    \n",
    "    return df_investable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>916328</th>\n",
       "      <th>936365</th>\n",
       "      <th>905271</th>\n",
       "      <th>905113</th>\n",
       "      <th>905802</th>\n",
       "      <th>905425</th>\n",
       "      <th>906156</th>\n",
       "      <th>916305</th>\n",
       "      <th>992816</th>\n",
       "      <th>921093</th>\n",
       "      <th>...</th>\n",
       "      <th>311917</th>\n",
       "      <th>69568X</th>\n",
       "      <th>543755</th>\n",
       "      <th>77463M</th>\n",
       "      <th>29235J</th>\n",
       "      <th>131745</th>\n",
       "      <th>69487D</th>\n",
       "      <th>68157P</th>\n",
       "      <th>9110RA</th>\n",
       "      <th>292703</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-05-11</th>\n",
       "      <td>-0.033261</td>\n",
       "      <td>-0.056147</td>\n",
       "      <td>-0.056673</td>\n",
       "      <td>-0.039225</td>\n",
       "      <td>-0.078155</td>\n",
       "      <td>-0.079176</td>\n",
       "      <td>-0.055281</td>\n",
       "      <td>-0.036198</td>\n",
       "      <td>-0.062879</td>\n",
       "      <td>-0.060282</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030098</td>\n",
       "      <td>-0.058198</td>\n",
       "      <td>-0.028429</td>\n",
       "      <td>-0.050278</td>\n",
       "      <td>-0.080133</td>\n",
       "      <td>-0.024472</td>\n",
       "      <td>-0.052140</td>\n",
       "      <td>-0.019944</td>\n",
       "      <td>-7.289672e-02</td>\n",
       "      <td>-0.031952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-10</th>\n",
       "      <td>-0.001857</td>\n",
       "      <td>-0.007957</td>\n",
       "      <td>-0.056608</td>\n",
       "      <td>-0.033577</td>\n",
       "      <td>-0.012766</td>\n",
       "      <td>-0.038382</td>\n",
       "      <td>-0.031704</td>\n",
       "      <td>-0.042638</td>\n",
       "      <td>-0.012963</td>\n",
       "      <td>-0.051148</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056529</td>\n",
       "      <td>-0.045813</td>\n",
       "      <td>-0.015254</td>\n",
       "      <td>0.108949</td>\n",
       "      <td>-0.000408</td>\n",
       "      <td>-0.018463</td>\n",
       "      <td>-0.030641</td>\n",
       "      <td>-0.039673</td>\n",
       "      <td>7.790037e-03</td>\n",
       "      <td>-0.075911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-09</th>\n",
       "      <td>-0.032497</td>\n",
       "      <td>0.014964</td>\n",
       "      <td>-0.041537</td>\n",
       "      <td>-0.019209</td>\n",
       "      <td>0.009759</td>\n",
       "      <td>-0.152829</td>\n",
       "      <td>-0.034908</td>\n",
       "      <td>-0.049782</td>\n",
       "      <td>-0.029491</td>\n",
       "      <td>-0.085189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030915</td>\n",
       "      <td>-0.006775</td>\n",
       "      <td>-0.098720</td>\n",
       "      <td>0.026074</td>\n",
       "      <td>-0.007489</td>\n",
       "      <td>-0.041385</td>\n",
       "      <td>-0.049166</td>\n",
       "      <td>-0.000544</td>\n",
       "      <td>-3.590120e-02</td>\n",
       "      <td>-0.052591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-08</th>\n",
       "      <td>-0.119326</td>\n",
       "      <td>-0.029451</td>\n",
       "      <td>-0.070543</td>\n",
       "      <td>-0.015943</td>\n",
       "      <td>0.022881</td>\n",
       "      <td>-0.186745</td>\n",
       "      <td>-0.023214</td>\n",
       "      <td>-0.047493</td>\n",
       "      <td>-0.034577</td>\n",
       "      <td>-0.046590</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051192</td>\n",
       "      <td>-0.039952</td>\n",
       "      <td>-0.071749</td>\n",
       "      <td>-0.048822</td>\n",
       "      <td>-0.039231</td>\n",
       "      <td>0.017063</td>\n",
       "      <td>-0.038039</td>\n",
       "      <td>-0.024803</td>\n",
       "      <td>-2.449209e-02</td>\n",
       "      <td>-0.070835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-07</th>\n",
       "      <td>-0.039900</td>\n",
       "      <td>0.012059</td>\n",
       "      <td>-0.056630</td>\n",
       "      <td>-0.000645</td>\n",
       "      <td>-0.030319</td>\n",
       "      <td>-0.127271</td>\n",
       "      <td>-0.049799</td>\n",
       "      <td>-0.023339</td>\n",
       "      <td>-0.029050</td>\n",
       "      <td>-0.079179</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046167</td>\n",
       "      <td>0.016789</td>\n",
       "      <td>0.003434</td>\n",
       "      <td>0.055430</td>\n",
       "      <td>-0.044564</td>\n",
       "      <td>-0.025124</td>\n",
       "      <td>-0.087165</td>\n",
       "      <td>0.004368</td>\n",
       "      <td>-4.533198e-02</td>\n",
       "      <td>-0.074954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-02</th>\n",
       "      <td>-0.072393</td>\n",
       "      <td>-0.072199</td>\n",
       "      <td>-0.091406</td>\n",
       "      <td>-0.072376</td>\n",
       "      <td>-0.001441</td>\n",
       "      <td>-0.069006</td>\n",
       "      <td>-0.054409</td>\n",
       "      <td>-0.027164</td>\n",
       "      <td>0.062017</td>\n",
       "      <td>-0.128361</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.066717</td>\n",
       "      <td>-0.000397</td>\n",
       "      <td>-0.034413</td>\n",
       "      <td>-0.047709</td>\n",
       "      <td>-0.072616</td>\n",
       "      <td>-0.041856</td>\n",
       "      <td>-0.050222</td>\n",
       "      <td>-0.016288</td>\n",
       "      <td>-8.383439e-03</td>\n",
       "      <td>-0.039350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-01</th>\n",
       "      <td>-0.081210</td>\n",
       "      <td>-0.020391</td>\n",
       "      <td>0.099262</td>\n",
       "      <td>-0.018858</td>\n",
       "      <td>-0.136634</td>\n",
       "      <td>-0.083397</td>\n",
       "      <td>-0.056063</td>\n",
       "      <td>-0.039072</td>\n",
       "      <td>-0.025318</td>\n",
       "      <td>0.021731</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054461</td>\n",
       "      <td>-0.056564</td>\n",
       "      <td>-0.034920</td>\n",
       "      <td>0.007488</td>\n",
       "      <td>-0.025096</td>\n",
       "      <td>-0.018787</td>\n",
       "      <td>-0.029751</td>\n",
       "      <td>0.011307</td>\n",
       "      <td>-3.998029e-02</td>\n",
       "      <td>-0.038914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-31</th>\n",
       "      <td>-0.112960</td>\n",
       "      <td>-0.071512</td>\n",
       "      <td>-0.105417</td>\n",
       "      <td>-0.066444</td>\n",
       "      <td>-0.069153</td>\n",
       "      <td>-0.081507</td>\n",
       "      <td>-0.042095</td>\n",
       "      <td>-0.034474</td>\n",
       "      <td>-0.057237</td>\n",
       "      <td>-0.034575</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058288</td>\n",
       "      <td>-0.096375</td>\n",
       "      <td>-0.038522</td>\n",
       "      <td>-0.056268</td>\n",
       "      <td>-0.050467</td>\n",
       "      <td>-0.035428</td>\n",
       "      <td>-0.043417</td>\n",
       "      <td>-0.056971</td>\n",
       "      <td>2.785112e-17</td>\n",
       "      <td>-0.038776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-28</th>\n",
       "      <td>-0.118123</td>\n",
       "      <td>-0.051635</td>\n",
       "      <td>-0.075864</td>\n",
       "      <td>-0.067341</td>\n",
       "      <td>-0.003632</td>\n",
       "      <td>-0.103423</td>\n",
       "      <td>-0.024447</td>\n",
       "      <td>-0.036920</td>\n",
       "      <td>-0.061538</td>\n",
       "      <td>-0.066325</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081006</td>\n",
       "      <td>-0.032072</td>\n",
       "      <td>-0.057375</td>\n",
       "      <td>-0.008117</td>\n",
       "      <td>-0.085291</td>\n",
       "      <td>-0.091201</td>\n",
       "      <td>-0.048182</td>\n",
       "      <td>-0.059747</td>\n",
       "      <td>-5.193123e-02</td>\n",
       "      <td>-0.086363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-27</th>\n",
       "      <td>-0.101479</td>\n",
       "      <td>-0.102112</td>\n",
       "      <td>-0.078104</td>\n",
       "      <td>-0.096516</td>\n",
       "      <td>-0.022157</td>\n",
       "      <td>-0.010757</td>\n",
       "      <td>-0.104324</td>\n",
       "      <td>-0.040602</td>\n",
       "      <td>-0.088521</td>\n",
       "      <td>-0.060760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.047624</td>\n",
       "      <td>-0.083475</td>\n",
       "      <td>-0.066640</td>\n",
       "      <td>-0.092776</td>\n",
       "      <td>-0.089591</td>\n",
       "      <td>-0.087135</td>\n",
       "      <td>-0.037175</td>\n",
       "      <td>-0.049250</td>\n",
       "      <td>-7.456348e-02</td>\n",
       "      <td>-0.088761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 650 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              916328    936365    905271    905113    905802    905425  \\\n",
       "date                                                                     \n",
       "2018-05-11 -0.033261 -0.056147 -0.056673 -0.039225 -0.078155 -0.079176   \n",
       "2018-05-10 -0.001857 -0.007957 -0.056608 -0.033577 -0.012766 -0.038382   \n",
       "2018-05-09 -0.032497  0.014964 -0.041537 -0.019209  0.009759 -0.152829   \n",
       "2018-05-08 -0.119326 -0.029451 -0.070543 -0.015943  0.022881 -0.186745   \n",
       "2018-05-07 -0.039900  0.012059 -0.056630 -0.000645 -0.030319 -0.127271   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2017-08-02 -0.072393 -0.072199 -0.091406 -0.072376 -0.001441 -0.069006   \n",
       "2017-08-01 -0.081210 -0.020391  0.099262 -0.018858 -0.136634 -0.083397   \n",
       "2017-07-31 -0.112960 -0.071512 -0.105417 -0.066444 -0.069153 -0.081507   \n",
       "2017-07-28 -0.118123 -0.051635 -0.075864 -0.067341 -0.003632 -0.103423   \n",
       "2017-07-27 -0.101479 -0.102112 -0.078104 -0.096516 -0.022157 -0.010757   \n",
       "\n",
       "              906156    916305    992816    921093  ...    311917    69568X  \\\n",
       "date                                                ...                       \n",
       "2018-05-11 -0.055281 -0.036198 -0.062879 -0.060282  ... -0.030098 -0.058198   \n",
       "2018-05-10 -0.031704 -0.042638 -0.012963 -0.051148  ... -0.056529 -0.045813   \n",
       "2018-05-09 -0.034908 -0.049782 -0.029491 -0.085189  ...  0.030915 -0.006775   \n",
       "2018-05-08 -0.023214 -0.047493 -0.034577 -0.046590  ... -0.051192 -0.039952   \n",
       "2018-05-07 -0.049799 -0.023339 -0.029050 -0.079179  ... -0.046167  0.016789   \n",
       "...              ...       ...       ...       ...  ...       ...       ...   \n",
       "2017-08-02 -0.054409 -0.027164  0.062017 -0.128361  ... -0.066717 -0.000397   \n",
       "2017-08-01 -0.056063 -0.039072 -0.025318  0.021731  ... -0.054461 -0.056564   \n",
       "2017-07-31 -0.042095 -0.034474 -0.057237 -0.034575  ... -0.058288 -0.096375   \n",
       "2017-07-28 -0.024447 -0.036920 -0.061538 -0.066325  ... -0.081006 -0.032072   \n",
       "2017-07-27 -0.104324 -0.040602 -0.088521 -0.060760  ... -0.047624 -0.083475   \n",
       "\n",
       "              543755    77463M    29235J    131745    69487D    68157P  \\\n",
       "date                                                                     \n",
       "2018-05-11 -0.028429 -0.050278 -0.080133 -0.024472 -0.052140 -0.019944   \n",
       "2018-05-10 -0.015254  0.108949 -0.000408 -0.018463 -0.030641 -0.039673   \n",
       "2018-05-09 -0.098720  0.026074 -0.007489 -0.041385 -0.049166 -0.000544   \n",
       "2018-05-08 -0.071749 -0.048822 -0.039231  0.017063 -0.038039 -0.024803   \n",
       "2018-05-07  0.003434  0.055430 -0.044564 -0.025124 -0.087165  0.004368   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2017-08-02 -0.034413 -0.047709 -0.072616 -0.041856 -0.050222 -0.016288   \n",
       "2017-08-01 -0.034920  0.007488 -0.025096 -0.018787 -0.029751  0.011307   \n",
       "2017-07-31 -0.038522 -0.056268 -0.050467 -0.035428 -0.043417 -0.056971   \n",
       "2017-07-28 -0.057375 -0.008117 -0.085291 -0.091201 -0.048182 -0.059747   \n",
       "2017-07-27 -0.066640 -0.092776 -0.089591 -0.087135 -0.037175 -0.049250   \n",
       "\n",
       "                  9110RA    292703  \n",
       "date                                \n",
       "2018-05-11 -7.289672e-02 -0.031952  \n",
       "2018-05-10  7.790037e-03 -0.075911  \n",
       "2018-05-09 -3.590120e-02 -0.052591  \n",
       "2018-05-08 -2.449209e-02 -0.070835  \n",
       "2018-05-07 -4.533198e-02 -0.074954  \n",
       "...                  ...       ...  \n",
       "2017-08-02 -8.383439e-03 -0.039350  \n",
       "2017-08-01 -3.998029e-02 -0.038914  \n",
       "2017-07-31  2.785112e-17 -0.038776  \n",
       "2017-07-28 -5.193123e-02 -0.086363  \n",
       "2017-07-27 -7.456348e-02 -0.088761  \n",
       "\n",
       "[200 rows x 650 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_investable(pd.to_datetime('2018-05-11'), 200)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tts = train_test_split(df, test_size=0.2, shuffle=False)\n",
    "train = tts[0]\n",
    "test = tts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>916328</th>\n",
       "      <th>936365</th>\n",
       "      <th>905271</th>\n",
       "      <th>905113</th>\n",
       "      <th>905802</th>\n",
       "      <th>905425</th>\n",
       "      <th>906156</th>\n",
       "      <th>916305</th>\n",
       "      <th>992816</th>\n",
       "      <th>921093</th>\n",
       "      <th>...</th>\n",
       "      <th>311917</th>\n",
       "      <th>69568X</th>\n",
       "      <th>543755</th>\n",
       "      <th>77463M</th>\n",
       "      <th>29235J</th>\n",
       "      <th>131745</th>\n",
       "      <th>69487D</th>\n",
       "      <th>68157P</th>\n",
       "      <th>9110RA</th>\n",
       "      <th>292703</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-09-28</th>\n",
       "      <td>0.033774</td>\n",
       "      <td>-0.032232</td>\n",
       "      <td>-0.071387</td>\n",
       "      <td>-0.084644</td>\n",
       "      <td>-0.076437</td>\n",
       "      <td>-0.076465</td>\n",
       "      <td>-0.035913</td>\n",
       "      <td>-0.049128</td>\n",
       "      <td>-0.059533</td>\n",
       "      <td>-0.089435</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.053948</td>\n",
       "      <td>-0.042377</td>\n",
       "      <td>-0.044853</td>\n",
       "      <td>0.024979</td>\n",
       "      <td>-0.039263</td>\n",
       "      <td>-0.017803</td>\n",
       "      <td>-0.042860</td>\n",
       "      <td>-0.043971</td>\n",
       "      <td>-0.016182</td>\n",
       "      <td>-0.061822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-27</th>\n",
       "      <td>-0.130996</td>\n",
       "      <td>0.005307</td>\n",
       "      <td>-0.063771</td>\n",
       "      <td>-0.039784</td>\n",
       "      <td>-0.017728</td>\n",
       "      <td>-0.143054</td>\n",
       "      <td>-0.030959</td>\n",
       "      <td>-0.014198</td>\n",
       "      <td>-0.029317</td>\n",
       "      <td>-0.102140</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050615</td>\n",
       "      <td>-0.057166</td>\n",
       "      <td>-0.010357</td>\n",
       "      <td>0.044859</td>\n",
       "      <td>0.029447</td>\n",
       "      <td>-0.029128</td>\n",
       "      <td>-0.038377</td>\n",
       "      <td>-0.028755</td>\n",
       "      <td>0.010398</td>\n",
       "      <td>-0.014246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-26</th>\n",
       "      <td>-0.060831</td>\n",
       "      <td>-0.052681</td>\n",
       "      <td>-0.084904</td>\n",
       "      <td>-0.036838</td>\n",
       "      <td>-0.040810</td>\n",
       "      <td>-0.079077</td>\n",
       "      <td>-0.041050</td>\n",
       "      <td>-0.057685</td>\n",
       "      <td>-0.006364</td>\n",
       "      <td>-0.070131</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.062638</td>\n",
       "      <td>-0.033960</td>\n",
       "      <td>-0.045905</td>\n",
       "      <td>-0.014777</td>\n",
       "      <td>-0.020430</td>\n",
       "      <td>-0.003253</td>\n",
       "      <td>-0.046713</td>\n",
       "      <td>-0.067935</td>\n",
       "      <td>-0.022240</td>\n",
       "      <td>-0.070503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-25</th>\n",
       "      <td>-0.071303</td>\n",
       "      <td>-0.115842</td>\n",
       "      <td>-0.041493</td>\n",
       "      <td>-0.057992</td>\n",
       "      <td>0.042759</td>\n",
       "      <td>-0.058970</td>\n",
       "      <td>-0.062454</td>\n",
       "      <td>-0.030108</td>\n",
       "      <td>-0.065593</td>\n",
       "      <td>-0.051800</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045482</td>\n",
       "      <td>-0.065947</td>\n",
       "      <td>-0.069041</td>\n",
       "      <td>-0.064799</td>\n",
       "      <td>-0.071611</td>\n",
       "      <td>-0.040592</td>\n",
       "      <td>-0.050160</td>\n",
       "      <td>-0.048418</td>\n",
       "      <td>-0.053214</td>\n",
       "      <td>-0.049827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-22</th>\n",
       "      <td>0.003938</td>\n",
       "      <td>-0.045452</td>\n",
       "      <td>-0.090654</td>\n",
       "      <td>-0.104774</td>\n",
       "      <td>-0.000160</td>\n",
       "      <td>-0.142855</td>\n",
       "      <td>-0.057031</td>\n",
       "      <td>-0.033048</td>\n",
       "      <td>-0.067770</td>\n",
       "      <td>-0.059431</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060346</td>\n",
       "      <td>-0.109388</td>\n",
       "      <td>-0.023386</td>\n",
       "      <td>-0.023368</td>\n",
       "      <td>-0.035876</td>\n",
       "      <td>-0.042236</td>\n",
       "      <td>-0.047319</td>\n",
       "      <td>-0.029264</td>\n",
       "      <td>-0.056097</td>\n",
       "      <td>-0.055420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 650 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              916328    936365    905271    905113    905802    905425  \\\n",
       "date                                                                     \n",
       "2017-09-28  0.033774 -0.032232 -0.071387 -0.084644 -0.076437 -0.076465   \n",
       "2017-09-27 -0.130996  0.005307 -0.063771 -0.039784 -0.017728 -0.143054   \n",
       "2017-09-26 -0.060831 -0.052681 -0.084904 -0.036838 -0.040810 -0.079077   \n",
       "2017-09-25 -0.071303 -0.115842 -0.041493 -0.057992  0.042759 -0.058970   \n",
       "2017-09-22  0.003938 -0.045452 -0.090654 -0.104774 -0.000160 -0.142855   \n",
       "\n",
       "              906156    916305    992816    921093  ...    311917    69568X  \\\n",
       "date                                                ...                       \n",
       "2017-09-28 -0.035913 -0.049128 -0.059533 -0.089435  ... -0.053948 -0.042377   \n",
       "2017-09-27 -0.030959 -0.014198 -0.029317 -0.102140  ... -0.050615 -0.057166   \n",
       "2017-09-26 -0.041050 -0.057685 -0.006364 -0.070131  ... -0.062638 -0.033960   \n",
       "2017-09-25 -0.062454 -0.030108 -0.065593 -0.051800  ... -0.045482 -0.065947   \n",
       "2017-09-22 -0.057031 -0.033048 -0.067770 -0.059431  ... -0.060346 -0.109388   \n",
       "\n",
       "              543755    77463M    29235J    131745    69487D    68157P  \\\n",
       "date                                                                     \n",
       "2017-09-28 -0.044853  0.024979 -0.039263 -0.017803 -0.042860 -0.043971   \n",
       "2017-09-27 -0.010357  0.044859  0.029447 -0.029128 -0.038377 -0.028755   \n",
       "2017-09-26 -0.045905 -0.014777 -0.020430 -0.003253 -0.046713 -0.067935   \n",
       "2017-09-25 -0.069041 -0.064799 -0.071611 -0.040592 -0.050160 -0.048418   \n",
       "2017-09-22 -0.023386 -0.023368 -0.035876 -0.042236 -0.047319 -0.029264   \n",
       "\n",
       "              9110RA    292703  \n",
       "date                            \n",
       "2017-09-28 -0.016182 -0.061822  \n",
       "2017-09-27  0.010398 -0.014246  \n",
       "2017-09-26 -0.022240 -0.070503  \n",
       "2017-09-25 -0.053214 -0.049827  \n",
       "2017-09-22 -0.056097 -0.055420  \n",
       "\n",
       "[5 rows x 650 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>916328</th>\n",
       "      <th>936365</th>\n",
       "      <th>905271</th>\n",
       "      <th>905113</th>\n",
       "      <th>905802</th>\n",
       "      <th>905425</th>\n",
       "      <th>906156</th>\n",
       "      <th>916305</th>\n",
       "      <th>992816</th>\n",
       "      <th>921093</th>\n",
       "      <th>...</th>\n",
       "      <th>311917</th>\n",
       "      <th>69568X</th>\n",
       "      <th>543755</th>\n",
       "      <th>77463M</th>\n",
       "      <th>29235J</th>\n",
       "      <th>131745</th>\n",
       "      <th>69487D</th>\n",
       "      <th>68157P</th>\n",
       "      <th>9110RA</th>\n",
       "      <th>292703</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-08-02</th>\n",
       "      <td>-0.072393</td>\n",
       "      <td>-0.072199</td>\n",
       "      <td>-0.091406</td>\n",
       "      <td>-0.072376</td>\n",
       "      <td>-0.001441</td>\n",
       "      <td>-0.069006</td>\n",
       "      <td>-0.054409</td>\n",
       "      <td>-0.027164</td>\n",
       "      <td>0.062017</td>\n",
       "      <td>-0.128361</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.066717</td>\n",
       "      <td>-0.000397</td>\n",
       "      <td>-0.034413</td>\n",
       "      <td>-0.047709</td>\n",
       "      <td>-0.072616</td>\n",
       "      <td>-0.041856</td>\n",
       "      <td>-0.050222</td>\n",
       "      <td>-0.016288</td>\n",
       "      <td>-8.383439e-03</td>\n",
       "      <td>-0.039350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-01</th>\n",
       "      <td>-0.081210</td>\n",
       "      <td>-0.020391</td>\n",
       "      <td>0.099262</td>\n",
       "      <td>-0.018858</td>\n",
       "      <td>-0.136634</td>\n",
       "      <td>-0.083397</td>\n",
       "      <td>-0.056063</td>\n",
       "      <td>-0.039072</td>\n",
       "      <td>-0.025318</td>\n",
       "      <td>0.021731</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054461</td>\n",
       "      <td>-0.056564</td>\n",
       "      <td>-0.034920</td>\n",
       "      <td>0.007488</td>\n",
       "      <td>-0.025096</td>\n",
       "      <td>-0.018787</td>\n",
       "      <td>-0.029751</td>\n",
       "      <td>0.011307</td>\n",
       "      <td>-3.998029e-02</td>\n",
       "      <td>-0.038914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-31</th>\n",
       "      <td>-0.112960</td>\n",
       "      <td>-0.071512</td>\n",
       "      <td>-0.105417</td>\n",
       "      <td>-0.066444</td>\n",
       "      <td>-0.069153</td>\n",
       "      <td>-0.081507</td>\n",
       "      <td>-0.042095</td>\n",
       "      <td>-0.034474</td>\n",
       "      <td>-0.057237</td>\n",
       "      <td>-0.034575</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058288</td>\n",
       "      <td>-0.096375</td>\n",
       "      <td>-0.038522</td>\n",
       "      <td>-0.056268</td>\n",
       "      <td>-0.050467</td>\n",
       "      <td>-0.035428</td>\n",
       "      <td>-0.043417</td>\n",
       "      <td>-0.056971</td>\n",
       "      <td>2.785112e-17</td>\n",
       "      <td>-0.038776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-28</th>\n",
       "      <td>-0.118123</td>\n",
       "      <td>-0.051635</td>\n",
       "      <td>-0.075864</td>\n",
       "      <td>-0.067341</td>\n",
       "      <td>-0.003632</td>\n",
       "      <td>-0.103423</td>\n",
       "      <td>-0.024447</td>\n",
       "      <td>-0.036920</td>\n",
       "      <td>-0.061538</td>\n",
       "      <td>-0.066325</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081006</td>\n",
       "      <td>-0.032072</td>\n",
       "      <td>-0.057375</td>\n",
       "      <td>-0.008117</td>\n",
       "      <td>-0.085291</td>\n",
       "      <td>-0.091201</td>\n",
       "      <td>-0.048182</td>\n",
       "      <td>-0.059747</td>\n",
       "      <td>-5.193123e-02</td>\n",
       "      <td>-0.086363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-27</th>\n",
       "      <td>-0.101479</td>\n",
       "      <td>-0.102112</td>\n",
       "      <td>-0.078104</td>\n",
       "      <td>-0.096516</td>\n",
       "      <td>-0.022157</td>\n",
       "      <td>-0.010757</td>\n",
       "      <td>-0.104324</td>\n",
       "      <td>-0.040602</td>\n",
       "      <td>-0.088521</td>\n",
       "      <td>-0.060760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.047624</td>\n",
       "      <td>-0.083475</td>\n",
       "      <td>-0.066640</td>\n",
       "      <td>-0.092776</td>\n",
       "      <td>-0.089591</td>\n",
       "      <td>-0.087135</td>\n",
       "      <td>-0.037175</td>\n",
       "      <td>-0.049250</td>\n",
       "      <td>-7.456348e-02</td>\n",
       "      <td>-0.088761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 650 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              916328    936365    905271    905113    905802    905425  \\\n",
       "date                                                                     \n",
       "2017-08-02 -0.072393 -0.072199 -0.091406 -0.072376 -0.001441 -0.069006   \n",
       "2017-08-01 -0.081210 -0.020391  0.099262 -0.018858 -0.136634 -0.083397   \n",
       "2017-07-31 -0.112960 -0.071512 -0.105417 -0.066444 -0.069153 -0.081507   \n",
       "2017-07-28 -0.118123 -0.051635 -0.075864 -0.067341 -0.003632 -0.103423   \n",
       "2017-07-27 -0.101479 -0.102112 -0.078104 -0.096516 -0.022157 -0.010757   \n",
       "\n",
       "              906156    916305    992816    921093  ...    311917    69568X  \\\n",
       "date                                                ...                       \n",
       "2017-08-02 -0.054409 -0.027164  0.062017 -0.128361  ... -0.066717 -0.000397   \n",
       "2017-08-01 -0.056063 -0.039072 -0.025318  0.021731  ... -0.054461 -0.056564   \n",
       "2017-07-31 -0.042095 -0.034474 -0.057237 -0.034575  ... -0.058288 -0.096375   \n",
       "2017-07-28 -0.024447 -0.036920 -0.061538 -0.066325  ... -0.081006 -0.032072   \n",
       "2017-07-27 -0.104324 -0.040602 -0.088521 -0.060760  ... -0.047624 -0.083475   \n",
       "\n",
       "              543755    77463M    29235J    131745    69487D    68157P  \\\n",
       "date                                                                     \n",
       "2017-08-02 -0.034413 -0.047709 -0.072616 -0.041856 -0.050222 -0.016288   \n",
       "2017-08-01 -0.034920  0.007488 -0.025096 -0.018787 -0.029751  0.011307   \n",
       "2017-07-31 -0.038522 -0.056268 -0.050467 -0.035428 -0.043417 -0.056971   \n",
       "2017-07-28 -0.057375 -0.008117 -0.085291 -0.091201 -0.048182 -0.059747   \n",
       "2017-07-27 -0.066640 -0.092776 -0.089591 -0.087135 -0.037175 -0.049250   \n",
       "\n",
       "                  9110RA    292703  \n",
       "date                                \n",
       "2017-08-02 -8.383439e-03 -0.039350  \n",
       "2017-08-01 -3.998029e-02 -0.038914  \n",
       "2017-07-31  2.785112e-17 -0.038776  \n",
       "2017-07-28 -5.193123e-02 -0.086363  \n",
       "2017-07-27 -7.456348e-02 -0.088761  \n",
       "\n",
       "[5 rows x 650 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>640</th>\n",
       "      <th>641</th>\n",
       "      <th>642</th>\n",
       "      <th>643</th>\n",
       "      <th>644</th>\n",
       "      <th>645</th>\n",
       "      <th>646</th>\n",
       "      <th>647</th>\n",
       "      <th>648</th>\n",
       "      <th>649</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.661406</td>\n",
       "      <td>0.441272</td>\n",
       "      <td>0.526470</td>\n",
       "      <td>0.828958</td>\n",
       "      <td>0.460741</td>\n",
       "      <td>0.520905</td>\n",
       "      <td>0.428907</td>\n",
       "      <td>0.635693</td>\n",
       "      <td>0.393566</td>\n",
       "      <td>0.529545</td>\n",
       "      <td>...</td>\n",
       "      <td>0.343729</td>\n",
       "      <td>0.461508</td>\n",
       "      <td>0.571224</td>\n",
       "      <td>0.259734</td>\n",
       "      <td>0.385493</td>\n",
       "      <td>0.413940</td>\n",
       "      <td>0.525361</td>\n",
       "      <td>0.613774</td>\n",
       "      <td>0.145999</td>\n",
       "      <td>0.745686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.749938</td>\n",
       "      <td>0.551324</td>\n",
       "      <td>0.526684</td>\n",
       "      <td>0.837788</td>\n",
       "      <td>0.648450</td>\n",
       "      <td>0.707915</td>\n",
       "      <td>0.492168</td>\n",
       "      <td>0.598313</td>\n",
       "      <td>0.634965</td>\n",
       "      <td>0.560486</td>\n",
       "      <td>...</td>\n",
       "      <td>0.270439</td>\n",
       "      <td>0.505696</td>\n",
       "      <td>0.629955</td>\n",
       "      <td>0.714857</td>\n",
       "      <td>0.675644</td>\n",
       "      <td>0.429399</td>\n",
       "      <td>0.575786</td>\n",
       "      <td>0.548986</td>\n",
       "      <td>0.374450</td>\n",
       "      <td>0.664123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.663560</td>\n",
       "      <td>0.603668</td>\n",
       "      <td>0.576641</td>\n",
       "      <td>0.860254</td>\n",
       "      <td>0.713111</td>\n",
       "      <td>0.183266</td>\n",
       "      <td>0.483571</td>\n",
       "      <td>0.556848</td>\n",
       "      <td>0.555032</td>\n",
       "      <td>0.445173</td>\n",
       "      <td>...</td>\n",
       "      <td>0.512915</td>\n",
       "      <td>0.644981</td>\n",
       "      <td>0.257864</td>\n",
       "      <td>0.477973</td>\n",
       "      <td>0.649873</td>\n",
       "      <td>0.370430</td>\n",
       "      <td>0.532337</td>\n",
       "      <td>0.677480</td>\n",
       "      <td>0.250746</td>\n",
       "      <td>0.707392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.418774</td>\n",
       "      <td>0.502238</td>\n",
       "      <td>0.480496</td>\n",
       "      <td>0.865360</td>\n",
       "      <td>0.750778</td>\n",
       "      <td>0.027789</td>\n",
       "      <td>0.514947</td>\n",
       "      <td>0.570130</td>\n",
       "      <td>0.530440</td>\n",
       "      <td>0.575925</td>\n",
       "      <td>...</td>\n",
       "      <td>0.285238</td>\n",
       "      <td>0.526607</td>\n",
       "      <td>0.378104</td>\n",
       "      <td>0.263895</td>\n",
       "      <td>0.534352</td>\n",
       "      <td>0.520791</td>\n",
       "      <td>0.558434</td>\n",
       "      <td>0.597818</td>\n",
       "      <td>0.283049</td>\n",
       "      <td>0.673541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.642690</td>\n",
       "      <td>0.597032</td>\n",
       "      <td>0.526614</td>\n",
       "      <td>0.889280</td>\n",
       "      <td>0.598061</td>\n",
       "      <td>0.300429</td>\n",
       "      <td>0.443615</td>\n",
       "      <td>0.710329</td>\n",
       "      <td>0.557165</td>\n",
       "      <td>0.465532</td>\n",
       "      <td>...</td>\n",
       "      <td>0.299173</td>\n",
       "      <td>0.729053</td>\n",
       "      <td>0.713267</td>\n",
       "      <td>0.561883</td>\n",
       "      <td>0.514943</td>\n",
       "      <td>0.412264</td>\n",
       "      <td>0.443215</td>\n",
       "      <td>0.693610</td>\n",
       "      <td>0.224044</td>\n",
       "      <td>0.665899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 650 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.661406  0.441272  0.526470  0.828958  0.460741  0.520905  0.428907   \n",
       "1  0.749938  0.551324  0.526684  0.837788  0.648450  0.707915  0.492168   \n",
       "2  0.663560  0.603668  0.576641  0.860254  0.713111  0.183266  0.483571   \n",
       "3  0.418774  0.502238  0.480496  0.865360  0.750778  0.027789  0.514947   \n",
       "4  0.642690  0.597032  0.526614  0.889280  0.598061  0.300429  0.443615   \n",
       "\n",
       "        7         8         9    ...       640       641       642       643  \\\n",
       "0  0.635693  0.393566  0.529545  ...  0.343729  0.461508  0.571224  0.259734   \n",
       "1  0.598313  0.634965  0.560486  ...  0.270439  0.505696  0.629955  0.714857   \n",
       "2  0.556848  0.555032  0.445173  ...  0.512915  0.644981  0.257864  0.477973   \n",
       "3  0.570130  0.530440  0.575925  ...  0.285238  0.526607  0.378104  0.263895   \n",
       "4  0.710329  0.557165  0.465532  ...  0.299173  0.729053  0.713267  0.561883   \n",
       "\n",
       "        644       645       646       647       648       649  \n",
       "0  0.385493  0.413940  0.525361  0.613774  0.145999  0.745686  \n",
       "1  0.675644  0.429399  0.575786  0.548986  0.374450  0.664123  \n",
       "2  0.649873  0.370430  0.532337  0.677480  0.250746  0.707392  \n",
       "3  0.534352  0.520791  0.558434  0.597818  0.283049  0.673541  \n",
       "4  0.514943  0.412264  0.443215  0.693610  0.224044  0.665899  \n",
       "\n",
       "[5 rows x 650 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Scaling\n",
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "training_set_scaled = sc.fit_transform(train)\n",
    "test_set_scaled = sc.fit_transform(test)\n",
    "pd.DataFrame(training_set_scaled).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple multi-layer percepetron (MLP) autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 650)]             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 1953      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 650)               2600      \n",
      "=================================================================\n",
      "Total params: 4,553\n",
      "Trainable params: 4,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.3036 - val_loss: 0.2993\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3014 - val_loss: 0.2972\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.2993 - val_loss: 0.2945\n",
      "Epoch 4/500\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.2967 - val_loss: 0.2918\n",
      "Epoch 5/500\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.2939 - val_loss: 0.2890\n",
      "Epoch 6/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.2911 - val_loss: 0.2860\n",
      "Epoch 7/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.2882 - val_loss: 0.2828\n",
      "Epoch 8/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.2851 - val_loss: 0.2796\n",
      "Epoch 9/500\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.2819 - val_loss: 0.2764\n",
      "Epoch 10/500\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.2787 - val_loss: 0.2730\n",
      "Epoch 11/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.2754 - val_loss: 0.2696\n",
      "Epoch 12/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.2720 - val_loss: 0.2661\n",
      "Epoch 13/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.2685 - val_loss: 0.2625\n",
      "Epoch 14/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.2650 - val_loss: 0.2588\n",
      "Epoch 15/500\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.2614 - val_loss: 0.2551\n",
      "Epoch 16/500\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.2577 - val_loss: 0.2513\n",
      "Epoch 17/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.2540 - val_loss: 0.2476\n",
      "Epoch 18/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.2502 - val_loss: 0.2438\n",
      "Epoch 19/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.2465 - val_loss: 0.2400\n",
      "Epoch 20/500\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.2428 - val_loss: 0.2363\n",
      "Epoch 21/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2391 - val_loss: 0.2326\n",
      "Epoch 22/500\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.2355 - val_loss: 0.2289\n",
      "Epoch 23/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2318 - val_loss: 0.2253\n",
      "Epoch 24/500\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.2282 - val_loss: 0.2217\n",
      "Epoch 25/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.2247 - val_loss: 0.2182\n",
      "Epoch 26/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.2212 - val_loss: 0.2147\n",
      "Epoch 27/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.2177 - val_loss: 0.2112\n",
      "Epoch 28/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.2143 - val_loss: 0.2078\n",
      "Epoch 29/500\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.2109 - val_loss: 0.2044\n",
      "Epoch 30/500\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.2076 - val_loss: 0.2011\n",
      "Epoch 31/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.2043 - val_loss: 0.1978\n",
      "Epoch 32/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.2011 - val_loss: 0.1945\n",
      "Epoch 33/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.1979 - val_loss: 0.1913\n",
      "Epoch 34/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1947 - val_loss: 0.1882\n",
      "Epoch 35/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.1916 - val_loss: 0.1851\n",
      "Epoch 36/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.1885 - val_loss: 0.1820\n",
      "Epoch 37/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.1855 - val_loss: 0.1790\n",
      "Epoch 38/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.1826 - val_loss: 0.1761\n",
      "Epoch 39/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1796 - val_loss: 0.1731\n",
      "Epoch 40/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.1767 - val_loss: 0.1703\n",
      "Epoch 41/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.1739 - val_loss: 0.1674\n",
      "Epoch 42/500\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.1711 - val_loss: 0.1647\n",
      "Epoch 43/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.1684 - val_loss: 0.1619\n",
      "Epoch 44/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.1657 - val_loss: 0.1592\n",
      "Epoch 45/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1630 - val_loss: 0.1566\n",
      "Epoch 46/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.1604 - val_loss: 0.1539\n",
      "Epoch 47/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.1578 - val_loss: 0.1514\n",
      "Epoch 48/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.1553 - val_loss: 0.1489\n",
      "Epoch 49/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.1528 - val_loss: 0.1464\n",
      "Epoch 50/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.1503 - val_loss: 0.1439\n",
      "Epoch 51/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.1479 - val_loss: 0.1415\n",
      "Epoch 52/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.1456 - val_loss: 0.1391\n",
      "Epoch 53/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.1432 - val_loss: 0.1368\n",
      "Epoch 54/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.1409 - val_loss: 0.1345\n",
      "Epoch 55/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1387 - val_loss: 0.1323\n",
      "Epoch 56/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.1365 - val_loss: 0.1301\n",
      "Epoch 57/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.1343 - val_loss: 0.1279\n",
      "Epoch 58/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1321 - val_loss: 0.1258\n",
      "Epoch 59/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.1300 - val_loss: 0.1237\n",
      "Epoch 60/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.1280 - val_loss: 0.1216\n",
      "Epoch 61/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.1259 - val_loss: 0.1196\n",
      "Epoch 62/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.1239 - val_loss: 0.1176\n",
      "Epoch 63/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.1220 - val_loss: 0.1156\n",
      "Epoch 64/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.1201 - val_loss: 0.1137\n",
      "Epoch 65/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.1182 - val_loss: 0.1118\n",
      "Epoch 66/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.1163 - val_loss: 0.1099\n",
      "Epoch 67/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.1145 - val_loss: 0.1081\n",
      "Epoch 68/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.1127 - val_loss: 0.1063\n",
      "Epoch 69/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.1109 - val_loss: 0.1045\n",
      "Epoch 70/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1092 - val_loss: 0.1028\n",
      "Epoch 71/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1075 - val_loss: 0.1011\n",
      "Epoch 72/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1058 - val_loss: 0.0994\n",
      "Epoch 73/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1041 - val_loss: 0.0978\n",
      "Epoch 74/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1025 - val_loss: 0.0961\n",
      "Epoch 75/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.1009 - val_loss: 0.0946\n",
      "Epoch 76/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0994 - val_loss: 0.0930\n",
      "Epoch 77/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0978 - val_loss: 0.0915\n",
      "Epoch 78/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0963 - val_loss: 0.0899\n",
      "Epoch 79/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0948 - val_loss: 0.0885\n",
      "Epoch 80/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0934 - val_loss: 0.0870\n",
      "Epoch 81/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0920 - val_loss: 0.0856\n",
      "Epoch 82/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0906 - val_loss: 0.0842\n",
      "Epoch 83/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0892 - val_loss: 0.0828\n",
      "Epoch 84/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0878 - val_loss: 0.0814\n",
      "Epoch 85/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0865 - val_loss: 0.0801\n",
      "Epoch 86/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0852 - val_loss: 0.0788\n",
      "Epoch 87/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0839 - val_loss: 0.0775\n",
      "Epoch 88/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0827 - val_loss: 0.0763\n",
      "Epoch 89/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0814 - val_loss: 0.0750\n",
      "Epoch 90/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0802 - val_loss: 0.0738\n",
      "Epoch 91/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0790 - val_loss: 0.0726\n",
      "Epoch 92/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0779 - val_loss: 0.0715\n",
      "Epoch 93/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0767 - val_loss: 0.0703\n",
      "Epoch 94/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0756 - val_loss: 0.0692\n",
      "Epoch 95/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0745 - val_loss: 0.0681\n",
      "Epoch 96/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0734 - val_loss: 0.0670\n",
      "Epoch 97/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0723 - val_loss: 0.0659\n",
      "Epoch 98/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0713 - val_loss: 0.0649\n",
      "Epoch 99/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0703 - val_loss: 0.0638\n",
      "Epoch 100/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0693 - val_loss: 0.0628\n",
      "Epoch 101/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0683 - val_loss: 0.0618\n",
      "Epoch 102/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0673 - val_loss: 0.0609\n",
      "Epoch 103/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0664 - val_loss: 0.0599\n",
      "Epoch 104/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0654 - val_loss: 0.0590\n",
      "Epoch 105/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0645 - val_loss: 0.0581\n",
      "Epoch 106/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0636 - val_loss: 0.0572\n",
      "Epoch 107/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0627 - val_loss: 0.0563\n",
      "Epoch 108/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0619 - val_loss: 0.0554\n",
      "Epoch 109/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0610 - val_loss: 0.0545\n",
      "Epoch 110/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0602 - val_loss: 0.0537\n",
      "Epoch 111/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0594 - val_loss: 0.0529\n",
      "Epoch 112/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0586 - val_loss: 0.0521\n",
      "Epoch 113/500\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0578 - val_loss: 0.0513\n",
      "Epoch 114/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0570 - val_loss: 0.0505\n",
      "Epoch 115/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0562 - val_loss: 0.0498\n",
      "Epoch 116/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0555 - val_loss: 0.0490\n",
      "Epoch 117/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0548 - val_loss: 0.0483\n",
      "Epoch 118/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0541 - val_loss: 0.0476\n",
      "Epoch 119/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0534 - val_loss: 0.0469\n",
      "Epoch 120/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0527 - val_loss: 0.0462\n",
      "Epoch 121/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0520 - val_loss: 0.0455\n",
      "Epoch 122/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0513 - val_loss: 0.0448\n",
      "Epoch 123/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0507 - val_loss: 0.0442\n",
      "Epoch 124/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0500 - val_loss: 0.0435\n",
      "Epoch 125/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0494 - val_loss: 0.0429\n",
      "Epoch 126/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0488 - val_loss: 0.0423\n",
      "Epoch 127/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0482 - val_loss: 0.0417\n",
      "Epoch 128/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0476 - val_loss: 0.0411\n",
      "Epoch 129/500\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0470 - val_loss: 0.0405\n",
      "Epoch 130/500\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.0464 - val_loss: 0.0400\n",
      "Epoch 131/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0459 - val_loss: 0.0394\n",
      "Epoch 132/500\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0453 - val_loss: 0.0389\n",
      "Epoch 133/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0448 - val_loss: 0.0383\n",
      "Epoch 134/500\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0442 - val_loss: 0.0378\n",
      "Epoch 135/500\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0437 - val_loss: 0.0373\n",
      "Epoch 136/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0432 - val_loss: 0.0368\n",
      "Epoch 137/500\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0427 - val_loss: 0.0364\n",
      "Epoch 138/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0422 - val_loss: 0.0359\n",
      "Epoch 139/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0417 - val_loss: 0.0355\n",
      "Epoch 140/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0412 - val_loss: 0.0350\n",
      "Epoch 141/500\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0407 - val_loss: 0.0345\n",
      "Epoch 142/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0402 - val_loss: 0.0341\n",
      "Epoch 143/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0397 - val_loss: 0.0336\n",
      "Epoch 144/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0393 - val_loss: 0.0331\n",
      "Epoch 145/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0388 - val_loss: 0.0327\n",
      "Epoch 146/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0384 - val_loss: 0.0323\n",
      "Epoch 147/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0379 - val_loss: 0.0319\n",
      "Epoch 148/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0375 - val_loss: 0.0315\n",
      "Epoch 149/500\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0371 - val_loss: 0.0311\n",
      "Epoch 150/500\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0367 - val_loss: 0.0307\n",
      "Epoch 151/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0363 - val_loss: 0.0303\n",
      "Epoch 152/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0359 - val_loss: 0.0299\n",
      "Epoch 153/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0355 - val_loss: 0.0296\n",
      "Epoch 154/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0352 - val_loss: 0.0292\n",
      "Epoch 155/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0348 - val_loss: 0.0289\n",
      "Epoch 156/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0344 - val_loss: 0.0285\n",
      "Epoch 157/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0341 - val_loss: 0.0282\n",
      "Epoch 158/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0338 - val_loss: 0.0279\n",
      "Epoch 159/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0334 - val_loss: 0.0276\n",
      "Epoch 160/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0331 - val_loss: 0.0273\n",
      "Epoch 161/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0328 - val_loss: 0.0269\n",
      "Epoch 162/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0325 - val_loss: 0.0266\n",
      "Epoch 163/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0321 - val_loss: 0.0263\n",
      "Epoch 164/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0318 - val_loss: 0.0260\n",
      "Epoch 165/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0315 - val_loss: 0.0258\n",
      "Epoch 166/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0312 - val_loss: 0.0255\n",
      "Epoch 167/500\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0310 - val_loss: 0.0252\n",
      "Epoch 168/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0307 - val_loss: 0.0249\n",
      "Epoch 169/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0304 - val_loss: 0.0247\n",
      "Epoch 170/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0301 - val_loss: 0.0244\n",
      "Epoch 171/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0299 - val_loss: 0.0242\n",
      "Epoch 172/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0296 - val_loss: 0.0239\n",
      "Epoch 173/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0293 - val_loss: 0.0237\n",
      "Epoch 174/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0291 - val_loss: 0.0234\n",
      "Epoch 175/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0288 - val_loss: 0.0232\n",
      "Epoch 176/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0286 - val_loss: 0.0230\n",
      "Epoch 177/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0284 - val_loss: 0.0228\n",
      "Epoch 178/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0281 - val_loss: 0.0225\n",
      "Epoch 179/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0279 - val_loss: 0.0223\n",
      "Epoch 180/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0277 - val_loss: 0.0221\n",
      "Epoch 181/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0275 - val_loss: 0.0219\n",
      "Epoch 182/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0272 - val_loss: 0.0217\n",
      "Epoch 183/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0270 - val_loss: 0.0215\n",
      "Epoch 184/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0268 - val_loss: 0.0213\n",
      "Epoch 185/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0266 - val_loss: 0.0211\n",
      "Epoch 186/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0264 - val_loss: 0.0209\n",
      "Epoch 187/500\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0262 - val_loss: 0.0208\n",
      "Epoch 188/500\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0260 - val_loss: 0.0206\n",
      "Epoch 189/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0258 - val_loss: 0.0204\n",
      "Epoch 190/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0257 - val_loss: 0.0202\n",
      "Epoch 191/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0255 - val_loss: 0.0201\n",
      "Epoch 192/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0253 - val_loss: 0.0199\n",
      "Epoch 193/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0251 - val_loss: 0.0198\n",
      "Epoch 194/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0250 - val_loss: 0.0196\n",
      "Epoch 195/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0248 - val_loss: 0.0195\n",
      "Epoch 196/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0246 - val_loss: 0.0193\n",
      "Epoch 197/500\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0245 - val_loss: 0.0192\n",
      "Epoch 198/500\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0243 - val_loss: 0.0190\n",
      "Epoch 199/500\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0242 - val_loss: 0.0189\n",
      "Epoch 200/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0240 - val_loss: 0.0187\n",
      "Epoch 201/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0239 - val_loss: 0.0186\n",
      "Epoch 202/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0237 - val_loss: 0.0185\n",
      "Epoch 203/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0236 - val_loss: 0.0184\n",
      "Epoch 204/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0235 - val_loss: 0.0182\n",
      "Epoch 205/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0233 - val_loss: 0.0181\n",
      "Epoch 206/500\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.0232 - val_loss: 0.0180\n",
      "Epoch 207/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0231 - val_loss: 0.0179\n",
      "Epoch 208/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0229 - val_loss: 0.0178\n",
      "Epoch 209/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0228 - val_loss: 0.0177\n",
      "Epoch 210/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0227 - val_loss: 0.0176\n",
      "Epoch 211/500\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0226 - val_loss: 0.0174\n",
      "Epoch 212/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0225 - val_loss: 0.0173\n",
      "Epoch 213/500\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0223 - val_loss: 0.0172\n",
      "Epoch 214/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0222 - val_loss: 0.0171\n",
      "Epoch 215/500\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0221 - val_loss: 0.0170\n",
      "Epoch 216/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0220 - val_loss: 0.0170\n",
      "Epoch 217/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0219 - val_loss: 0.0169\n",
      "Epoch 218/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0218 - val_loss: 0.0168\n",
      "Epoch 219/500\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0217 - val_loss: 0.0167\n",
      "Epoch 220/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0216 - val_loss: 0.0166\n",
      "Epoch 221/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0215 - val_loss: 0.0165\n",
      "Epoch 222/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0214 - val_loss: 0.0164\n",
      "Epoch 223/500\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0213 - val_loss: 0.0164\n",
      "Epoch 224/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0212 - val_loss: 0.0163\n",
      "Epoch 225/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0211 - val_loss: 0.0162\n",
      "Epoch 226/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0210 - val_loss: 0.0161\n",
      "Epoch 227/500\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0210 - val_loss: 0.0161\n",
      "Epoch 228/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0209 - val_loss: 0.0160\n",
      "Epoch 229/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0208 - val_loss: 0.0159\n",
      "Epoch 230/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0207 - val_loss: 0.0159\n",
      "Epoch 231/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0206 - val_loss: 0.0158\n",
      "Epoch 232/500\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0205 - val_loss: 0.0157\n",
      "Epoch 233/500\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0205 - val_loss: 0.0157\n",
      "Epoch 234/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0204 - val_loss: 0.0156\n",
      "Epoch 235/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0203 - val_loss: 0.0155\n",
      "Epoch 236/500\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0202 - val_loss: 0.0155\n",
      "Epoch 237/500\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0202 - val_loss: 0.0154\n",
      "Epoch 238/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0201 - val_loss: 0.0154\n",
      "Epoch 239/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0200 - val_loss: 0.0153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 240/500\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0200 - val_loss: 0.0153\n",
      "Epoch 241/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0199 - val_loss: 0.0152\n",
      "Epoch 242/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0198 - val_loss: 0.0152\n",
      "Epoch 243/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0197 - val_loss: 0.0151\n",
      "Epoch 244/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0197 - val_loss: 0.0151\n",
      "Epoch 245/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0196 - val_loss: 0.0150\n",
      "Epoch 246/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0196 - val_loss: 0.0150\n",
      "Epoch 247/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0195 - val_loss: 0.0149\n",
      "Epoch 248/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0194 - val_loss: 0.0149\n",
      "Epoch 249/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0194 - val_loss: 0.0148\n",
      "Epoch 250/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0193 - val_loss: 0.0148\n",
      "Epoch 251/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0193 - val_loss: 0.0148\n",
      "Epoch 252/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0192 - val_loss: 0.0147\n",
      "Epoch 253/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0191 - val_loss: 0.0147\n",
      "Epoch 254/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0191 - val_loss: 0.0146\n",
      "Epoch 255/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 256/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 257/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0189 - val_loss: 0.0145\n",
      "Epoch 258/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0189 - val_loss: 0.0145\n",
      "Epoch 259/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0188 - val_loss: 0.0145\n",
      "Epoch 260/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0188 - val_loss: 0.0144\n",
      "Epoch 261/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0187 - val_loss: 0.0144\n",
      "Epoch 262/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0187 - val_loss: 0.0144\n",
      "Epoch 263/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0186 - val_loss: 0.0143\n",
      "Epoch 264/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0186 - val_loss: 0.0143\n",
      "Epoch 265/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0186 - val_loss: 0.0143\n",
      "Epoch 266/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0185 - val_loss: 0.0142\n",
      "Epoch 267/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0185 - val_loss: 0.0142\n",
      "Epoch 268/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0184 - val_loss: 0.0142\n",
      "Epoch 269/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0184 - val_loss: 0.0142\n",
      "Epoch 270/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0183 - val_loss: 0.0141\n",
      "Epoch 271/500\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0183 - val_loss: 0.0141\n",
      "Epoch 272/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0183 - val_loss: 0.0141\n",
      "Epoch 273/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0182 - val_loss: 0.0141\n",
      "Epoch 274/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0182 - val_loss: 0.0140\n",
      "Epoch 275/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0181 - val_loss: 0.0140\n",
      "Epoch 276/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0181 - val_loss: 0.0140\n",
      "Epoch 277/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0181 - val_loss: 0.0140\n",
      "Epoch 278/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0180 - val_loss: 0.0139\n",
      "Epoch 279/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0180 - val_loss: 0.0139\n",
      "Epoch 280/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0180 - val_loss: 0.0139\n",
      "Epoch 281/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0179 - val_loss: 0.0139\n",
      "Epoch 282/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0179 - val_loss: 0.0139\n",
      "Epoch 283/500\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0179 - val_loss: 0.0138\n",
      "Epoch 284/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0178 - val_loss: 0.0138\n",
      "Epoch 285/500\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0178 - val_loss: 0.0138\n",
      "Epoch 286/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0178 - val_loss: 0.0138\n",
      "Epoch 287/500\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0177 - val_loss: 0.0138\n",
      "Epoch 288/500\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0177 - val_loss: 0.0138\n",
      "Epoch 289/500\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0177 - val_loss: 0.0137\n",
      "Epoch 290/500\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0176 - val_loss: 0.0137\n",
      "Epoch 291/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0176 - val_loss: 0.0137\n",
      "Epoch 292/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0176 - val_loss: 0.0137\n",
      "Epoch 293/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0175 - val_loss: 0.0137\n",
      "Epoch 294/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0175 - val_loss: 0.0137\n",
      "Epoch 295/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0175 - val_loss: 0.0136\n",
      "Epoch 296/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0175 - val_loss: 0.0136\n",
      "Epoch 297/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0174 - val_loss: 0.0136\n",
      "Epoch 298/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0174 - val_loss: 0.0136\n",
      "Epoch 299/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0174 - val_loss: 0.0136\n",
      "Epoch 300/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0174 - val_loss: 0.0136\n",
      "Epoch 301/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0173 - val_loss: 0.0136\n",
      "Epoch 302/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0173 - val_loss: 0.0136\n",
      "Epoch 303/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0173 - val_loss: 0.0135\n",
      "Epoch 304/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0172 - val_loss: 0.0135\n",
      "Epoch 305/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0172 - val_loss: 0.0135\n",
      "Epoch 306/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0172 - val_loss: 0.0135\n",
      "Epoch 307/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0172 - val_loss: 0.0135\n",
      "Epoch 308/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0171 - val_loss: 0.0135\n",
      "Epoch 309/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0171 - val_loss: 0.0135\n",
      "Epoch 310/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0171 - val_loss: 0.0135\n",
      "Epoch 311/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0171 - val_loss: 0.0135\n",
      "Epoch 312/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0171 - val_loss: 0.0134\n",
      "Epoch 313/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0170 - val_loss: 0.0134\n",
      "Epoch 314/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0170 - val_loss: 0.0134\n",
      "Epoch 315/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0170 - val_loss: 0.0134\n",
      "Epoch 316/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0170 - val_loss: 0.0134\n",
      "Epoch 317/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0169 - val_loss: 0.0134\n",
      "Epoch 318/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0169 - val_loss: 0.0134\n",
      "Epoch 319/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0169 - val_loss: 0.0134\n",
      "Epoch 320/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0169 - val_loss: 0.0134\n",
      "Epoch 321/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0169 - val_loss: 0.0134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 322/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0168 - val_loss: 0.0134\n",
      "Epoch 323/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0168 - val_loss: 0.0134\n",
      "Epoch 324/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0168 - val_loss: 0.0133\n",
      "Epoch 325/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0168 - val_loss: 0.0133\n",
      "Epoch 326/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0168 - val_loss: 0.0133\n",
      "Epoch 327/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0167 - val_loss: 0.0133\n",
      "Epoch 328/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0167 - val_loss: 0.0133\n",
      "Epoch 329/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0167 - val_loss: 0.0133\n",
      "Epoch 330/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0167 - val_loss: 0.0133\n",
      "Epoch 331/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0167 - val_loss: 0.0133\n",
      "Epoch 332/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0167 - val_loss: 0.0133\n",
      "Epoch 333/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0166 - val_loss: 0.0133\n",
      "Epoch 334/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0166 - val_loss: 0.0133\n",
      "Epoch 335/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0166 - val_loss: 0.0133\n",
      "Epoch 336/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0166 - val_loss: 0.0133\n",
      "Epoch 337/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0166 - val_loss: 0.0133\n",
      "Epoch 338/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0166 - val_loss: 0.0133\n",
      "Epoch 339/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0165 - val_loss: 0.0133\n",
      "Epoch 340/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0165 - val_loss: 0.0133\n",
      "Epoch 341/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0165 - val_loss: 0.0133\n",
      "Epoch 342/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0165 - val_loss: 0.0132\n",
      "Epoch 343/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0165 - val_loss: 0.0132\n",
      "Epoch 344/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0165 - val_loss: 0.0132\n",
      "Epoch 345/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0164 - val_loss: 0.0132\n",
      "Epoch 346/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0164 - val_loss: 0.0132\n",
      "Epoch 347/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0164 - val_loss: 0.0132\n",
      "Epoch 348/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0164 - val_loss: 0.0132\n",
      "Epoch 349/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0164 - val_loss: 0.0132\n",
      "Epoch 350/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0164 - val_loss: 0.0132\n",
      "Epoch 351/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0164 - val_loss: 0.0132\n",
      "Epoch 352/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0163 - val_loss: 0.0132\n",
      "Epoch 353/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0163 - val_loss: 0.0132\n",
      "Epoch 354/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0163 - val_loss: 0.0132\n",
      "Epoch 355/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0163 - val_loss: 0.0132\n",
      "Epoch 356/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0163 - val_loss: 0.0132\n",
      "Epoch 357/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0163 - val_loss: 0.0132\n",
      "Epoch 358/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0163 - val_loss: 0.0132\n",
      "Epoch 359/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0162 - val_loss: 0.0132\n",
      "Epoch 360/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0162 - val_loss: 0.0132\n",
      "Epoch 361/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0162 - val_loss: 0.0132\n",
      "Epoch 362/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0162 - val_loss: 0.0132\n",
      "Epoch 363/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0162 - val_loss: 0.0132\n",
      "Epoch 364/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0162 - val_loss: 0.0132\n",
      "Epoch 365/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0162 - val_loss: 0.0132\n",
      "Epoch 366/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0162 - val_loss: 0.0132\n",
      "Epoch 367/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0161 - val_loss: 0.0132\n",
      "Epoch 368/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0161 - val_loss: 0.0132\n",
      "Epoch 369/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0161 - val_loss: 0.0131\n",
      "Epoch 370/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0161 - val_loss: 0.0131\n",
      "Epoch 371/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0161 - val_loss: 0.0131\n",
      "Epoch 372/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0161 - val_loss: 0.0131\n",
      "Epoch 373/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0161 - val_loss: 0.0131\n",
      "Epoch 374/500\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0161 - val_loss: 0.0131\n",
      "Epoch 375/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0160 - val_loss: 0.0131\n",
      "Epoch 376/500\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0160 - val_loss: 0.0131\n",
      "Epoch 377/500\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0160 - val_loss: 0.0131\n",
      "Epoch 378/500\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0160 - val_loss: 0.0131\n",
      "Epoch 379/500\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0160 - val_loss: 0.0131\n",
      "Epoch 380/500\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0160 - val_loss: 0.0131\n",
      "Epoch 381/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0160 - val_loss: 0.0131\n",
      "Epoch 382/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0160 - val_loss: 0.0131\n",
      "Epoch 383/500\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0160 - val_loss: 0.0131\n",
      "Epoch 384/500\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0159 - val_loss: 0.0131\n",
      "Epoch 385/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0159 - val_loss: 0.0131\n",
      "Epoch 386/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0159 - val_loss: 0.0131\n",
      "Epoch 387/500\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0159 - val_loss: 0.0131\n",
      "Epoch 388/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0159 - val_loss: 0.0131\n",
      "Epoch 389/500\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0159 - val_loss: 0.0131\n",
      "Epoch 390/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0159 - val_loss: 0.0131\n",
      "Epoch 391/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0159 - val_loss: 0.0132\n",
      "Epoch 392/500\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0159 - val_loss: 0.0131\n",
      "Epoch 393/500\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0158 - val_loss: 0.0131\n",
      "Epoch 394/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0159 - val_loss: 0.0132\n",
      "Epoch 395/500\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0159 - val_loss: 0.0132\n",
      "Epoch 396/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0158 - val_loss: 0.0130\n",
      "Epoch 397/500\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0159 - val_loss: 0.0131\n",
      "Epoch 398/500\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0158 - val_loss: 0.0132\n",
      "Epoch 399/500\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0158 - val_loss: 0.0131\n",
      "Epoch 400/500\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0158 - val_loss: 0.0130\n",
      "Epoch 401/500\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 0.0158 - val_loss: 0.0131\n",
      "Epoch 402/500\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0158 - val_loss: 0.0132\n",
      "Epoch 403/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0158 - val_loss: 0.0131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 404/500\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0158 - val_loss: 0.0131\n",
      "Epoch 405/500\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0158 - val_loss: 0.0131\n",
      "Epoch 406/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0157 - val_loss: 0.0132\n",
      "Epoch 407/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0157 - val_loss: 0.0131\n",
      "Epoch 408/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0157 - val_loss: 0.0131\n",
      "Epoch 409/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0157 - val_loss: 0.0131\n",
      "Epoch 410/500\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0157 - val_loss: 0.0131\n",
      "Epoch 411/500\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0157 - val_loss: 0.0131\n",
      "Epoch 412/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0157 - val_loss: 0.0131\n",
      "Epoch 413/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0157 - val_loss: 0.0131\n",
      "Epoch 414/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0157 - val_loss: 0.0131\n",
      "Epoch 415/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0157 - val_loss: 0.0131\n",
      "Epoch 416/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0157 - val_loss: 0.0131\n",
      "Epoch 417/500\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0156 - val_loss: 0.0131\n",
      "Epoch 418/500\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0156 - val_loss: 0.0131\n",
      "Epoch 419/500\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.0156 - val_loss: 0.0131\n",
      "Epoch 420/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0156 - val_loss: 0.0131\n",
      "Epoch 421/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0156 - val_loss: 0.0131\n",
      "Epoch 422/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0156 - val_loss: 0.0131\n",
      "Epoch 423/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0156 - val_loss: 0.0131\n",
      "Epoch 424/500\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0156 - val_loss: 0.0131\n",
      "Epoch 425/500\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0156 - val_loss: 0.0131\n",
      "Epoch 426/500\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0156 - val_loss: 0.0131\n",
      "Epoch 427/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0156 - val_loss: 0.0131\n",
      "Epoch 428/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0156 - val_loss: 0.0131\n",
      "Epoch 429/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0156 - val_loss: 0.0131\n",
      "Epoch 430/500\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0156 - val_loss: 0.0131\n",
      "Epoch 431/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0155 - val_loss: 0.0131\n",
      "Epoch 432/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0155 - val_loss: 0.0131\n",
      "Epoch 433/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0155 - val_loss: 0.0131\n",
      "Epoch 434/500\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0155 - val_loss: 0.0131\n",
      "Epoch 435/500\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.0155 - val_loss: 0.0131\n",
      "Epoch 436/500\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0155 - val_loss: 0.0131\n",
      "Epoch 437/500\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0155 - val_loss: 0.0131\n",
      "Epoch 438/500\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.0155 - val_loss: 0.0131\n",
      "Epoch 439/500\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.0155 - val_loss: 0.0131\n",
      "Epoch 440/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0155 - val_loss: 0.0131\n",
      "Epoch 441/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0155 - val_loss: 0.0131\n",
      "Epoch 442/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0155 - val_loss: 0.0131\n",
      "Epoch 443/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0155 - val_loss: 0.0131\n",
      "Epoch 444/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0155 - val_loss: 0.0131\n",
      "Epoch 445/500\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0154 - val_loss: 0.0131\n",
      "Epoch 446/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0154 - val_loss: 0.0131\n",
      "Epoch 447/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0154 - val_loss: 0.0131\n",
      "Epoch 448/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0154 - val_loss: 0.0131\n",
      "Epoch 449/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0154 - val_loss: 0.0131\n",
      "Epoch 450/500\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0154 - val_loss: 0.0131\n",
      "Epoch 451/500\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0154 - val_loss: 0.0131\n",
      "Epoch 452/500\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0154 - val_loss: 0.0131\n",
      "Epoch 453/500\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0154 - val_loss: 0.0131\n",
      "Epoch 454/500\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0154 - val_loss: 0.0131\n",
      "Epoch 455/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0154 - val_loss: 0.0131\n",
      "Epoch 456/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0154 - val_loss: 0.0131\n",
      "Epoch 457/500\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0154 - val_loss: 0.0131\n",
      "Epoch 458/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0154 - val_loss: 0.0131\n",
      "Epoch 459/500\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0154 - val_loss: 0.0131\n",
      "Epoch 460/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0153 - val_loss: 0.0131\n",
      "Epoch 461/500\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0153 - val_loss: 0.0131\n",
      "Epoch 462/500\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0153 - val_loss: 0.0131\n",
      "Epoch 463/500\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0153 - val_loss: 0.0131\n",
      "Epoch 464/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0153 - val_loss: 0.0131\n",
      "Epoch 465/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0153 - val_loss: 0.0131\n",
      "Epoch 466/500\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0153 - val_loss: 0.0131\n",
      "Epoch 467/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0153 - val_loss: 0.0131\n",
      "Epoch 468/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0153 - val_loss: 0.0131\n",
      "Epoch 469/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0153 - val_loss: 0.0131\n",
      "Epoch 470/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0153 - val_loss: 0.0131\n",
      "Epoch 471/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0153 - val_loss: 0.0131\n",
      "Epoch 472/500\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0153 - val_loss: 0.0131\n",
      "Epoch 473/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0153 - val_loss: 0.0131\n",
      "Epoch 474/500\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0153 - val_loss: 0.0131\n",
      "Epoch 475/500\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0153 - val_loss: 0.0131\n",
      "Epoch 476/500\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0153 - val_loss: 0.0131\n",
      "Epoch 477/500\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0152 - val_loss: 0.0131\n",
      "Epoch 478/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0152 - val_loss: 0.0131\n",
      "Epoch 479/500\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0152 - val_loss: 0.0131\n",
      "Epoch 480/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0152 - val_loss: 0.0131\n",
      "Epoch 481/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0152 - val_loss: 0.0131\n",
      "Epoch 482/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0152 - val_loss: 0.0131\n",
      "Epoch 483/500\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0152 - val_loss: 0.0131\n",
      "Epoch 484/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0152 - val_loss: 0.0131\n",
      "Epoch 485/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0152 - val_loss: 0.0131\n",
      "Epoch 486/500\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0152 - val_loss: 0.0131\n",
      "Epoch 487/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0152 - val_loss: 0.0131\n",
      "Epoch 488/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0152 - val_loss: 0.0131\n",
      "Epoch 489/500\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0152 - val_loss: 0.0131\n",
      "Epoch 490/500\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0152 - val_loss: 0.0131\n",
      "Epoch 491/500\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0152 - val_loss: 0.0131\n",
      "Epoch 492/500\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0152 - val_loss: 0.0131\n",
      "Epoch 493/500\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 0.0152 - val_loss: 0.0131\n",
      "Epoch 494/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0151 - val_loss: 0.0131\n",
      "Epoch 495/500\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0151 - val_loss: 0.0131\n",
      "Epoch 496/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0151 - val_loss: 0.0131\n",
      "Epoch 497/500\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0151 - val_loss: 0.0131\n",
      "Epoch 498/500\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0151 - val_loss: 0.0131\n",
      "Epoch 499/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0151 - val_loss: 0.0131\n",
      "Epoch 500/500\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0151 - val_loss: 0.0131\n"
     ]
    }
   ],
   "source": [
    "# calculated log returns (i.e. the log of the difference between the price x+1 and price x)\n",
    "# windows of train.shape[1] consecutive returns will be produced. \n",
    "# Can be normalized with a MinMaxScaler to the range [0,1]??\n",
    "\n",
    "window_length = training_set_scaled.shape[1]\n",
    "encoding_dim = 3\n",
    "epochs = 500\n",
    "\n",
    "# compress the input to a 3-dimensional latent space. \n",
    "\n",
    "# input placeholder\n",
    "input_window = Input(shape=(window_length,))\n",
    "# encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation='tanh')(input_window) #tanh, linear, leakyrelu\n",
    "# lossy reconstruction of the input\n",
    "decoded = Dense(window_length, activation='linear')(encoded) #linear\n",
    "\n",
    "# model mapping an input to its reconstruction\n",
    "autoencoder = Model(input_window, decoded)\n",
    "\n",
    "# model mapping an input to its encoded representation\n",
    "encoder = Model(input_window, encoded)\n",
    "\n",
    "\n",
    "autoencoder.summary()\n",
    "autoencoder.compile(optimizer='adam', loss='MeanSquaredError') #MSE\n",
    "history = autoencoder.fit(training_set_scaled, training_set_scaled,\n",
    "                epochs=epochs,\n",
    "                batch_size=1024,\n",
    "                shuffle=True,\n",
    "                validation_split = 0.2)       \n",
    "#                 validation_data=(test_set_scaled, test_set_scaled))\n",
    "\n",
    "decoded_stocks = autoencoder.predict(test_set_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.997219</td>\n",
       "      <td>0.944816</td>\n",
       "      <td>0.966394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.998349</td>\n",
       "      <td>0.973568</td>\n",
       "      <td>0.957678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.997940</td>\n",
       "      <td>0.943733</td>\n",
       "      <td>0.975412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.998998</td>\n",
       "      <td>0.975444</td>\n",
       "      <td>0.993826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.998769</td>\n",
       "      <td>0.988266</td>\n",
       "      <td>0.993771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.998554</td>\n",
       "      <td>0.946912</td>\n",
       "      <td>0.979083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.998091</td>\n",
       "      <td>0.918583</td>\n",
       "      <td>0.958994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.998828</td>\n",
       "      <td>0.984794</td>\n",
       "      <td>0.979714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.999664</td>\n",
       "      <td>0.986845</td>\n",
       "      <td>0.996951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.995219</td>\n",
       "      <td>0.892267</td>\n",
       "      <td>0.996083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.998672</td>\n",
       "      <td>0.959982</td>\n",
       "      <td>0.960761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.999134</td>\n",
       "      <td>0.981018</td>\n",
       "      <td>0.990743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.993481</td>\n",
       "      <td>0.772151</td>\n",
       "      <td>0.840317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.998654</td>\n",
       "      <td>0.987438</td>\n",
       "      <td>0.976674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.999153</td>\n",
       "      <td>0.968303</td>\n",
       "      <td>0.992332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.999004</td>\n",
       "      <td>0.974307</td>\n",
       "      <td>0.986668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.997997</td>\n",
       "      <td>0.968127</td>\n",
       "      <td>0.941091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.994870</td>\n",
       "      <td>0.840796</td>\n",
       "      <td>0.969511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.997879</td>\n",
       "      <td>0.969070</td>\n",
       "      <td>0.953123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.994452</td>\n",
       "      <td>0.949851</td>\n",
       "      <td>0.930176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.996625</td>\n",
       "      <td>0.890691</td>\n",
       "      <td>0.869342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.999239</td>\n",
       "      <td>0.988402</td>\n",
       "      <td>0.995515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.997461</td>\n",
       "      <td>0.928139</td>\n",
       "      <td>0.965144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.997747</td>\n",
       "      <td>0.980335</td>\n",
       "      <td>0.959002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.977329</td>\n",
       "      <td>0.402392</td>\n",
       "      <td>0.040570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.997614</td>\n",
       "      <td>0.947102</td>\n",
       "      <td>0.959814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.997377</td>\n",
       "      <td>0.923885</td>\n",
       "      <td>0.974206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.999748</td>\n",
       "      <td>0.983305</td>\n",
       "      <td>0.998278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.997635</td>\n",
       "      <td>0.966816</td>\n",
       "      <td>0.957049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.960820</td>\n",
       "      <td>0.357483</td>\n",
       "      <td>0.164000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.997891</td>\n",
       "      <td>0.873378</td>\n",
       "      <td>0.961415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.995995</td>\n",
       "      <td>0.929818</td>\n",
       "      <td>0.944701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.996934</td>\n",
       "      <td>0.950037</td>\n",
       "      <td>0.954585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.999126</td>\n",
       "      <td>0.983442</td>\n",
       "      <td>0.968180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.994799</td>\n",
       "      <td>0.956685</td>\n",
       "      <td>0.976715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.992700</td>\n",
       "      <td>0.722097</td>\n",
       "      <td>0.973388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.996476</td>\n",
       "      <td>0.943133</td>\n",
       "      <td>0.965747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.995114</td>\n",
       "      <td>0.969281</td>\n",
       "      <td>0.986056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.998869</td>\n",
       "      <td>0.871189</td>\n",
       "      <td>0.989837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.997782</td>\n",
       "      <td>0.953510</td>\n",
       "      <td>0.979865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2\n",
       "0   0.997219  0.944816  0.966394\n",
       "1   0.998349  0.973568  0.957678\n",
       "2   0.997940  0.943733  0.975412\n",
       "3   0.998998  0.975444  0.993826\n",
       "4   0.998769  0.988266  0.993771\n",
       "5   0.998554  0.946912  0.979083\n",
       "6   0.998091  0.918583  0.958994\n",
       "7   0.998828  0.984794  0.979714\n",
       "8   0.999664  0.986845  0.996951\n",
       "9   0.995219  0.892267  0.996083\n",
       "10  0.998672  0.959982  0.960761\n",
       "11  0.999134  0.981018  0.990743\n",
       "12  0.993481  0.772151  0.840317\n",
       "13  0.998654  0.987438  0.976674\n",
       "14  0.999153  0.968303  0.992332\n",
       "15  0.999004  0.974307  0.986668\n",
       "16  0.997997  0.968127  0.941091\n",
       "17  0.994870  0.840796  0.969511\n",
       "18  0.997879  0.969070  0.953123\n",
       "19  0.994452  0.949851  0.930176\n",
       "20  0.996625  0.890691  0.869342\n",
       "21  0.999239  0.988402  0.995515\n",
       "22  0.997461  0.928139  0.965144\n",
       "23  0.997747  0.980335  0.959002\n",
       "24  0.977329  0.402392  0.040570\n",
       "25  0.997614  0.947102  0.959814\n",
       "26  0.997377  0.923885  0.974206\n",
       "27  0.999748  0.983305  0.998278\n",
       "28  0.997635  0.966816  0.957049\n",
       "29  0.960820  0.357483  0.164000\n",
       "30  0.997891  0.873378  0.961415\n",
       "31  0.995995  0.929818  0.944701\n",
       "32  0.996934  0.950037  0.954585\n",
       "33  0.999126  0.983442  0.968180\n",
       "34  0.994799  0.956685  0.976715\n",
       "35  0.992700  0.722097  0.973388\n",
       "36  0.996476  0.943133  0.965747\n",
       "37  0.995114  0.969281  0.986056\n",
       "38  0.998869  0.871189  0.989837\n",
       "39  0.997782  0.953510  0.979865"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(encoder.predict(test_set_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>640</th>\n",
       "      <th>641</th>\n",
       "      <th>642</th>\n",
       "      <th>643</th>\n",
       "      <th>644</th>\n",
       "      <th>645</th>\n",
       "      <th>646</th>\n",
       "      <th>647</th>\n",
       "      <th>648</th>\n",
       "      <th>649</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.602029</td>\n",
       "      <td>0.534340</td>\n",
       "      <td>0.528308</td>\n",
       "      <td>0.850182</td>\n",
       "      <td>0.611104</td>\n",
       "      <td>0.507425</td>\n",
       "      <td>0.488035</td>\n",
       "      <td>0.651951</td>\n",
       "      <td>0.506625</td>\n",
       "      <td>0.579119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.320729</td>\n",
       "      <td>0.600826</td>\n",
       "      <td>0.553998</td>\n",
       "      <td>0.391519</td>\n",
       "      <td>0.567897</td>\n",
       "      <td>0.414377</td>\n",
       "      <td>0.545895</td>\n",
       "      <td>0.546801</td>\n",
       "      <td>0.299661</td>\n",
       "      <td>0.745794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.606245</td>\n",
       "      <td>0.537300</td>\n",
       "      <td>0.532513</td>\n",
       "      <td>0.856057</td>\n",
       "      <td>0.612496</td>\n",
       "      <td>0.511252</td>\n",
       "      <td>0.488982</td>\n",
       "      <td>0.656954</td>\n",
       "      <td>0.511792</td>\n",
       "      <td>0.580247</td>\n",
       "      <td>...</td>\n",
       "      <td>0.320500</td>\n",
       "      <td>0.604097</td>\n",
       "      <td>0.558255</td>\n",
       "      <td>0.392923</td>\n",
       "      <td>0.570007</td>\n",
       "      <td>0.415443</td>\n",
       "      <td>0.551795</td>\n",
       "      <td>0.548511</td>\n",
       "      <td>0.302581</td>\n",
       "      <td>0.748608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.603670</td>\n",
       "      <td>0.535562</td>\n",
       "      <td>0.529603</td>\n",
       "      <td>0.851949</td>\n",
       "      <td>0.613004</td>\n",
       "      <td>0.508727</td>\n",
       "      <td>0.489753</td>\n",
       "      <td>0.653561</td>\n",
       "      <td>0.507100</td>\n",
       "      <td>0.581100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.321700</td>\n",
       "      <td>0.602515</td>\n",
       "      <td>0.555094</td>\n",
       "      <td>0.392149</td>\n",
       "      <td>0.569365</td>\n",
       "      <td>0.415168</td>\n",
       "      <td>0.546309</td>\n",
       "      <td>0.547907</td>\n",
       "      <td>0.300242</td>\n",
       "      <td>0.747958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.613798</td>\n",
       "      <td>0.542850</td>\n",
       "      <td>0.538759</td>\n",
       "      <td>0.864232</td>\n",
       "      <td>0.620366</td>\n",
       "      <td>0.517471</td>\n",
       "      <td>0.496109</td>\n",
       "      <td>0.664572</td>\n",
       "      <td>0.514541</td>\n",
       "      <td>0.588619</td>\n",
       "      <td>...</td>\n",
       "      <td>0.324287</td>\n",
       "      <td>0.611622</td>\n",
       "      <td>0.563594</td>\n",
       "      <td>0.395554</td>\n",
       "      <td>0.576323</td>\n",
       "      <td>0.418675</td>\n",
       "      <td>0.554445</td>\n",
       "      <td>0.553041</td>\n",
       "      <td>0.305621</td>\n",
       "      <td>0.757997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.616340</td>\n",
       "      <td>0.544645</td>\n",
       "      <td>0.541208</td>\n",
       "      <td>0.867482</td>\n",
       "      <td>0.621658</td>\n",
       "      <td>0.519757</td>\n",
       "      <td>0.497166</td>\n",
       "      <td>0.667472</td>\n",
       "      <td>0.516972</td>\n",
       "      <td>0.589912</td>\n",
       "      <td>...</td>\n",
       "      <td>0.324503</td>\n",
       "      <td>0.613739</td>\n",
       "      <td>0.565947</td>\n",
       "      <td>0.396343</td>\n",
       "      <td>0.577800</td>\n",
       "      <td>0.419376</td>\n",
       "      <td>0.557186</td>\n",
       "      <td>0.554107</td>\n",
       "      <td>0.307199</td>\n",
       "      <td>0.760097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.605106</td>\n",
       "      <td>0.536609</td>\n",
       "      <td>0.530847</td>\n",
       "      <td>0.853677</td>\n",
       "      <td>0.614213</td>\n",
       "      <td>0.509927</td>\n",
       "      <td>0.490803</td>\n",
       "      <td>0.655082</td>\n",
       "      <td>0.508041</td>\n",
       "      <td>0.582313</td>\n",
       "      <td>...</td>\n",
       "      <td>0.322196</td>\n",
       "      <td>0.603854</td>\n",
       "      <td>0.556242</td>\n",
       "      <td>0.392678</td>\n",
       "      <td>0.570435</td>\n",
       "      <td>0.415740</td>\n",
       "      <td>0.547314</td>\n",
       "      <td>0.548736</td>\n",
       "      <td>0.300937</td>\n",
       "      <td>0.749492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.595403</td>\n",
       "      <td>0.529624</td>\n",
       "      <td>0.522109</td>\n",
       "      <td>0.842056</td>\n",
       "      <td>0.606960</td>\n",
       "      <td>0.501555</td>\n",
       "      <td>0.484495</td>\n",
       "      <td>0.644585</td>\n",
       "      <td>0.501239</td>\n",
       "      <td>0.574826</td>\n",
       "      <td>...</td>\n",
       "      <td>0.319561</td>\n",
       "      <td>0.595066</td>\n",
       "      <td>0.548197</td>\n",
       "      <td>0.389454</td>\n",
       "      <td>0.563681</td>\n",
       "      <td>0.412365</td>\n",
       "      <td>0.539907</td>\n",
       "      <td>0.543819</td>\n",
       "      <td>0.295864</td>\n",
       "      <td>0.739690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.612889</td>\n",
       "      <td>0.542129</td>\n",
       "      <td>0.538270</td>\n",
       "      <td>0.863666</td>\n",
       "      <td>0.618366</td>\n",
       "      <td>0.516862</td>\n",
       "      <td>0.494200</td>\n",
       "      <td>0.663918</td>\n",
       "      <td>0.515424</td>\n",
       "      <td>0.586403</td>\n",
       "      <td>...</td>\n",
       "      <td>0.323009</td>\n",
       "      <td>0.610393</td>\n",
       "      <td>0.563390</td>\n",
       "      <td>0.395184</td>\n",
       "      <td>0.575062</td>\n",
       "      <td>0.418003</td>\n",
       "      <td>0.555610</td>\n",
       "      <td>0.552168</td>\n",
       "      <td>0.305686</td>\n",
       "      <td>0.756033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.616788</td>\n",
       "      <td>0.544996</td>\n",
       "      <td>0.541504</td>\n",
       "      <td>0.867984</td>\n",
       "      <td>0.622333</td>\n",
       "      <td>0.520065</td>\n",
       "      <td>0.497766</td>\n",
       "      <td>0.667876</td>\n",
       "      <td>0.517040</td>\n",
       "      <td>0.590565</td>\n",
       "      <td>...</td>\n",
       "      <td>0.324889</td>\n",
       "      <td>0.614244</td>\n",
       "      <td>0.566199</td>\n",
       "      <td>0.396581</td>\n",
       "      <td>0.578283</td>\n",
       "      <td>0.419680</td>\n",
       "      <td>0.557209</td>\n",
       "      <td>0.554533</td>\n",
       "      <td>0.307293</td>\n",
       "      <td>0.760780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.597031</td>\n",
       "      <td>0.530922</td>\n",
       "      <td>0.522852</td>\n",
       "      <td>0.842331</td>\n",
       "      <td>0.611448</td>\n",
       "      <td>0.502657</td>\n",
       "      <td>0.488933</td>\n",
       "      <td>0.645521</td>\n",
       "      <td>0.498061</td>\n",
       "      <td>0.580150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.322557</td>\n",
       "      <td>0.597566</td>\n",
       "      <td>0.548102</td>\n",
       "      <td>0.389886</td>\n",
       "      <td>0.566335</td>\n",
       "      <td>0.413595</td>\n",
       "      <td>0.535938</td>\n",
       "      <td>0.545302</td>\n",
       "      <td>0.295386</td>\n",
       "      <td>0.744077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.604155</td>\n",
       "      <td>0.535842</td>\n",
       "      <td>0.530413</td>\n",
       "      <td>0.853262</td>\n",
       "      <td>0.611774</td>\n",
       "      <td>0.509323</td>\n",
       "      <td>0.488449</td>\n",
       "      <td>0.654486</td>\n",
       "      <td>0.509420</td>\n",
       "      <td>0.579564</td>\n",
       "      <td>...</td>\n",
       "      <td>0.320588</td>\n",
       "      <td>0.602460</td>\n",
       "      <td>0.556182</td>\n",
       "      <td>0.392301</td>\n",
       "      <td>0.568955</td>\n",
       "      <td>0.414965</td>\n",
       "      <td>0.549083</td>\n",
       "      <td>0.547751</td>\n",
       "      <td>0.301145</td>\n",
       "      <td>0.747146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.614332</td>\n",
       "      <td>0.543214</td>\n",
       "      <td>0.539344</td>\n",
       "      <td>0.865061</td>\n",
       "      <td>0.620330</td>\n",
       "      <td>0.517983</td>\n",
       "      <td>0.496017</td>\n",
       "      <td>0.665258</td>\n",
       "      <td>0.515442</td>\n",
       "      <td>0.588517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.324092</td>\n",
       "      <td>0.611970</td>\n",
       "      <td>0.564221</td>\n",
       "      <td>0.395722</td>\n",
       "      <td>0.576489</td>\n",
       "      <td>0.418754</td>\n",
       "      <td>0.555493</td>\n",
       "      <td>0.553192</td>\n",
       "      <td>0.306077</td>\n",
       "      <td>0.758185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.542063</td>\n",
       "      <td>0.491148</td>\n",
       "      <td>0.474404</td>\n",
       "      <td>0.778383</td>\n",
       "      <td>0.565977</td>\n",
       "      <td>0.455756</td>\n",
       "      <td>0.448773</td>\n",
       "      <td>0.587150</td>\n",
       "      <td>0.464794</td>\n",
       "      <td>0.532583</td>\n",
       "      <td>...</td>\n",
       "      <td>0.304210</td>\n",
       "      <td>0.546421</td>\n",
       "      <td>0.504384</td>\n",
       "      <td>0.371506</td>\n",
       "      <td>0.526000</td>\n",
       "      <td>0.393380</td>\n",
       "      <td>0.500391</td>\n",
       "      <td>0.516223</td>\n",
       "      <td>0.268440</td>\n",
       "      <td>0.685014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.612803</td>\n",
       "      <td>0.542049</td>\n",
       "      <td>0.538277</td>\n",
       "      <td>0.863676</td>\n",
       "      <td>0.617977</td>\n",
       "      <td>0.516837</td>\n",
       "      <td>0.493825</td>\n",
       "      <td>0.663906</td>\n",
       "      <td>0.515714</td>\n",
       "      <td>0.585977</td>\n",
       "      <td>...</td>\n",
       "      <td>0.322733</td>\n",
       "      <td>0.610216</td>\n",
       "      <td>0.563450</td>\n",
       "      <td>0.395127</td>\n",
       "      <td>0.574846</td>\n",
       "      <td>0.417876</td>\n",
       "      <td>0.555976</td>\n",
       "      <td>0.552008</td>\n",
       "      <td>0.305774</td>\n",
       "      <td>0.755695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.612086</td>\n",
       "      <td>0.541632</td>\n",
       "      <td>0.537151</td>\n",
       "      <td>0.862109</td>\n",
       "      <td>0.619332</td>\n",
       "      <td>0.515952</td>\n",
       "      <td>0.495234</td>\n",
       "      <td>0.662659</td>\n",
       "      <td>0.513095</td>\n",
       "      <td>0.587559</td>\n",
       "      <td>...</td>\n",
       "      <td>0.324014</td>\n",
       "      <td>0.610145</td>\n",
       "      <td>0.562078</td>\n",
       "      <td>0.395016</td>\n",
       "      <td>0.575250</td>\n",
       "      <td>0.418160</td>\n",
       "      <td>0.552832</td>\n",
       "      <td>0.552273</td>\n",
       "      <td>0.304625</td>\n",
       "      <td>0.756453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.612165</td>\n",
       "      <td>0.541656</td>\n",
       "      <td>0.537383</td>\n",
       "      <td>0.862446</td>\n",
       "      <td>0.618751</td>\n",
       "      <td>0.516109</td>\n",
       "      <td>0.494650</td>\n",
       "      <td>0.662904</td>\n",
       "      <td>0.513870</td>\n",
       "      <td>0.586895</td>\n",
       "      <td>...</td>\n",
       "      <td>0.323536</td>\n",
       "      <td>0.610020</td>\n",
       "      <td>0.562407</td>\n",
       "      <td>0.395002</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.418009</td>\n",
       "      <td>0.553776</td>\n",
       "      <td>0.552103</td>\n",
       "      <td>0.304928</td>\n",
       "      <td>0.756029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.601851</td>\n",
       "      <td>0.534095</td>\n",
       "      <td>0.528760</td>\n",
       "      <td>0.851109</td>\n",
       "      <td>0.608396</td>\n",
       "      <td>0.507572</td>\n",
       "      <td>0.485313</td>\n",
       "      <td>0.652404</td>\n",
       "      <td>0.509639</td>\n",
       "      <td>0.575927</td>\n",
       "      <td>...</td>\n",
       "      <td>0.318670</td>\n",
       "      <td>0.599866</td>\n",
       "      <td>0.554949</td>\n",
       "      <td>0.391415</td>\n",
       "      <td>0.566559</td>\n",
       "      <td>0.413690</td>\n",
       "      <td>0.549577</td>\n",
       "      <td>0.546022</td>\n",
       "      <td>0.300618</td>\n",
       "      <td>0.743526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.581411</td>\n",
       "      <td>0.519715</td>\n",
       "      <td>0.508611</td>\n",
       "      <td>0.823400</td>\n",
       "      <td>0.600433</td>\n",
       "      <td>0.489077</td>\n",
       "      <td>0.479425</td>\n",
       "      <td>0.628459</td>\n",
       "      <td>0.486408</td>\n",
       "      <td>0.568824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.318832</td>\n",
       "      <td>0.583619</td>\n",
       "      <td>0.534885</td>\n",
       "      <td>0.384758</td>\n",
       "      <td>0.555779</td>\n",
       "      <td>0.408363</td>\n",
       "      <td>0.523147</td>\n",
       "      <td>0.537629</td>\n",
       "      <td>0.286950</td>\n",
       "      <td>0.728806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.604388</td>\n",
       "      <td>0.535952</td>\n",
       "      <td>0.530880</td>\n",
       "      <td>0.853836</td>\n",
       "      <td>0.610990</td>\n",
       "      <td>0.509680</td>\n",
       "      <td>0.487669</td>\n",
       "      <td>0.654974</td>\n",
       "      <td>0.510563</td>\n",
       "      <td>0.578714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.319904</td>\n",
       "      <td>0.602381</td>\n",
       "      <td>0.556754</td>\n",
       "      <td>0.392268</td>\n",
       "      <td>0.568653</td>\n",
       "      <td>0.414740</td>\n",
       "      <td>0.550475</td>\n",
       "      <td>0.547492</td>\n",
       "      <td>0.301660</td>\n",
       "      <td>0.746655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.595592</td>\n",
       "      <td>0.529543</td>\n",
       "      <td>0.523250</td>\n",
       "      <td>0.843295</td>\n",
       "      <td>0.603571</td>\n",
       "      <td>0.502322</td>\n",
       "      <td>0.481213</td>\n",
       "      <td>0.645659</td>\n",
       "      <td>0.504869</td>\n",
       "      <td>0.571243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.316858</td>\n",
       "      <td>0.594173</td>\n",
       "      <td>0.549737</td>\n",
       "      <td>0.389053</td>\n",
       "      <td>0.562098</td>\n",
       "      <td>0.411256</td>\n",
       "      <td>0.544402</td>\n",
       "      <td>0.542447</td>\n",
       "      <td>0.297413</td>\n",
       "      <td>0.737237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.572010</td>\n",
       "      <td>0.512562</td>\n",
       "      <td>0.502146</td>\n",
       "      <td>0.815807</td>\n",
       "      <td>0.585032</td>\n",
       "      <td>0.481957</td>\n",
       "      <td>0.464849</td>\n",
       "      <td>0.620385</td>\n",
       "      <td>0.489966</td>\n",
       "      <td>0.551673</td>\n",
       "      <td>...</td>\n",
       "      <td>0.309741</td>\n",
       "      <td>0.572511</td>\n",
       "      <td>0.530653</td>\n",
       "      <td>0.381458</td>\n",
       "      <td>0.545284</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>0.528318</td>\n",
       "      <td>0.530586</td>\n",
       "      <td>0.285451</td>\n",
       "      <td>0.712534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.616767</td>\n",
       "      <td>0.544966</td>\n",
       "      <td>0.541546</td>\n",
       "      <td>0.867989</td>\n",
       "      <td>0.622120</td>\n",
       "      <td>0.520089</td>\n",
       "      <td>0.497571</td>\n",
       "      <td>0.667901</td>\n",
       "      <td>0.517183</td>\n",
       "      <td>0.590362</td>\n",
       "      <td>...</td>\n",
       "      <td>0.324730</td>\n",
       "      <td>0.614167</td>\n",
       "      <td>0.566253</td>\n",
       "      <td>0.396529</td>\n",
       "      <td>0.578170</td>\n",
       "      <td>0.419592</td>\n",
       "      <td>0.557393</td>\n",
       "      <td>0.554416</td>\n",
       "      <td>0.307363</td>\n",
       "      <td>0.760622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.598456</td>\n",
       "      <td>0.531808</td>\n",
       "      <td>0.524906</td>\n",
       "      <td>0.845665</td>\n",
       "      <td>0.609137</td>\n",
       "      <td>0.504234</td>\n",
       "      <td>0.486402</td>\n",
       "      <td>0.647912</td>\n",
       "      <td>0.503370</td>\n",
       "      <td>0.577135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.320308</td>\n",
       "      <td>0.597804</td>\n",
       "      <td>0.550753</td>\n",
       "      <td>0.390398</td>\n",
       "      <td>0.565748</td>\n",
       "      <td>0.413347</td>\n",
       "      <td>0.542243</td>\n",
       "      <td>0.545248</td>\n",
       "      <td>0.297505</td>\n",
       "      <td>0.742726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.607793</td>\n",
       "      <td>0.538392</td>\n",
       "      <td>0.533994</td>\n",
       "      <td>0.857945</td>\n",
       "      <td>0.613377</td>\n",
       "      <td>0.512651</td>\n",
       "      <td>0.489736</td>\n",
       "      <td>0.658696</td>\n",
       "      <td>0.513085</td>\n",
       "      <td>0.581187</td>\n",
       "      <td>...</td>\n",
       "      <td>0.320705</td>\n",
       "      <td>0.605418</td>\n",
       "      <td>0.559636</td>\n",
       "      <td>0.393368</td>\n",
       "      <td>0.570946</td>\n",
       "      <td>0.415863</td>\n",
       "      <td>0.553245</td>\n",
       "      <td>0.549137</td>\n",
       "      <td>0.303505</td>\n",
       "      <td>0.749983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.308663</td>\n",
       "      <td>0.321381</td>\n",
       "      <td>0.272813</td>\n",
       "      <td>0.512251</td>\n",
       "      <td>0.357136</td>\n",
       "      <td>0.259002</td>\n",
       "      <td>0.262778</td>\n",
       "      <td>0.343205</td>\n",
       "      <td>0.340436</td>\n",
       "      <td>0.313225</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213999</td>\n",
       "      <td>0.324453</td>\n",
       "      <td>0.325126</td>\n",
       "      <td>0.292030</td>\n",
       "      <td>0.347154</td>\n",
       "      <td>0.302801</td>\n",
       "      <td>0.370288</td>\n",
       "      <td>0.387027</td>\n",
       "      <td>0.160480</td>\n",
       "      <td>0.422068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.601246</td>\n",
       "      <td>0.533758</td>\n",
       "      <td>0.527710</td>\n",
       "      <td>0.849491</td>\n",
       "      <td>0.610038</td>\n",
       "      <td>0.506794</td>\n",
       "      <td>0.487029</td>\n",
       "      <td>0.651226</td>\n",
       "      <td>0.506711</td>\n",
       "      <td>0.577917</td>\n",
       "      <td>...</td>\n",
       "      <td>0.320141</td>\n",
       "      <td>0.599966</td>\n",
       "      <td>0.553562</td>\n",
       "      <td>0.391275</td>\n",
       "      <td>0.567128</td>\n",
       "      <td>0.414007</td>\n",
       "      <td>0.546065</td>\n",
       "      <td>0.546308</td>\n",
       "      <td>0.299447</td>\n",
       "      <td>0.744600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.599368</td>\n",
       "      <td>0.532500</td>\n",
       "      <td>0.525546</td>\n",
       "      <td>0.846437</td>\n",
       "      <td>0.610571</td>\n",
       "      <td>0.504929</td>\n",
       "      <td>0.487752</td>\n",
       "      <td>0.648711</td>\n",
       "      <td>0.503110</td>\n",
       "      <td>0.578721</td>\n",
       "      <td>...</td>\n",
       "      <td>0.321143</td>\n",
       "      <td>0.598862</td>\n",
       "      <td>0.551191</td>\n",
       "      <td>0.390723</td>\n",
       "      <td>0.566738</td>\n",
       "      <td>0.413854</td>\n",
       "      <td>0.541844</td>\n",
       "      <td>0.545923</td>\n",
       "      <td>0.297675</td>\n",
       "      <td>0.744252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.616347</td>\n",
       "      <td>0.544691</td>\n",
       "      <td>0.541042</td>\n",
       "      <td>0.867364</td>\n",
       "      <td>0.622254</td>\n",
       "      <td>0.519647</td>\n",
       "      <td>0.497727</td>\n",
       "      <td>0.667336</td>\n",
       "      <td>0.516455</td>\n",
       "      <td>0.590504</td>\n",
       "      <td>...</td>\n",
       "      <td>0.324965</td>\n",
       "      <td>0.613921</td>\n",
       "      <td>0.565731</td>\n",
       "      <td>0.396454</td>\n",
       "      <td>0.578096</td>\n",
       "      <td>0.419599</td>\n",
       "      <td>0.556533</td>\n",
       "      <td>0.554396</td>\n",
       "      <td>0.306959</td>\n",
       "      <td>0.760529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.604674</td>\n",
       "      <td>0.536172</td>\n",
       "      <td>0.531065</td>\n",
       "      <td>0.854016</td>\n",
       "      <td>0.611531</td>\n",
       "      <td>0.509896</td>\n",
       "      <td>0.488192</td>\n",
       "      <td>0.655201</td>\n",
       "      <td>0.510338</td>\n",
       "      <td>0.579338</td>\n",
       "      <td>...</td>\n",
       "      <td>0.320237</td>\n",
       "      <td>0.602743</td>\n",
       "      <td>0.556849</td>\n",
       "      <td>0.392356</td>\n",
       "      <td>0.569005</td>\n",
       "      <td>0.414908</td>\n",
       "      <td>0.550179</td>\n",
       "      <td>0.547707</td>\n",
       "      <td>0.301677</td>\n",
       "      <td>0.747217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.321815</td>\n",
       "      <td>0.331051</td>\n",
       "      <td>0.283076</td>\n",
       "      <td>0.522603</td>\n",
       "      <td>0.375315</td>\n",
       "      <td>0.269969</td>\n",
       "      <td>0.280300</td>\n",
       "      <td>0.355296</td>\n",
       "      <td>0.336986</td>\n",
       "      <td>0.334691</td>\n",
       "      <td>...</td>\n",
       "      <td>0.224096</td>\n",
       "      <td>0.339045</td>\n",
       "      <td>0.332089</td>\n",
       "      <td>0.295303</td>\n",
       "      <td>0.360089</td>\n",
       "      <td>0.308426</td>\n",
       "      <td>0.365224</td>\n",
       "      <td>0.394309</td>\n",
       "      <td>0.163980</td>\n",
       "      <td>0.442801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.586754</td>\n",
       "      <td>0.523509</td>\n",
       "      <td>0.513773</td>\n",
       "      <td>0.830828</td>\n",
       "      <td>0.602691</td>\n",
       "      <td>0.493799</td>\n",
       "      <td>0.481062</td>\n",
       "      <td>0.634680</td>\n",
       "      <td>0.492650</td>\n",
       "      <td>0.570671</td>\n",
       "      <td>...</td>\n",
       "      <td>0.318926</td>\n",
       "      <td>0.587906</td>\n",
       "      <td>0.540113</td>\n",
       "      <td>0.386694</td>\n",
       "      <td>0.558704</td>\n",
       "      <td>0.409949</td>\n",
       "      <td>0.530215</td>\n",
       "      <td>0.540120</td>\n",
       "      <td>0.290446</td>\n",
       "      <td>0.732692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.594601</td>\n",
       "      <td>0.528944</td>\n",
       "      <td>0.521833</td>\n",
       "      <td>0.841499</td>\n",
       "      <td>0.604779</td>\n",
       "      <td>0.501150</td>\n",
       "      <td>0.482464</td>\n",
       "      <td>0.644105</td>\n",
       "      <td>0.502170</td>\n",
       "      <td>0.572579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.318110</td>\n",
       "      <td>0.593865</td>\n",
       "      <td>0.548140</td>\n",
       "      <td>0.388944</td>\n",
       "      <td>0.562350</td>\n",
       "      <td>0.411532</td>\n",
       "      <td>0.541164</td>\n",
       "      <td>0.542708</td>\n",
       "      <td>0.296095</td>\n",
       "      <td>0.737715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.600727</td>\n",
       "      <td>0.533351</td>\n",
       "      <td>0.527388</td>\n",
       "      <td>0.849003</td>\n",
       "      <td>0.609132</td>\n",
       "      <td>0.506439</td>\n",
       "      <td>0.486197</td>\n",
       "      <td>0.650791</td>\n",
       "      <td>0.506839</td>\n",
       "      <td>0.576981</td>\n",
       "      <td>...</td>\n",
       "      <td>0.319596</td>\n",
       "      <td>0.599340</td>\n",
       "      <td>0.553331</td>\n",
       "      <td>0.391025</td>\n",
       "      <td>0.566514</td>\n",
       "      <td>0.413646</td>\n",
       "      <td>0.546283</td>\n",
       "      <td>0.545816</td>\n",
       "      <td>0.299388</td>\n",
       "      <td>0.743693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.610390</td>\n",
       "      <td>0.540304</td>\n",
       "      <td>0.536163</td>\n",
       "      <td>0.860979</td>\n",
       "      <td>0.615865</td>\n",
       "      <td>0.514770</td>\n",
       "      <td>0.491927</td>\n",
       "      <td>0.661373</td>\n",
       "      <td>0.514481</td>\n",
       "      <td>0.583702</td>\n",
       "      <td>...</td>\n",
       "      <td>0.321837</td>\n",
       "      <td>0.607931</td>\n",
       "      <td>0.561593</td>\n",
       "      <td>0.394362</td>\n",
       "      <td>0.573027</td>\n",
       "      <td>0.416995</td>\n",
       "      <td>0.554682</td>\n",
       "      <td>0.550757</td>\n",
       "      <td>0.304637</td>\n",
       "      <td>0.752984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.606146</td>\n",
       "      <td>0.537267</td>\n",
       "      <td>0.532114</td>\n",
       "      <td>0.854849</td>\n",
       "      <td>0.614079</td>\n",
       "      <td>0.511096</td>\n",
       "      <td>0.490703</td>\n",
       "      <td>0.656422</td>\n",
       "      <td>0.509183</td>\n",
       "      <td>0.582431</td>\n",
       "      <td>...</td>\n",
       "      <td>0.321769</td>\n",
       "      <td>0.604540</td>\n",
       "      <td>0.557386</td>\n",
       "      <td>0.392660</td>\n",
       "      <td>0.570686</td>\n",
       "      <td>0.415607</td>\n",
       "      <td>0.548695</td>\n",
       "      <td>0.548567</td>\n",
       "      <td>0.301860</td>\n",
       "      <td>0.749993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.558011</td>\n",
       "      <td>0.503126</td>\n",
       "      <td>0.486218</td>\n",
       "      <td>0.793022</td>\n",
       "      <td>0.588413</td>\n",
       "      <td>0.468221</td>\n",
       "      <td>0.469726</td>\n",
       "      <td>0.601774</td>\n",
       "      <td>0.463439</td>\n",
       "      <td>0.557197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.316749</td>\n",
       "      <td>0.564115</td>\n",
       "      <td>0.513165</td>\n",
       "      <td>0.377130</td>\n",
       "      <td>0.542075</td>\n",
       "      <td>0.401593</td>\n",
       "      <td>0.497289</td>\n",
       "      <td>0.527302</td>\n",
       "      <td>0.272485</td>\n",
       "      <td>0.709574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.601471</td>\n",
       "      <td>0.533929</td>\n",
       "      <td>0.527833</td>\n",
       "      <td>0.849442</td>\n",
       "      <td>0.610661</td>\n",
       "      <td>0.506976</td>\n",
       "      <td>0.487672</td>\n",
       "      <td>0.651352</td>\n",
       "      <td>0.506142</td>\n",
       "      <td>0.578728</td>\n",
       "      <td>...</td>\n",
       "      <td>0.320558</td>\n",
       "      <td>0.600317</td>\n",
       "      <td>0.553529</td>\n",
       "      <td>0.391272</td>\n",
       "      <td>0.567488</td>\n",
       "      <td>0.414128</td>\n",
       "      <td>0.545374</td>\n",
       "      <td>0.546431</td>\n",
       "      <td>0.299382</td>\n",
       "      <td>0.745243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.610553</td>\n",
       "      <td>0.540443</td>\n",
       "      <td>0.536072</td>\n",
       "      <td>0.860127</td>\n",
       "      <td>0.617409</td>\n",
       "      <td>0.514891</td>\n",
       "      <td>0.493600</td>\n",
       "      <td>0.661183</td>\n",
       "      <td>0.512250</td>\n",
       "      <td>0.585862</td>\n",
       "      <td>...</td>\n",
       "      <td>0.322993</td>\n",
       "      <td>0.608542</td>\n",
       "      <td>0.561029</td>\n",
       "      <td>0.394135</td>\n",
       "      <td>0.573772</td>\n",
       "      <td>0.417156</td>\n",
       "      <td>0.552029</td>\n",
       "      <td>0.550823</td>\n",
       "      <td>0.304150</td>\n",
       "      <td>0.754468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.592012</td>\n",
       "      <td>0.527399</td>\n",
       "      <td>0.518007</td>\n",
       "      <td>0.836415</td>\n",
       "      <td>0.608577</td>\n",
       "      <td>0.498054</td>\n",
       "      <td>0.486413</td>\n",
       "      <td>0.639884</td>\n",
       "      <td>0.494180</td>\n",
       "      <td>0.576918</td>\n",
       "      <td>...</td>\n",
       "      <td>0.321880</td>\n",
       "      <td>0.593267</td>\n",
       "      <td>0.543673</td>\n",
       "      <td>0.388588</td>\n",
       "      <td>0.563301</td>\n",
       "      <td>0.412342</td>\n",
       "      <td>0.531579</td>\n",
       "      <td>0.543455</td>\n",
       "      <td>0.292395</td>\n",
       "      <td>0.739528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.606492</td>\n",
       "      <td>0.537581</td>\n",
       "      <td>0.532197</td>\n",
       "      <td>0.855369</td>\n",
       "      <td>0.614932</td>\n",
       "      <td>0.511198</td>\n",
       "      <td>0.491417</td>\n",
       "      <td>0.656659</td>\n",
       "      <td>0.509240</td>\n",
       "      <td>0.583097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.322325</td>\n",
       "      <td>0.605018</td>\n",
       "      <td>0.557503</td>\n",
       "      <td>0.393054</td>\n",
       "      <td>0.571241</td>\n",
       "      <td>0.416083</td>\n",
       "      <td>0.548668</td>\n",
       "      <td>0.549250</td>\n",
       "      <td>0.301793</td>\n",
       "      <td>0.750678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40 rows × 650 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6    \\\n",
       "0   0.602029  0.534340  0.528308  0.850182  0.611104  0.507425  0.488035   \n",
       "1   0.606245  0.537300  0.532513  0.856057  0.612496  0.511252  0.488982   \n",
       "2   0.603670  0.535562  0.529603  0.851949  0.613004  0.508727  0.489753   \n",
       "3   0.613798  0.542850  0.538759  0.864232  0.620366  0.517471  0.496109   \n",
       "4   0.616340  0.544645  0.541208  0.867482  0.621658  0.519757  0.497166   \n",
       "5   0.605106  0.536609  0.530847  0.853677  0.614213  0.509927  0.490803   \n",
       "6   0.595403  0.529624  0.522109  0.842056  0.606960  0.501555  0.484495   \n",
       "7   0.612889  0.542129  0.538270  0.863666  0.618366  0.516862  0.494200   \n",
       "8   0.616788  0.544996  0.541504  0.867984  0.622333  0.520065  0.497766   \n",
       "9   0.597031  0.530922  0.522852  0.842331  0.611448  0.502657  0.488933   \n",
       "10  0.604155  0.535842  0.530413  0.853262  0.611774  0.509323  0.488449   \n",
       "11  0.614332  0.543214  0.539344  0.865061  0.620330  0.517983  0.496017   \n",
       "12  0.542063  0.491148  0.474404  0.778383  0.565977  0.455756  0.448773   \n",
       "13  0.612803  0.542049  0.538277  0.863676  0.617977  0.516837  0.493825   \n",
       "14  0.612086  0.541632  0.537151  0.862109  0.619332  0.515952  0.495234   \n",
       "15  0.612165  0.541656  0.537383  0.862446  0.618751  0.516109  0.494650   \n",
       "16  0.601851  0.534095  0.528760  0.851109  0.608396  0.507572  0.485313   \n",
       "17  0.581411  0.519715  0.508611  0.823400  0.600433  0.489077  0.479425   \n",
       "18  0.604388  0.535952  0.530880  0.853836  0.610990  0.509680  0.487669   \n",
       "19  0.595592  0.529543  0.523250  0.843295  0.603571  0.502322  0.481213   \n",
       "20  0.572010  0.512562  0.502146  0.815807  0.585032  0.481957  0.464849   \n",
       "21  0.616767  0.544966  0.541546  0.867989  0.622120  0.520089  0.497571   \n",
       "22  0.598456  0.531808  0.524906  0.845665  0.609137  0.504234  0.486402   \n",
       "23  0.607793  0.538392  0.533994  0.857945  0.613377  0.512651  0.489736   \n",
       "24  0.308663  0.321381  0.272813  0.512251  0.357136  0.259002  0.262778   \n",
       "25  0.601246  0.533758  0.527710  0.849491  0.610038  0.506794  0.487029   \n",
       "26  0.599368  0.532500  0.525546  0.846437  0.610571  0.504929  0.487752   \n",
       "27  0.616347  0.544691  0.541042  0.867364  0.622254  0.519647  0.497727   \n",
       "28  0.604674  0.536172  0.531065  0.854016  0.611531  0.509896  0.488192   \n",
       "29  0.321815  0.331051  0.283076  0.522603  0.375315  0.269969  0.280300   \n",
       "30  0.586754  0.523509  0.513773  0.830828  0.602691  0.493799  0.481062   \n",
       "31  0.594601  0.528944  0.521833  0.841499  0.604779  0.501150  0.482464   \n",
       "32  0.600727  0.533351  0.527388  0.849003  0.609132  0.506439  0.486197   \n",
       "33  0.610390  0.540304  0.536163  0.860979  0.615865  0.514770  0.491927   \n",
       "34  0.606146  0.537267  0.532114  0.854849  0.614079  0.511096  0.490703   \n",
       "35  0.558011  0.503126  0.486218  0.793022  0.588413  0.468221  0.469726   \n",
       "36  0.601471  0.533929  0.527833  0.849442  0.610661  0.506976  0.487672   \n",
       "37  0.610553  0.540443  0.536072  0.860127  0.617409  0.514891  0.493600   \n",
       "38  0.592012  0.527399  0.518007  0.836415  0.608577  0.498054  0.486413   \n",
       "39  0.606492  0.537581  0.532197  0.855369  0.614932  0.511198  0.491417   \n",
       "\n",
       "         7         8         9    ...       640       641       642       643  \\\n",
       "0   0.651951  0.506625  0.579119  ...  0.320729  0.600826  0.553998  0.391519   \n",
       "1   0.656954  0.511792  0.580247  ...  0.320500  0.604097  0.558255  0.392923   \n",
       "2   0.653561  0.507100  0.581100  ...  0.321700  0.602515  0.555094  0.392149   \n",
       "3   0.664572  0.514541  0.588619  ...  0.324287  0.611622  0.563594  0.395554   \n",
       "4   0.667472  0.516972  0.589912  ...  0.324503  0.613739  0.565947  0.396343   \n",
       "5   0.655082  0.508041  0.582313  ...  0.322196  0.603854  0.556242  0.392678   \n",
       "6   0.644585  0.501239  0.574826  ...  0.319561  0.595066  0.548197  0.389454   \n",
       "7   0.663918  0.515424  0.586403  ...  0.323009  0.610393  0.563390  0.395184   \n",
       "8   0.667876  0.517040  0.590565  ...  0.324889  0.614244  0.566199  0.396581   \n",
       "9   0.645521  0.498061  0.580150  ...  0.322557  0.597566  0.548102  0.389886   \n",
       "10  0.654486  0.509420  0.579564  ...  0.320588  0.602460  0.556182  0.392301   \n",
       "11  0.665258  0.515442  0.588517  ...  0.324092  0.611970  0.564221  0.395722   \n",
       "12  0.587150  0.464794  0.532583  ...  0.304210  0.546421  0.504384  0.371506   \n",
       "13  0.663906  0.515714  0.585977  ...  0.322733  0.610216  0.563450  0.395127   \n",
       "14  0.662659  0.513095  0.587559  ...  0.324014  0.610145  0.562078  0.395016   \n",
       "15  0.662904  0.513870  0.586895  ...  0.323536  0.610020  0.562407  0.395002   \n",
       "16  0.652404  0.509639  0.575927  ...  0.318670  0.599866  0.554949  0.391415   \n",
       "17  0.628459  0.486408  0.568824  ...  0.318832  0.583619  0.534885  0.384758   \n",
       "18  0.654974  0.510563  0.578714  ...  0.319904  0.602381  0.556754  0.392268   \n",
       "19  0.645659  0.504869  0.571243  ...  0.316858  0.594173  0.549737  0.389053   \n",
       "20  0.620385  0.489966  0.551673  ...  0.309741  0.572511  0.530653  0.381458   \n",
       "21  0.667901  0.517183  0.590362  ...  0.324730  0.614167  0.566253  0.396529   \n",
       "22  0.647912  0.503370  0.577135  ...  0.320308  0.597804  0.550753  0.390398   \n",
       "23  0.658696  0.513085  0.581187  ...  0.320705  0.605418  0.559636  0.393368   \n",
       "24  0.343205  0.340436  0.313225  ...  0.213999  0.324453  0.325126  0.292030   \n",
       "25  0.651226  0.506711  0.577917  ...  0.320141  0.599966  0.553562  0.391275   \n",
       "26  0.648711  0.503110  0.578721  ...  0.321143  0.598862  0.551191  0.390723   \n",
       "27  0.667336  0.516455  0.590504  ...  0.324965  0.613921  0.565731  0.396454   \n",
       "28  0.655201  0.510338  0.579338  ...  0.320237  0.602743  0.556849  0.392356   \n",
       "29  0.355296  0.336986  0.334691  ...  0.224096  0.339045  0.332089  0.295303   \n",
       "30  0.634680  0.492650  0.570671  ...  0.318926  0.587906  0.540113  0.386694   \n",
       "31  0.644105  0.502170  0.572579  ...  0.318110  0.593865  0.548140  0.388944   \n",
       "32  0.650791  0.506839  0.576981  ...  0.319596  0.599340  0.553331  0.391025   \n",
       "33  0.661373  0.514481  0.583702  ...  0.321837  0.607931  0.561593  0.394362   \n",
       "34  0.656422  0.509183  0.582431  ...  0.321769  0.604540  0.557386  0.392660   \n",
       "35  0.601774  0.463439  0.557197  ...  0.316749  0.564115  0.513165  0.377130   \n",
       "36  0.651352  0.506142  0.578728  ...  0.320558  0.600317  0.553529  0.391272   \n",
       "37  0.661183  0.512250  0.585862  ...  0.322993  0.608542  0.561029  0.394135   \n",
       "38  0.639884  0.494180  0.576918  ...  0.321880  0.593267  0.543673  0.388588   \n",
       "39  0.656659  0.509240  0.583097  ...  0.322325  0.605018  0.557503  0.393054   \n",
       "\n",
       "         644       645       646       647       648       649  \n",
       "0   0.567897  0.414377  0.545895  0.546801  0.299661  0.745794  \n",
       "1   0.570007  0.415443  0.551795  0.548511  0.302581  0.748608  \n",
       "2   0.569365  0.415168  0.546309  0.547907  0.300242  0.747958  \n",
       "3   0.576323  0.418675  0.554445  0.553041  0.305621  0.757997  \n",
       "4   0.577800  0.419376  0.557186  0.554107  0.307199  0.760097  \n",
       "5   0.570435  0.415740  0.547314  0.548736  0.300937  0.749492  \n",
       "6   0.563681  0.412365  0.539907  0.543819  0.295864  0.739690  \n",
       "7   0.575062  0.418003  0.555610  0.552168  0.305686  0.756033  \n",
       "8   0.578283  0.419680  0.557209  0.554533  0.307293  0.760780  \n",
       "9   0.566335  0.413595  0.535938  0.545302  0.295386  0.744077  \n",
       "10  0.568955  0.414965  0.549083  0.547751  0.301145  0.747146  \n",
       "11  0.576489  0.418754  0.555493  0.553192  0.306077  0.758185  \n",
       "12  0.526000  0.393380  0.500391  0.516223  0.268440  0.685014  \n",
       "13  0.574846  0.417876  0.555976  0.552008  0.305774  0.755695  \n",
       "14  0.575250  0.418160  0.552832  0.552273  0.304625  0.756453  \n",
       "15  0.575000  0.418009  0.553776  0.552103  0.304928  0.756029  \n",
       "16  0.566559  0.413690  0.549577  0.546022  0.300618  0.743526  \n",
       "17  0.555779  0.408363  0.523147  0.537629  0.286950  0.728806  \n",
       "18  0.568653  0.414740  0.550475  0.547492  0.301660  0.746655  \n",
       "19  0.562098  0.411256  0.544402  0.542447  0.297413  0.737237  \n",
       "20  0.545284  0.403034  0.528318  0.530586  0.285451  0.712534  \n",
       "21  0.578170  0.419592  0.557393  0.554416  0.307363  0.760622  \n",
       "22  0.565748  0.413347  0.542243  0.545248  0.297505  0.742726  \n",
       "23  0.570946  0.415863  0.553245  0.549137  0.303505  0.749983  \n",
       "24  0.347154  0.302801  0.370288  0.387027  0.160480  0.422068  \n",
       "25  0.567128  0.414007  0.546065  0.546308  0.299447  0.744600  \n",
       "26  0.566738  0.413854  0.541844  0.545923  0.297675  0.744252  \n",
       "27  0.578096  0.419599  0.556533  0.554396  0.306959  0.760529  \n",
       "28  0.569005  0.414908  0.550179  0.547707  0.301677  0.747217  \n",
       "29  0.360089  0.308426  0.365224  0.394309  0.163980  0.442801  \n",
       "30  0.558704  0.409949  0.530215  0.540120  0.290446  0.732692  \n",
       "31  0.562350  0.411532  0.541164  0.542708  0.296095  0.737715  \n",
       "32  0.566514  0.413646  0.546283  0.545816  0.299388  0.743693  \n",
       "33  0.573027  0.416995  0.554682  0.550757  0.304637  0.752984  \n",
       "34  0.570686  0.415607  0.548695  0.548567  0.301860  0.749993  \n",
       "35  0.542075  0.401593  0.497289  0.527302  0.272485  0.709574  \n",
       "36  0.567488  0.414128  0.545374  0.546431  0.299382  0.745243  \n",
       "37  0.573772  0.417156  0.552029  0.550823  0.304150  0.754468  \n",
       "38  0.563301  0.412342  0.531579  0.543455  0.292395  0.739528  \n",
       "39  0.571241  0.416083  0.548668  0.549250  0.301793  0.750678  \n",
       "\n",
       "[40 rows x 650 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(decoded_stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    ax = plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history[\"loss\"])\n",
    "    plt.title(\"Train loss\")\n",
    "    ax = plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history[\"val_loss\"])\n",
    "    plt.title(\"Test loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loss vs Epoch\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D convolutional autoencoder\n",
    "(Kernel size = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 650, 1)]          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 650, 16)           64        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 325, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 325, 1)            49        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 163, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 163, 1)            4         \n",
      "_________________________________________________________________\n",
      "up_sampling1d_2 (UpSampling1 (None, 326, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 325, 16)           48        \n",
      "_________________________________________________________________\n",
      "up_sampling1d_3 (UpSampling1 (None, 650, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 650, 1)            49        \n",
      "=================================================================\n",
      "Total params: 214\n",
      "Trainable params: 214\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "1/1 [==============================] - 26s 26s/step - loss: 0.3086 - val_loss: 0.2953\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.2973 - val_loss: 0.2847\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.2868 - val_loss: 0.2737\n",
      "Epoch 4/500\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 0.2758 - val_loss: 0.2618\n",
      "Epoch 5/500\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 0.2641 - val_loss: 0.2493\n",
      "Epoch 6/500\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.2515 - val_loss: 0.2360\n",
      "Epoch 7/500\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 0.2383 - val_loss: 0.2221\n",
      "Epoch 8/500\n",
      "1/1 [==============================] - 0s 240ms/step - loss: 0.2245 - val_loss: 0.2077\n",
      "Epoch 9/500\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 0.2102 - val_loss: 0.1929\n",
      "Epoch 10/500\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 0.1955 - val_loss: 0.1779\n",
      "Epoch 11/500\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 0.1806 - val_loss: 0.1628\n",
      "Epoch 12/500\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.1656 - val_loss: 0.1477\n",
      "Epoch 13/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 0.1507 - val_loss: 0.1330\n",
      "Epoch 14/500\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 0.1360 - val_loss: 0.1186\n",
      "Epoch 15/500\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1218 - val_loss: 0.1048\n",
      "Epoch 16/500\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 0.1081 - val_loss: 0.0918\n",
      "Epoch 17/500\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.0953 - val_loss: 0.0798\n",
      "Epoch 18/500\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.0833 - val_loss: 0.0688\n",
      "Epoch 19/500\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.0724 - val_loss: 0.0590\n",
      "Epoch 20/500\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 0.0627 - val_loss: 0.0505\n",
      "Epoch 21/500\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0543 - val_loss: 0.0433\n",
      "Epoch 22/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 0.0473 - val_loss: 0.0375\n",
      "Epoch 23/500\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 0.0416 - val_loss: 0.0331\n",
      "Epoch 24/500\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 0.0373 - val_loss: 0.0301\n",
      "Epoch 25/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 0.0343 - val_loss: 0.0282\n",
      "Epoch 26/500\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.0325 - val_loss: 0.0274\n",
      "Epoch 27/500\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 0.0317 - val_loss: 0.0276\n",
      "Epoch 28/500\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.0319 - val_loss: 0.0284\n",
      "Epoch 29/500\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 0.0327 - val_loss: 0.0296\n",
      "Epoch 30/500\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.0340 - val_loss: 0.0311\n",
      "Epoch 31/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.0355 - val_loss: 0.0326\n",
      "Epoch 32/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 0.0370 - val_loss: 0.0340\n",
      "Epoch 33/500\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 0.0384 - val_loss: 0.0350\n",
      "Epoch 34/500\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 0.0394 - val_loss: 0.0357\n",
      "Epoch 35/500\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 0.0401 - val_loss: 0.0360\n",
      "Epoch 36/500\n",
      "1/1 [==============================] - 0s 240ms/step - loss: 0.0404 - val_loss: 0.0358\n",
      "Epoch 37/500\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.0402 - val_loss: 0.0354\n",
      "Epoch 38/500\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 0.0398 - val_loss: 0.0347\n",
      "Epoch 39/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0391 - val_loss: 0.0337\n",
      "Epoch 40/500\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 0.0381 - val_loss: 0.0327\n",
      "Epoch 41/500\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 0.0371 - val_loss: 0.0317\n",
      "Epoch 42/500\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.0361 - val_loss: 0.0307\n",
      "Epoch 43/500\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 0.0351 - val_loss: 0.0298\n",
      "Epoch 44/500\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.0342 - val_loss: 0.0290\n",
      "Epoch 45/500\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 0.0334 - val_loss: 0.0284\n",
      "Epoch 46/500\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0328 - val_loss: 0.0279\n",
      "Epoch 47/500\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 0.0323 - val_loss: 0.0276\n",
      "Epoch 48/500\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 0.0320 - val_loss: 0.0275\n",
      "Epoch 49/500\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 0.0318 - val_loss: 0.0274\n",
      "Epoch 50/500\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 0.0317 - val_loss: 0.0275\n",
      "Epoch 51/500\n",
      "1/1 [==============================] - 0s 231ms/step - loss: 0.0318 - val_loss: 0.0276\n",
      "Epoch 52/500\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.0319 - val_loss: 0.0277\n",
      "Epoch 53/500\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.0320 - val_loss: 0.0279\n",
      "Epoch 54/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 0.0321 - val_loss: 0.0280\n",
      "Epoch 55/500\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.0323 - val_loss: 0.0281\n",
      "Epoch 56/500\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 0.0324 - val_loss: 0.0282\n",
      "Epoch 57/500\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 0.0325 - val_loss: 0.0283\n",
      "Epoch 58/500\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.0325 - val_loss: 0.0283\n",
      "Epoch 59/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 0.0325 - val_loss: 0.0283\n",
      "Epoch 60/500\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.0325 - val_loss: 0.0282\n",
      "Epoch 61/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 0.0324 - val_loss: 0.0281\n",
      "Epoch 62/500\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 0.0324 - val_loss: 0.0280\n",
      "Epoch 63/500\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 0.0322 - val_loss: 0.0279\n",
      "Epoch 64/500\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 0.0321 - val_loss: 0.0278\n",
      "Epoch 65/500\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 0.0320 - val_loss: 0.0277\n",
      "Epoch 66/500\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.0319 - val_loss: 0.0276\n",
      "Epoch 67/500\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.0318 - val_loss: 0.0275\n",
      "Epoch 68/500\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 0.0317 - val_loss: 0.0274\n",
      "Epoch 69/500\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 0.0317 - val_loss: 0.0274\n",
      "Epoch 70/500\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 0.0316 - val_loss: 0.0274\n",
      "Epoch 71/500\n",
      "1/1 [==============================] - 0s 365ms/step - loss: 0.0316 - val_loss: 0.0273\n",
      "Epoch 72/500\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 0.0316 - val_loss: 0.0273\n",
      "Epoch 73/500\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 0.0315 - val_loss: 0.0273\n",
      "Epoch 74/500\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 0.0315 - val_loss: 0.0274\n",
      "Epoch 75/500\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0316 - val_loss: 0.0274\n",
      "Epoch 76/500\n",
      "1/1 [==============================] - 0s 358ms/step - loss: 0.0316 - val_loss: 0.0274\n",
      "Epoch 77/500\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.0316 - val_loss: 0.0274\n",
      "Epoch 78/500\n",
      "1/1 [==============================] - 0s 242ms/step - loss: 0.0316 - val_loss: 0.0274\n",
      "Epoch 79/500\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 0.0316 - val_loss: 0.0274\n",
      "Epoch 80/500\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 0.0316 - val_loss: 0.0274\n",
      "Epoch 81/500\n",
      "1/1 [==============================] - 0s 442ms/step - loss: 0.0315 - val_loss: 0.0274\n",
      "Epoch 82/500\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 0.0315 - val_loss: 0.0274\n",
      "Epoch 83/500\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.0315 - val_loss: 0.0273\n",
      "Epoch 84/500\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 0.0315 - val_loss: 0.0273\n",
      "Epoch 85/500\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.0315 - val_loss: 0.0273\n",
      "Epoch 86/500\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.0314 - val_loss: 0.0273\n",
      "Epoch 87/500\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.0314 - val_loss: 0.0273\n",
      "Epoch 88/500\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 0.0314 - val_loss: 0.0273\n",
      "Epoch 89/500\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.0314 - val_loss: 0.0272\n",
      "Epoch 90/500\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.0313 - val_loss: 0.0272\n",
      "Epoch 91/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 0.0313 - val_loss: 0.0272\n",
      "Epoch 92/500\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.0313 - val_loss: 0.0272\n",
      "Epoch 93/500\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.0313 - val_loss: 0.0272\n",
      "Epoch 94/500\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.0313 - val_loss: 0.0272\n",
      "Epoch 95/500\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.0313 - val_loss: 0.0272\n",
      "Epoch 96/500\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 0.0313 - val_loss: 0.0272\n",
      "Epoch 97/500\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.0312 - val_loss: 0.0272\n",
      "Epoch 98/500\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.0312 - val_loss: 0.0272\n",
      "Epoch 99/500\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 0.0312 - val_loss: 0.0272\n",
      "Epoch 100/500\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 0.0312 - val_loss: 0.0272\n",
      "Epoch 101/500\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.0312 - val_loss: 0.0272\n",
      "Epoch 102/500\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.0312 - val_loss: 0.0272\n",
      "Epoch 103/500\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 0.0312 - val_loss: 0.0272\n",
      "Epoch 104/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 0.0311 - val_loss: 0.0272\n",
      "Epoch 105/500\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.0311 - val_loss: 0.0272\n",
      "Epoch 106/500\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 0.0311 - val_loss: 0.0271\n",
      "Epoch 107/500\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.0311 - val_loss: 0.0271\n",
      "Epoch 108/500\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 0.0311 - val_loss: 0.0271\n",
      "Epoch 109/500\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.0311 - val_loss: 0.0271\n",
      "Epoch 110/500\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.0310 - val_loss: 0.0271\n",
      "Epoch 111/500\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.0310 - val_loss: 0.0271\n",
      "Epoch 112/500\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 0.0310 - val_loss: 0.0271\n",
      "Epoch 113/500\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.0310 - val_loss: 0.0271\n",
      "Epoch 114/500\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.0310 - val_loss: 0.0271\n",
      "Epoch 115/500\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.0310 - val_loss: 0.0271\n",
      "Epoch 116/500\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 0.0310 - val_loss: 0.0271\n",
      "Epoch 117/500\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.0309 - val_loss: 0.0271\n",
      "Epoch 118/500\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.0309 - val_loss: 0.0271\n",
      "Epoch 119/500\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0309 - val_loss: 0.0271\n",
      "Epoch 120/500\n",
      "1/1 [==============================] - 0s 389ms/step - loss: 0.0309 - val_loss: 0.0271\n",
      "Epoch 121/500\n",
      "1/1 [==============================] - 0s 363ms/step - loss: 0.0309 - val_loss: 0.0270\n",
      "Epoch 122/500\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.0309 - val_loss: 0.0270\n",
      "Epoch 123/500\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.0308 - val_loss: 0.0270\n",
      "Epoch 124/500\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.0308 - val_loss: 0.0270\n",
      "Epoch 125/500\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 0.0308 - val_loss: 0.0270\n",
      "Epoch 126/500\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 0.0308 - val_loss: 0.0270\n",
      "Epoch 127/500\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 0.0308 - val_loss: 0.0270\n",
      "Epoch 128/500\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 0.0308 - val_loss: 0.0270\n",
      "Epoch 129/500\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0307 - val_loss: 0.0270\n",
      "Epoch 130/500\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 0.0307 - val_loss: 0.0270\n",
      "Epoch 131/500\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 0.0307 - val_loss: 0.0270\n",
      "Epoch 132/500\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 0.0307 - val_loss: 0.0270\n",
      "Epoch 133/500\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 0.0307 - val_loss: 0.0270\n",
      "Epoch 134/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 0.0307 - val_loss: 0.0270\n",
      "Epoch 135/500\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 0.0306 - val_loss: 0.0269\n",
      "Epoch 136/500\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 0.0306 - val_loss: 0.0269\n",
      "Epoch 137/500\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 0.0306 - val_loss: 0.0269\n",
      "Epoch 138/500\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 0.0306 - val_loss: 0.0269\n",
      "Epoch 139/500\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 0.0306 - val_loss: 0.0269\n",
      "Epoch 140/500\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 0.0306 - val_loss: 0.0269\n",
      "Epoch 141/500\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 0.0305 - val_loss: 0.0269\n",
      "Epoch 142/500\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.0305 - val_loss: 0.0269\n",
      "Epoch 143/500\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.0305 - val_loss: 0.0269\n",
      "Epoch 144/500\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.0305 - val_loss: 0.0269\n",
      "Epoch 145/500\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 0.0305 - val_loss: 0.0269\n",
      "Epoch 146/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.0304 - val_loss: 0.0269\n",
      "Epoch 147/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 263ms/step - loss: 0.0304 - val_loss: 0.0269\n",
      "Epoch 148/500\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.0304 - val_loss: 0.0268\n",
      "Epoch 149/500\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 0.0304 - val_loss: 0.0268\n",
      "Epoch 150/500\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.0304 - val_loss: 0.0268\n",
      "Epoch 151/500\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 0.0304 - val_loss: 0.0268\n",
      "Epoch 152/500\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.0303 - val_loss: 0.0268\n",
      "Epoch 153/500\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.0303 - val_loss: 0.0268\n",
      "Epoch 154/500\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.0303 - val_loss: 0.0268\n",
      "Epoch 155/500\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0303 - val_loss: 0.0268\n",
      "Epoch 156/500\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.0303 - val_loss: 0.0268\n",
      "Epoch 157/500\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 0.0302 - val_loss: 0.0268\n",
      "Epoch 158/500\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.0302 - val_loss: 0.0268\n",
      "Epoch 159/500\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 0.0302 - val_loss: 0.0268\n",
      "Epoch 160/500\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.0302 - val_loss: 0.0267\n",
      "Epoch 161/500\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.0302 - val_loss: 0.0267\n",
      "Epoch 162/500\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 0.0301 - val_loss: 0.0267\n",
      "Epoch 163/500\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.0301 - val_loss: 0.0267\n",
      "Epoch 164/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 0.0301 - val_loss: 0.0267\n",
      "Epoch 165/500\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.0301 - val_loss: 0.0267\n",
      "Epoch 166/500\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.0301 - val_loss: 0.0267\n",
      "Epoch 167/500\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 0.0300 - val_loss: 0.0267\n",
      "Epoch 168/500\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 0.0300 - val_loss: 0.0267\n",
      "Epoch 169/500\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.0300 - val_loss: 0.0267\n",
      "Epoch 170/500\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.0300 - val_loss: 0.0267\n",
      "Epoch 171/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 0.0300 - val_loss: 0.0267\n",
      "Epoch 172/500\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0299 - val_loss: 0.0266\n",
      "Epoch 173/500\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 0.0299 - val_loss: 0.0266\n",
      "Epoch 174/500\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.0299 - val_loss: 0.0266\n",
      "Epoch 175/500\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 0.0299 - val_loss: 0.0266\n",
      "Epoch 176/500\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.0299 - val_loss: 0.0266\n",
      "Epoch 177/500\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 0.0298 - val_loss: 0.0266\n",
      "Epoch 178/500\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 0.0298 - val_loss: 0.0266\n",
      "Epoch 179/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.0298 - val_loss: 0.0266\n",
      "Epoch 180/500\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.0298 - val_loss: 0.0266\n",
      "Epoch 181/500\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.0298 - val_loss: 0.0266\n",
      "Epoch 182/500\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.0297 - val_loss: 0.0266\n",
      "Epoch 183/500\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 0.0297 - val_loss: 0.0265\n",
      "Epoch 184/500\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.0297 - val_loss: 0.0265\n",
      "Epoch 185/500\n",
      "1/1 [==============================] - 0s 231ms/step - loss: 0.0297 - val_loss: 0.0265\n",
      "Epoch 186/500\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 0.0297 - val_loss: 0.0265\n",
      "Epoch 187/500\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 0.0296 - val_loss: 0.0265\n",
      "Epoch 188/500\n",
      "1/1 [==============================] - 0s 242ms/step - loss: 0.0296 - val_loss: 0.0265\n",
      "Epoch 189/500\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 0.0296 - val_loss: 0.0265\n",
      "Epoch 190/500\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 0.0296 - val_loss: 0.0265\n",
      "Epoch 191/500\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0295 - val_loss: 0.0265\n",
      "Epoch 192/500\n",
      "1/1 [==============================] - 0s 242ms/step - loss: 0.0295 - val_loss: 0.0265\n",
      "Epoch 193/500\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0295 - val_loss: 0.0265\n",
      "Epoch 194/500\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 0.0295 - val_loss: 0.0265\n",
      "Epoch 195/500\n",
      "1/1 [==============================] - 0s 244ms/step - loss: 0.0295 - val_loss: 0.0264\n",
      "Epoch 196/500\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 0.0294 - val_loss: 0.0264\n",
      "Epoch 197/500\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.0294 - val_loss: 0.0264\n",
      "Epoch 198/500\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 0.0294 - val_loss: 0.0264\n",
      "Epoch 199/500\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 0.0294 - val_loss: 0.0264\n",
      "Epoch 200/500\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 0.0294 - val_loss: 0.0264\n",
      "Epoch 201/500\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 0.0293 - val_loss: 0.0264\n",
      "Epoch 202/500\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 0.0293 - val_loss: 0.0264\n",
      "Epoch 203/500\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 0.0293 - val_loss: 0.0264\n",
      "Epoch 204/500\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 0.0293 - val_loss: 0.0264\n",
      "Epoch 205/500\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 0.0292 - val_loss: 0.0264\n",
      "Epoch 206/500\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 0.0292 - val_loss: 0.0263\n",
      "Epoch 207/500\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 0.0292 - val_loss: 0.0263\n",
      "Epoch 208/500\n",
      "1/1 [==============================] - 0s 354ms/step - loss: 0.0292 - val_loss: 0.0263\n",
      "Epoch 209/500\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 0.0292 - val_loss: 0.0263\n",
      "Epoch 210/500\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.0291 - val_loss: 0.0263\n",
      "Epoch 211/500\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.0291 - val_loss: 0.0263\n",
      "Epoch 212/500\n",
      "1/1 [==============================] - 0s 231ms/step - loss: 0.0291 - val_loss: 0.0263\n",
      "Epoch 213/500\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.0291 - val_loss: 0.0263\n",
      "Epoch 214/500\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 0.0291 - val_loss: 0.0263\n",
      "Epoch 215/500\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 0.0290 - val_loss: 0.0263\n",
      "Epoch 216/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 0.0290 - val_loss: 0.0263\n",
      "Epoch 217/500\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 0.0290 - val_loss: 0.0262\n",
      "Epoch 218/500\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.0290 - val_loss: 0.0262\n",
      "Epoch 219/500\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 0.0289 - val_loss: 0.0262\n",
      "Epoch 220/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.0289 - val_loss: 0.0262\n",
      "Epoch 221/500\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.0289 - val_loss: 0.0262\n",
      "Epoch 222/500\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 0.0289 - val_loss: 0.0262\n",
      "Epoch 223/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.0289 - val_loss: 0.0262\n",
      "Epoch 224/500\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.0288 - val_loss: 0.0262\n",
      "Epoch 225/500\n",
      "1/1 [==============================] - 0s 244ms/step - loss: 0.0288 - val_loss: 0.0262\n",
      "Epoch 226/500\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 0.0288 - val_loss: 0.0262\n",
      "Epoch 227/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 0.0288 - val_loss: 0.0262\n",
      "Epoch 228/500\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 0.0288 - val_loss: 0.0261\n",
      "Epoch 229/500\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.0287 - val_loss: 0.0261\n",
      "Epoch 230/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.0287 - val_loss: 0.0261\n",
      "Epoch 231/500\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.0287 - val_loss: 0.0261\n",
      "Epoch 232/500\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 0.0287 - val_loss: 0.0261\n",
      "Epoch 233/500\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.0287 - val_loss: 0.0261\n",
      "Epoch 234/500\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.0286 - val_loss: 0.0261\n",
      "Epoch 235/500\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 0.0286 - val_loss: 0.0261\n",
      "Epoch 236/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 0.0286 - val_loss: 0.0261\n",
      "Epoch 237/500\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.0286 - val_loss: 0.0261\n",
      "Epoch 238/500\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 0.0286 - val_loss: 0.0261\n",
      "Epoch 239/500\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 0.0285 - val_loss: 0.0261\n",
      "Epoch 240/500\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 0.0285 - val_loss: 0.0260\n",
      "Epoch 241/500\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 0.0285 - val_loss: 0.0260\n",
      "Epoch 242/500\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0285 - val_loss: 0.0260\n",
      "Epoch 243/500\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 0.0284 - val_loss: 0.0260\n",
      "Epoch 244/500\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 0.0284 - val_loss: 0.0260\n",
      "Epoch 245/500\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.0284 - val_loss: 0.0260\n",
      "Epoch 246/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 0.0284 - val_loss: 0.0260\n",
      "Epoch 247/500\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 0.0284 - val_loss: 0.0260\n",
      "Epoch 248/500\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.0283 - val_loss: 0.0260\n",
      "Epoch 249/500\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.0283 - val_loss: 0.0260\n",
      "Epoch 250/500\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.0283 - val_loss: 0.0260\n",
      "Epoch 251/500\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 0.0283 - val_loss: 0.0260\n",
      "Epoch 252/500\n",
      "1/1 [==============================] - 0s 231ms/step - loss: 0.0283 - val_loss: 0.0259\n",
      "Epoch 253/500\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 0.0283 - val_loss: 0.0259\n",
      "Epoch 254/500\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.0282 - val_loss: 0.0259\n",
      "Epoch 255/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 0.0282 - val_loss: 0.0259\n",
      "Epoch 256/500\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 0.0282 - val_loss: 0.0259\n",
      "Epoch 257/500\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 0.0282 - val_loss: 0.0259\n",
      "Epoch 258/500\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.0282 - val_loss: 0.0259\n",
      "Epoch 259/500\n",
      "1/1 [==============================] - 0s 244ms/step - loss: 0.0281 - val_loss: 0.0259\n",
      "Epoch 260/500\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.0281 - val_loss: 0.0259\n",
      "Epoch 261/500\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 0.0281 - val_loss: 0.0259\n",
      "Epoch 262/500\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 0.0281 - val_loss: 0.0259\n",
      "Epoch 263/500\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.0281 - val_loss: 0.0259\n",
      "Epoch 264/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.0280 - val_loss: 0.0258\n",
      "Epoch 265/500\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.0280 - val_loss: 0.0258\n",
      "Epoch 266/500\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 0.0280 - val_loss: 0.0258\n",
      "Epoch 267/500\n",
      "1/1 [==============================] - 0s 448ms/step - loss: 0.0280 - val_loss: 0.0258\n",
      "Epoch 268/500\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.0280 - val_loss: 0.0258\n",
      "Epoch 269/500\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.0280 - val_loss: 0.0258\n",
      "Epoch 270/500\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 0.0279 - val_loss: 0.0258\n",
      "Epoch 271/500\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.0279 - val_loss: 0.0258\n",
      "Epoch 272/500\n",
      "1/1 [==============================] - 0s 231ms/step - loss: 0.0279 - val_loss: 0.0258\n",
      "Epoch 273/500\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.0279 - val_loss: 0.0258\n",
      "Epoch 274/500\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 0.0279 - val_loss: 0.0258\n",
      "Epoch 275/500\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.0279 - val_loss: 0.0258\n",
      "Epoch 276/500\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 0.0278 - val_loss: 0.0258\n",
      "Epoch 277/500\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 0.0278 - val_loss: 0.0257\n",
      "Epoch 278/500\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 0.0278 - val_loss: 0.0257\n",
      "Epoch 279/500\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.0278 - val_loss: 0.0257\n",
      "Epoch 280/500\n",
      "1/1 [==============================] - 0s 233ms/step - loss: 0.0278 - val_loss: 0.0257\n",
      "Epoch 281/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.0278 - val_loss: 0.0257\n",
      "Epoch 282/500\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.0277 - val_loss: 0.0257\n",
      "Epoch 283/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.0277 - val_loss: 0.0257\n",
      "Epoch 284/500\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.0277 - val_loss: 0.0257\n",
      "Epoch 285/500\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.0277 - val_loss: 0.0257\n",
      "Epoch 286/500\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 0.0277 - val_loss: 0.0257\n",
      "Epoch 287/500\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 0.0277 - val_loss: 0.0257\n",
      "Epoch 288/500\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 0.0276 - val_loss: 0.0257\n",
      "Epoch 289/500\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 0.0276 - val_loss: 0.0257\n",
      "Epoch 290/500\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.0276 - val_loss: 0.0257\n",
      "Epoch 291/500\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.0276 - val_loss: 0.0257\n",
      "Epoch 292/500\n",
      "1/1 [==============================] - 0s 231ms/step - loss: 0.0276 - val_loss: 0.0256\n",
      "Epoch 293/500\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.0276 - val_loss: 0.0256\n",
      "Epoch 294/500\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 0.0276 - val_loss: 0.0256\n",
      "Epoch 295/500\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 0.0275 - val_loss: 0.0256\n",
      "Epoch 296/500\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.0275 - val_loss: 0.0256\n",
      "Epoch 297/500\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.0275 - val_loss: 0.0256\n",
      "Epoch 298/500\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.0275 - val_loss: 0.0256\n",
      "Epoch 299/500\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.0275 - val_loss: 0.0256\n",
      "Epoch 300/500\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.0275 - val_loss: 0.0256\n",
      "Epoch 301/500\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.0275 - val_loss: 0.0256\n",
      "Epoch 302/500\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.0274 - val_loss: 0.0256\n",
      "Epoch 303/500\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.0274 - val_loss: 0.0256\n",
      "Epoch 304/500\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.0274 - val_loss: 0.0256\n",
      "Epoch 305/500\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0274 - val_loss: 0.0256\n",
      "Epoch 306/500\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 0.0274 - val_loss: 0.0256\n",
      "Epoch 307/500\n",
      "1/1 [==============================] - 0s 413ms/step - loss: 0.0274 - val_loss: 0.0256\n",
      "Epoch 308/500\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 0.0274 - val_loss: 0.0255\n",
      "Epoch 309/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 289ms/step - loss: 0.0274 - val_loss: 0.0255\n",
      "Epoch 310/500\n",
      "1/1 [==============================] - 0s 231ms/step - loss: 0.0273 - val_loss: 0.0255\n",
      "Epoch 311/500\n",
      "1/1 [==============================] - 0s 336ms/step - loss: 0.0273 - val_loss: 0.0255\n",
      "Epoch 312/500\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0273 - val_loss: 0.0255\n",
      "Epoch 313/500\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 0.0273 - val_loss: 0.0255\n",
      "Epoch 314/500\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.0273 - val_loss: 0.0255\n",
      "Epoch 315/500\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 0.0273 - val_loss: 0.0255\n",
      "Epoch 316/500\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.0273 - val_loss: 0.0255\n",
      "Epoch 317/500\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 0.0273 - val_loss: 0.0255\n",
      "Epoch 318/500\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 0.0273 - val_loss: 0.0255\n",
      "Epoch 319/500\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 0.0272 - val_loss: 0.0255\n",
      "Epoch 320/500\n",
      "1/1 [==============================] - 0s 340ms/step - loss: 0.0272 - val_loss: 0.0255\n",
      "Epoch 321/500\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 0.0272 - val_loss: 0.0255\n",
      "Epoch 322/500\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 0.0272 - val_loss: 0.0255\n",
      "Epoch 323/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 0.0272 - val_loss: 0.0255\n",
      "Epoch 324/500\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.0272 - val_loss: 0.0255\n",
      "Epoch 325/500\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 0.0272 - val_loss: 0.0254\n",
      "Epoch 326/500\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.0272 - val_loss: 0.0254\n",
      "Epoch 327/500\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 0.0272 - val_loss: 0.0254\n",
      "Epoch 328/500\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0272 - val_loss: 0.0254\n",
      "Epoch 329/500\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.0271 - val_loss: 0.0254\n",
      "Epoch 330/500\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 0.0271 - val_loss: 0.0254\n",
      "Epoch 331/500\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 0.0271 - val_loss: 0.0254\n",
      "Epoch 332/500\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 0.0271 - val_loss: 0.0254\n",
      "Epoch 333/500\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 0.0271 - val_loss: 0.0254\n",
      "Epoch 334/500\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 0.0271 - val_loss: 0.0254\n",
      "Epoch 335/500\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.0271 - val_loss: 0.0254\n",
      "Epoch 336/500\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.0271 - val_loss: 0.0254\n",
      "Epoch 337/500\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 0.0271 - val_loss: 0.0254\n",
      "Epoch 338/500\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.0271 - val_loss: 0.0254\n",
      "Epoch 339/500\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 0.0271 - val_loss: 0.0254\n",
      "Epoch 340/500\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 0.0270 - val_loss: 0.0254\n",
      "Epoch 341/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 0.0270 - val_loss: 0.0254\n",
      "Epoch 342/500\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.0270 - val_loss: 0.0254\n",
      "Epoch 343/500\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.0270 - val_loss: 0.0254\n",
      "Epoch 344/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 0.0270 - val_loss: 0.0254\n",
      "Epoch 345/500\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.0270 - val_loss: 0.0253\n",
      "Epoch 346/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.0270 - val_loss: 0.0253\n",
      "Epoch 347/500\n",
      "1/1 [==============================] - 0s 233ms/step - loss: 0.0270 - val_loss: 0.0253\n",
      "Epoch 348/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.0270 - val_loss: 0.0253\n",
      "Epoch 349/500\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 0.0270 - val_loss: 0.0253\n",
      "Epoch 350/500\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.0270 - val_loss: 0.0253\n",
      "Epoch 351/500\n",
      "1/1 [==============================] - 0s 233ms/step - loss: 0.0270 - val_loss: 0.0253\n",
      "Epoch 352/500\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.0270 - val_loss: 0.0253\n",
      "Epoch 353/500\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.0269 - val_loss: 0.0253\n",
      "Epoch 354/500\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 0.0269 - val_loss: 0.0253\n",
      "Epoch 355/500\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 0.0269 - val_loss: 0.0253\n",
      "Epoch 356/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 0.0269 - val_loss: 0.0253\n",
      "Epoch 357/500\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.0269 - val_loss: 0.0253\n",
      "Epoch 358/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 0.0269 - val_loss: 0.0253\n",
      "Epoch 359/500\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 0.0269 - val_loss: 0.0253\n",
      "Epoch 360/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 0.0269 - val_loss: 0.0253\n",
      "Epoch 361/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.0269 - val_loss: 0.0253\n",
      "Epoch 362/500\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.0269 - val_loss: 0.0253\n",
      "Epoch 363/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.0269 - val_loss: 0.0253\n",
      "Epoch 364/500\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 0.0269 - val_loss: 0.0253\n",
      "Epoch 365/500\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 0.0269 - val_loss: 0.0253\n",
      "Epoch 366/500\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 0.0269 - val_loss: 0.0252\n",
      "Epoch 367/500\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 0.0268 - val_loss: 0.0252\n",
      "Epoch 368/500\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.0268 - val_loss: 0.0252\n",
      "Epoch 369/500\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 0.0268 - val_loss: 0.0252\n",
      "Epoch 370/500\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 0.0268 - val_loss: 0.0252\n",
      "Epoch 371/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 0.0268 - val_loss: 0.0252\n",
      "Epoch 372/500\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 0.0268 - val_loss: 0.0252\n",
      "Epoch 373/500\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 0.0268 - val_loss: 0.0252\n",
      "Epoch 374/500\n",
      "1/1 [==============================] - 0s 240ms/step - loss: 0.0268 - val_loss: 0.0252\n",
      "Epoch 375/500\n",
      "1/1 [==============================] - 0s 242ms/step - loss: 0.0268 - val_loss: 0.0252\n",
      "Epoch 376/500\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 0.0268 - val_loss: 0.0252\n",
      "Epoch 377/500\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 0.0268 - val_loss: 0.0252\n",
      "Epoch 378/500\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.0268 - val_loss: 0.0252\n",
      "Epoch 379/500\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 0.0268 - val_loss: 0.0252\n",
      "Epoch 380/500\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.0268 - val_loss: 0.0252\n",
      "Epoch 381/500\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 0.0268 - val_loss: 0.0252\n",
      "Epoch 382/500\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.0268 - val_loss: 0.0252\n",
      "Epoch 383/500\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 0.0268 - val_loss: 0.0252\n",
      "Epoch 384/500\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 0.0267 - val_loss: 0.0252\n",
      "Epoch 385/500\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.0267 - val_loss: 0.0252\n",
      "Epoch 386/500\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 0.0267 - val_loss: 0.0252\n",
      "Epoch 387/500\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.0267 - val_loss: 0.0252\n",
      "Epoch 388/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 0.0267 - val_loss: 0.0252\n",
      "Epoch 389/500\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 0.0267 - val_loss: 0.0251\n",
      "Epoch 390/500\n",
      "1/1 [==============================] - 0s 361ms/step - loss: 0.0267 - val_loss: 0.0251\n",
      "Epoch 391/500\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 0.0267 - val_loss: 0.0251\n",
      "Epoch 392/500\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0267 - val_loss: 0.0251\n",
      "Epoch 393/500\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 0.0267 - val_loss: 0.0251\n",
      "Epoch 394/500\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 0.0267 - val_loss: 0.0251\n",
      "Epoch 395/500\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 0.0267 - val_loss: 0.0251\n",
      "Epoch 396/500\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 0.0267 - val_loss: 0.0251\n",
      "Epoch 397/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 0.0267 - val_loss: 0.0251\n",
      "Epoch 398/500\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 0.0267 - val_loss: 0.0251\n",
      "Epoch 399/500\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.0267 - val_loss: 0.0251\n",
      "Epoch 400/500\n",
      "1/1 [==============================] - 0s 231ms/step - loss: 0.0267 - val_loss: 0.0251\n",
      "Epoch 401/500\n",
      "1/1 [==============================] - 0s 233ms/step - loss: 0.0267 - val_loss: 0.0251\n",
      "Epoch 402/500\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.0267 - val_loss: 0.0251\n",
      "Epoch 403/500\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 0.0267 - val_loss: 0.0251\n",
      "Epoch 404/500\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 0.0266 - val_loss: 0.0251\n",
      "Epoch 405/500\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 0.0266 - val_loss: 0.0251\n",
      "Epoch 406/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 0.0266 - val_loss: 0.0251\n",
      "Epoch 407/500\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 0.0266 - val_loss: 0.0251\n",
      "Epoch 408/500\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.0266 - val_loss: 0.0251\n",
      "Epoch 409/500\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 0.0266 - val_loss: 0.0251\n",
      "Epoch 410/500\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 0.0266 - val_loss: 0.0251\n",
      "Epoch 411/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 0.0266 - val_loss: 0.0251\n",
      "Epoch 412/500\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.0266 - val_loss: 0.0251\n",
      "Epoch 413/500\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 0.0266 - val_loss: 0.0250\n",
      "Epoch 414/500\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.0266 - val_loss: 0.0250\n",
      "Epoch 415/500\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 0.0266 - val_loss: 0.0250\n",
      "Epoch 416/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 0.0266 - val_loss: 0.0250\n",
      "Epoch 417/500\n",
      "1/1 [==============================] - 0s 242ms/step - loss: 0.0266 - val_loss: 0.0250\n",
      "Epoch 418/500\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 0.0266 - val_loss: 0.0250\n",
      "Epoch 419/500\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 0.0266 - val_loss: 0.0250\n",
      "Epoch 420/500\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 0.0266 - val_loss: 0.0250\n",
      "Epoch 421/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 0.0266 - val_loss: 0.0250\n",
      "Epoch 422/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 0.0266 - val_loss: 0.0250\n",
      "Epoch 423/500\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.0266 - val_loss: 0.0250\n",
      "Epoch 424/500\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 0.0266 - val_loss: 0.0250\n",
      "Epoch 425/500\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 0.0266 - val_loss: 0.0250\n",
      "Epoch 426/500\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.0265 - val_loss: 0.0250\n",
      "Epoch 427/500\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 0.0265 - val_loss: 0.0250\n",
      "Epoch 428/500\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 0.0265 - val_loss: 0.0250\n",
      "Epoch 429/500\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0265 - val_loss: 0.0250\n",
      "Epoch 430/500\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 0.0265 - val_loss: 0.0250\n",
      "Epoch 431/500\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 0.0265 - val_loss: 0.0250\n",
      "Epoch 432/500\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 0.0265 - val_loss: 0.0250\n",
      "Epoch 433/500\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 0.0265 - val_loss: 0.0250\n",
      "Epoch 434/500\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 0.0265 - val_loss: 0.0250\n",
      "Epoch 435/500\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 0.0265 - val_loss: 0.0250\n",
      "Epoch 436/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 0.0265 - val_loss: 0.0250\n",
      "Epoch 437/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 0.0265 - val_loss: 0.0250\n",
      "Epoch 438/500\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.0265 - val_loss: 0.0249\n",
      "Epoch 439/500\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.0265 - val_loss: 0.0249\n",
      "Epoch 440/500\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 0.0265 - val_loss: 0.0249\n",
      "Epoch 441/500\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.0265 - val_loss: 0.0249\n",
      "Epoch 442/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 0.0265 - val_loss: 0.0249\n",
      "Epoch 443/500\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 0.0265 - val_loss: 0.0249\n",
      "Epoch 444/500\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.0265 - val_loss: 0.0249\n",
      "Epoch 445/500\n",
      "1/1 [==============================] - 0s 242ms/step - loss: 0.0265 - val_loss: 0.0249\n",
      "Epoch 446/500\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.0265 - val_loss: 0.0249\n",
      "Epoch 447/500\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 0.0265 - val_loss: 0.0249\n",
      "Epoch 448/500\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 0.0265 - val_loss: 0.0249\n",
      "Epoch 449/500\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 0.0265 - val_loss: 0.0249\n",
      "Epoch 450/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 0.0264 - val_loss: 0.0249\n",
      "Epoch 451/500\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0264 - val_loss: 0.0249\n",
      "Epoch 452/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.0264 - val_loss: 0.0249\n",
      "Epoch 453/500\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 0.0264 - val_loss: 0.0249\n",
      "Epoch 454/500\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 0.0264 - val_loss: 0.0249\n",
      "Epoch 455/500\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 0.0264 - val_loss: 0.0249\n",
      "Epoch 456/500\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 0.0264 - val_loss: 0.0249\n",
      "Epoch 457/500\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 0.0264 - val_loss: 0.0249\n",
      "Epoch 458/500\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 0.0264 - val_loss: 0.0249\n",
      "Epoch 459/500\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 0.0264 - val_loss: 0.0249\n",
      "Epoch 460/500\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 0.0264 - val_loss: 0.0249\n",
      "Epoch 461/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 0.0264 - val_loss: 0.0249\n",
      "Epoch 462/500\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 0.0264 - val_loss: 0.0249\n",
      "Epoch 463/500\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 0.0264 - val_loss: 0.0249\n",
      "Epoch 464/500\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 0.0264 - val_loss: 0.0249\n",
      "Epoch 465/500\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.0264 - val_loss: 0.0249\n",
      "Epoch 466/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.0264 - val_loss: 0.0248\n",
      "Epoch 467/500\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 0.0264 - val_loss: 0.0248\n",
      "Epoch 468/500\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 0.0264 - val_loss: 0.0248\n",
      "Epoch 469/500\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 0.0264 - val_loss: 0.0248\n",
      "Epoch 470/500\n",
      "1/1 [==============================] - 0s 342ms/step - loss: 0.0264 - val_loss: 0.0248\n",
      "Epoch 471/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 245ms/step - loss: 0.0264 - val_loss: 0.0248\n",
      "Epoch 472/500\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 0.0264 - val_loss: 0.0248\n",
      "Epoch 473/500\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.0264 - val_loss: 0.0248\n",
      "Epoch 474/500\n",
      "1/1 [==============================] - 0s 231ms/step - loss: 0.0264 - val_loss: 0.0248\n",
      "Epoch 475/500\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0264 - val_loss: 0.0248\n",
      "Epoch 476/500\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 0.0264 - val_loss: 0.0248\n",
      "Epoch 477/500\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 0.0263 - val_loss: 0.0248\n",
      "Epoch 478/500\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 0.0263 - val_loss: 0.0248\n",
      "Epoch 479/500\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 0.0263 - val_loss: 0.0248\n",
      "Epoch 480/500\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 0.0263 - val_loss: 0.0248\n",
      "Epoch 481/500\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0263 - val_loss: 0.0248\n",
      "Epoch 482/500\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 0.0263 - val_loss: 0.0248\n",
      "Epoch 483/500\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 0.0263 - val_loss: 0.0248\n",
      "Epoch 484/500\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.0263 - val_loss: 0.0248\n",
      "Epoch 485/500\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 0.0263 - val_loss: 0.0248\n",
      "Epoch 486/500\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 0.0263 - val_loss: 0.0248\n",
      "Epoch 487/500\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0263 - val_loss: 0.0248\n",
      "Epoch 488/500\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 0.0263 - val_loss: 0.0248\n",
      "Epoch 489/500\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 0.0263 - val_loss: 0.0248\n",
      "Epoch 490/500\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0263 - val_loss: 0.0248\n",
      "Epoch 491/500\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 0.0263 - val_loss: 0.0248\n",
      "Epoch 492/500\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 0.0263 - val_loss: 0.0248\n",
      "Epoch 493/500\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 0.0263 - val_loss: 0.0248\n",
      "Epoch 494/500\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.0263 - val_loss: 0.0248\n",
      "Epoch 495/500\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.0263 - val_loss: 0.0248\n",
      "Epoch 496/500\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 0.0263 - val_loss: 0.0247\n",
      "Epoch 497/500\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 0.0263 - val_loss: 0.0247\n",
      "Epoch 498/500\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0263 - val_loss: 0.0247\n",
      "Epoch 499/500\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.0263 - val_loss: 0.0247\n",
      "Epoch 500/500\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.0263 - val_loss: 0.0247\n"
     ]
    }
   ],
   "source": [
    "# main “event” very well represented while the overall reconstruction is very smooth \n",
    "\n",
    "input_window = Input(shape=(window_length,1))\n",
    "x = Conv1D(16, 3, activation=\"tanh\", padding=\"same\")(input_window) # 10 dims\n",
    "#x = BatchNormalization()(x)\n",
    "x = MaxPooling1D(2, padding=\"same\")(x) # 5 dims\n",
    "x = Conv1D(1, 3, activation=\"tanh\", padding=\"same\")(x) # 5 dims\n",
    "#x = BatchNormalization()(x)\n",
    "encoded = MaxPooling1D(2, padding=\"same\")(x) # 3 dims\n",
    "encoder = Model(input_window, encoded)\n",
    "\n",
    "# 3 dimensions in the encoded layer\n",
    "\n",
    "x = Conv1D(1, 3, activation=\"tanh\", padding=\"same\")(encoded) # 3 dims\n",
    "#x = BatchNormalization()(x)\n",
    "x = UpSampling1D(2)(x) # 6 dims\n",
    "x = Conv1D(16, 2, activation='tanh')(x) # 5 dims\n",
    "#x = BatchNormalization()(x)\n",
    "x = UpSampling1D(2)(x) # 10 dims\n",
    "decoded = Conv1D(1, 3, activation='linear', padding='same')(x) # 10 dims\n",
    "autoencoder = Model(input_window, decoded)\n",
    "autoencoder.summary()\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='MeanSquaredError')\n",
    "history = autoencoder.fit(training_set_scaled, training_set_scaled,\n",
    "                epochs=epochs,\n",
    "                batch_size=1024,\n",
    "                shuffle=True,\n",
    "                validation_split = 0.2)\n",
    "\n",
    "decoded_stocks = autoencoder.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 650, 1)]          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 650, 16)           64        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 325, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 325, 1)            49        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 163, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 163, 1)            4         \n",
      "_________________________________________________________________\n",
      "up_sampling1d_2 (UpSampling1 (None, 326, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 325, 16)           48        \n",
      "_________________________________________________________________\n",
      "up_sampling1d_3 (UpSampling1 (None, 650, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 650, 1)            49        \n",
      "=================================================================\n",
      "Total params: 214\n",
      "Trainable params: 214\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(encoder.predict(test_set_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot pickle '_thread.RLock' object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-bc290d24c926>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscoring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mgrid_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_set_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0mn_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_n_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m         \u001b[0mbase_estimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m         parallel = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mnew_object_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew_object_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mnew_object_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0mnew_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mnew_object_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mparams_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'get_params'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;31m# If is its own copy, don't memoize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__setstate__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;31m# If is its own copy, don't memoize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__setstate__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m_deepcopy_tuple\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_deepcopy_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m     \u001b[0;31m# We're not going to put the tuple in the memo, but it's still important we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;31m# check for it, in case the tuple contains recursive mutable structures.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_deepcopy_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m     \u001b[0;31m# We're not going to put the tuple in the memo, but it's still important we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;31m# check for it, in case the tuple contains recursive mutable structures.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;31m# If is its own copy, don't memoize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__setstate__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m_deepcopy_tuple\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_deepcopy_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m     \u001b[0;31m# We're not going to put the tuple in the memo, but it's still important we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;31m# check for it, in case the tuple contains recursive mutable structures.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_deepcopy_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m     \u001b[0;31m# We're not going to put the tuple in the memo, but it's still important we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;31m# check for it, in case the tuple contains recursive mutable structures.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;31m# If is its own copy, don't memoize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__setstate__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;31m# If is its own copy, don't memoize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__setstate__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;31m# If is its own copy, don't memoize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__setstate__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m_deepcopy_tuple\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_deepcopy_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m     \u001b[0;31m# We're not going to put the tuple in the memo, but it's still important we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;31m# check for it, in case the tuple contains recursive mutable structures.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_deepcopy_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m     \u001b[0;31m# We're not going to put the tuple in the memo, but it's still important we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;31m# check for it, in case the tuple contains recursive mutable structures.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;31m# If is its own copy, don't memoize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__setstate__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m_deepcopy_tuple\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_deepcopy_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m     \u001b[0;31m# We're not going to put the tuple in the memo, but it's still important we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;31m# check for it, in case the tuple contains recursive mutable structures.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_deepcopy_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m     \u001b[0;31m# We're not going to put the tuple in the memo, but it's still important we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;31m# check for it, in case the tuple contains recursive mutable structures.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;31m# If is its own copy, don't memoize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__setstate__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;31m# If is its own copy, don't memoize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__setstate__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;31m# If is its own copy, don't memoize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__setstate__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    159\u001b[0m                     \u001b[0mreductor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__reduce_ex__\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mreductor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                         \u001b[0mrv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreductor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                         \u001b[0mreductor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__reduce__\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot pickle '_thread.RLock' object"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "model_classifier = KerasClassifier(autoencoder, verbose=1, batch_size=10, epochs=10)\n",
    "# define the grid search parameters\n",
    "#dimensions = []\n",
    "#dropout = []\n",
    "batch_size = [10,20]\n",
    "loss = ['mean_squared_error', 'binary_crossentropy']\n",
    "optimizer = ['Adam', 'SGD', 'RMSprop']\n",
    "learning_rate = [0.001]\n",
    "epochs = [10, 15]\n",
    "scoring = ['accuracy']\n",
    "cv = [10]\n",
    "param_grid = dict(optimizer=optimizer, learning_rate=learning_rate, cv=cv,scoring = scoring)\n",
    "grid = GridSearchCV(cv=[(slice(None), slice(None))], estimator=model_classifier, param_grid=param_grid, n_jobs=1)\n",
    "\n",
    "## Error\n",
    "grid_result = grid.fit(training_set_scaled, training_set_scaled)\n",
    "\n",
    "best_parameters = grid_result.best_params_\n",
    "best_accuracy = grid_result.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
