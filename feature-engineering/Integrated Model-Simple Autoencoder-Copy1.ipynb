{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, RepeatVector, TimeDistributed, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline psuedo code\n",
    "    n = look back window\n",
    "    k = number of PCs to keep\n",
    "\n",
    "    for each time point t:\n",
    "        p = number of stocks in investable universe at time t\n",
    "        Define an n x p feature matrix X (lagged returns)\n",
    "\n",
    "        Perform PCA on X\n",
    "        Keep the first k PCs in an n x k matrix Z\n",
    "\n",
    "    for each stock s in the investable universe at time t:\n",
    "        Define an n x 1 outcome vector y (future returns of stock s)\n",
    "        Perform a linear regression of y on Z\n",
    "        Predict y for stock s at time t+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_pickle(\"../Data/returns.pkl\").iloc[1:]\n",
    "\n",
    "drop_columns = []\n",
    "for col in df1.columns:\n",
    "    if df1[col].isnull().all() == True:\n",
    "        drop_columns.append(col)\n",
    "        \n",
    "df1.drop(columns=drop_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_investable(t, n_rows):\n",
    "    \"Find stocks in investable universe at time t\\\n",
    "    (stocks in the S&P500 that have prices recorded for the last n_rows days)\"\n",
    "    \n",
    "    df_investable = df1.copy(deep = True).sort_index(ascending = False)\n",
    "    \n",
    "    #add 1 date to get the test features in investable\n",
    "    t = t + pd.DateOffset(1)\n",
    "    \n",
    "    #if t is now a non-trading day, advance until we reach a valid trading day\n",
    "    while t not in df_investable.index:\n",
    "        t = t + pd.DateOffset(1)\n",
    "    \n",
    "    t_index = df_investable.index.get_loc(t)\n",
    "    \n",
    "    #take n_rows worth of data upto time specified\n",
    "    df_investable = df_investable.iloc[t_index + 1:t_index + n_rows + 1]\n",
    "    \n",
    "    #find all stocks that exist in the S&P at this time period\n",
    "    investable_universe = []\n",
    "    for col in df_investable.columns:\n",
    "        if ~df_investable[col].iloc[:n_rows].isna().any():\n",
    "            investable_universe.append(col)\n",
    "        \n",
    "    df_investable = df_investable[investable_universe]\n",
    "    \n",
    "    return df_investable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_y(inv, stock):\n",
    "    y = inv[[stock]].shift(1)\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(X, y):\n",
    "    X_train = X.iloc[1:, :]\n",
    "    X_test = X.iloc[0:1, :]\n",
    "    y_train = y.iloc[1:]\n",
    "    y_test = y.iloc[0:1]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.to_datetime('2020-05-22')\n",
    "n = 200\n",
    "k = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = get_investable(pd.to_datetime('2018-05-11'),500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatX(investable_df):\n",
    "    train_values = []\n",
    "    # Iterates through each day in investable df and appends feature values to train_values\n",
    "    for i in range(len(investable_df.index)):\n",
    "        train_values.append(investable_df.iloc[i].values)\n",
    "    train_values = np.array(train_values) # converts to numpy array\n",
    "    train_values = np.reshape(train_values, (train_values.shape[0], 1, train_values.shape[1])) # reshapes to 3-dimensional\n",
    "    return train_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "tts = train_test_split(df1, test_size=0.2, shuffle=False)\n",
    "train = tts[0]\n",
    "test = tts[1]\n",
    "\n",
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "training_set_scaled = sc.fit_transform(train)\n",
    "test_set_scaled = sc.fit_transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 644)]             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 20)                12900     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 644)               13524     \n",
      "=================================================================\n",
      "Total params: 26,424\n",
      "Trainable params: 26,424\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/250\n",
      "1/1 [==============================] - 1s 664ms/step - loss: 0.2841 - mse: 0.2841 - val_loss: 0.2716 - val_mse: 0.2716\n",
      "Epoch 2/250\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.2669 - mse: 0.2669 - val_loss: 0.2615 - val_mse: 0.2615\n",
      "Epoch 3/250\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.2569 - mse: 0.2569 - val_loss: 0.2528 - val_mse: 0.2528\n",
      "Epoch 4/250\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.2483 - mse: 0.2483 - val_loss: 0.2430 - val_mse: 0.2430\n",
      "Epoch 5/250\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.2386 - mse: 0.2386 - val_loss: 0.2322 - val_mse: 0.2322\n",
      "Epoch 6/250\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2280 - mse: 0.2280 - val_loss: 0.2205 - val_mse: 0.2205\n",
      "Epoch 7/250\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.2164 - mse: 0.2164 - val_loss: 0.2085 - val_mse: 0.2085\n",
      "Epoch 8/250\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2046 - mse: 0.2046 - val_loss: 0.1968 - val_mse: 0.1968\n",
      "Epoch 9/250\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1930 - mse: 0.1930 - val_loss: 0.1854 - val_mse: 0.1854\n",
      "Epoch 10/250\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1817 - mse: 0.1817 - val_loss: 0.1743 - val_mse: 0.1743\n",
      "Epoch 11/250\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1707 - mse: 0.1707 - val_loss: 0.1633 - val_mse: 0.1633\n",
      "Epoch 12/250\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.1598 - mse: 0.1598 - val_loss: 0.1524 - val_mse: 0.1524\n",
      "Epoch 13/250\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.1490 - mse: 0.1490 - val_loss: 0.1418 - val_mse: 0.1418\n",
      "Epoch 14/250\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.1384 - mse: 0.1384 - val_loss: 0.1314 - val_mse: 0.1314\n",
      "Epoch 15/250\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.1282 - mse: 0.1282 - val_loss: 0.1216 - val_mse: 0.1216\n",
      "Epoch 16/250\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1184 - mse: 0.1184 - val_loss: 0.1122 - val_mse: 0.1122\n",
      "Epoch 17/250\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.1091 - mse: 0.1091 - val_loss: 0.1034 - val_mse: 0.1034\n",
      "Epoch 18/250\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.1004 - mse: 0.1004 - val_loss: 0.0951 - val_mse: 0.0951\n",
      "Epoch 19/250\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0922 - mse: 0.0922 - val_loss: 0.0874 - val_mse: 0.0874\n",
      "Epoch 20/250\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0846 - mse: 0.0846 - val_loss: 0.0802 - val_mse: 0.0802\n",
      "Epoch 21/250\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0774 - mse: 0.0774 - val_loss: 0.0735 - val_mse: 0.0735\n",
      "Epoch 22/250\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0708 - mse: 0.0708 - val_loss: 0.0672 - val_mse: 0.0672\n",
      "Epoch 23/250\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0646 - mse: 0.0646 - val_loss: 0.0615 - val_mse: 0.0615\n",
      "Epoch 24/250\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0590 - mse: 0.0590 - val_loss: 0.0563 - val_mse: 0.0563\n",
      "Epoch 25/250\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0538 - mse: 0.0538 - val_loss: 0.0515 - val_mse: 0.0515\n",
      "Epoch 26/250\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0491 - mse: 0.0491 - val_loss: 0.0472 - val_mse: 0.0472\n",
      "Epoch 27/250\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0448 - mse: 0.0448 - val_loss: 0.0433 - val_mse: 0.0433\n",
      "Epoch 28/250\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0410 - mse: 0.0410 - val_loss: 0.0398 - val_mse: 0.0398\n",
      "Epoch 29/250\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0375 - mse: 0.0375 - val_loss: 0.0366 - val_mse: 0.0366\n",
      "Epoch 30/250\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0343 - mse: 0.0343 - val_loss: 0.0338 - val_mse: 0.0338\n",
      "Epoch 31/250\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0315 - mse: 0.0315 - val_loss: 0.0313 - val_mse: 0.0313\n",
      "Epoch 32/250\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0291 - mse: 0.0291 - val_loss: 0.0290 - val_mse: 0.0290\n",
      "Epoch 33/250\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0268 - mse: 0.0268 - val_loss: 0.0270 - val_mse: 0.0270\n",
      "Epoch 34/250\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0249 - mse: 0.0249 - val_loss: 0.0253 - val_mse: 0.0253\n",
      "Epoch 35/250\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0231 - mse: 0.0231 - val_loss: 0.0237 - val_mse: 0.0237\n",
      "Epoch 36/250\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0216 - mse: 0.0216 - val_loss: 0.0223 - val_mse: 0.0223\n",
      "Epoch 37/250\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0202 - mse: 0.0202 - val_loss: 0.0211 - val_mse: 0.0211\n",
      "Epoch 38/250\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0190 - mse: 0.0190 - val_loss: 0.0200 - val_mse: 0.0200\n",
      "Epoch 39/250\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0180 - mse: 0.0180 - val_loss: 0.0191 - val_mse: 0.0191\n",
      "Epoch 40/250\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0170 - mse: 0.0170 - val_loss: 0.0183 - val_mse: 0.0183\n",
      "Epoch 41/250\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0162 - mse: 0.0162 - val_loss: 0.0175 - val_mse: 0.0175\n",
      "Epoch 42/250\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0155 - mse: 0.0155 - val_loss: 0.0169 - val_mse: 0.0169\n",
      "Epoch 43/250\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0148 - mse: 0.0148 - val_loss: 0.0163 - val_mse: 0.0163\n",
      "Epoch 44/250\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0143 - mse: 0.0143 - val_loss: 0.0158 - val_mse: 0.0158\n",
      "Epoch 45/250\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0154 - val_mse: 0.0154\n",
      "Epoch 46/250\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0134 - mse: 0.0134 - val_loss: 0.0150 - val_mse: 0.0150\n",
      "Epoch 47/250\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.0147 - val_mse: 0.0147\n",
      "Epoch 48/250\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0144 - val_mse: 0.0144\n",
      "Epoch 49/250\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.0142 - val_mse: 0.0142\n",
      "Epoch 50/250\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.0139 - val_mse: 0.0139\n",
      "Epoch 51/250\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.0137 - val_mse: 0.0137\n",
      "Epoch 52/250\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0136 - val_mse: 0.0136\n",
      "Epoch 53/250\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0134 - val_mse: 0.0134\n",
      "Epoch 54/250\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.0133 - val_mse: 0.0133\n",
      "Epoch 55/250\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.0131 - val_mse: 0.0131\n",
      "Epoch 56/250\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.0130 - val_mse: 0.0130\n",
      "Epoch 57/250\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.0129 - val_mse: 0.0129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/250\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0128 - val_mse: 0.0128\n",
      "Epoch 59/250\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.0127 - val_mse: 0.0127\n",
      "Epoch 60/250\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.0127 - val_mse: 0.0127\n",
      "Epoch 61/250\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0126 - val_mse: 0.0126\n",
      "Epoch 62/250\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0125 - val_mse: 0.0125\n",
      "Epoch 63/250\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0125 - val_mse: 0.0125\n",
      "Epoch 64/250\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0124 - val_mse: 0.0124\n",
      "Epoch 65/250\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.0124 - val_mse: 0.0124\n",
      "Epoch 66/250\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0124 - val_mse: 0.0124\n",
      "Epoch 67/250\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.0123 - val_mse: 0.0123\n",
      "Epoch 68/250\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0123 - val_mse: 0.0123\n",
      "Epoch 69/250\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0122 - val_mse: 0.0122\n",
      "Epoch 70/250\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.0122 - val_mse: 0.0122\n",
      "Epoch 71/250\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0122 - val_mse: 0.0122\n",
      "Epoch 72/250\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0122 - val_mse: 0.0122\n",
      "Epoch 73/250\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0121 - val_mse: 0.0121\n",
      "Epoch 74/250\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0121 - val_mse: 0.0121\n",
      "Epoch 75/250\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0121 - val_mse: 0.0121\n",
      "Epoch 76/250\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0121 - val_mse: 0.0121\n",
      "Epoch 77/250\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0121 - val_mse: 0.0121\n",
      "Epoch 78/250\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0121 - val_mse: 0.0121\n",
      "Epoch 79/250\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0120 - val_mse: 0.0120\n",
      "Epoch 80/250\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0120 - val_mse: 0.0120\n",
      "Epoch 81/250\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0120 - val_mse: 0.0120\n",
      "Epoch 82/250\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0120 - val_mse: 0.0120\n",
      "Epoch 83/250\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0120 - val_mse: 0.0120\n",
      "Epoch 84/250\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0120 - val_mse: 0.0120\n",
      "Epoch 85/250\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0120 - val_mse: 0.0120\n",
      "Epoch 86/250\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0120 - val_mse: 0.0120\n",
      "Epoch 87/250\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0120 - val_mse: 0.0120\n",
      "Epoch 88/250\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0119 - val_mse: 0.0119\n",
      "Epoch 89/250\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0119 - val_mse: 0.0119\n",
      "Epoch 90/250\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0119 - val_mse: 0.0119\n",
      "Epoch 91/250\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0119 - val_mse: 0.0119\n",
      "Epoch 92/250\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0119 - val_mse: 0.0119\n",
      "Epoch 93/250\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0119 - val_mse: 0.0119\n",
      "Epoch 94/250\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0119 - val_mse: 0.0119\n",
      "Epoch 95/250\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0119 - val_mse: 0.0119\n",
      "Epoch 96/250\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0119 - val_mse: 0.0119\n",
      "Epoch 97/250\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0119 - val_mse: 0.0119\n",
      "Epoch 98/250\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0119 - val_mse: 0.0119\n",
      "Epoch 99/250\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0118 - val_mse: 0.0118\n",
      "Epoch 100/250\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0118 - val_mse: 0.0118\n",
      "Epoch 101/250\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0118 - val_mse: 0.0118\n",
      "Epoch 102/250\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0118 - val_mse: 0.0118\n",
      "Epoch 103/250\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0118 - val_mse: 0.0118\n",
      "Epoch 104/250\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0118 - val_mse: 0.0118\n",
      "Epoch 105/250\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0118 - val_mse: 0.0118\n",
      "Epoch 106/250\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0118 - val_mse: 0.0118\n",
      "Epoch 107/250\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0118 - val_mse: 0.0118\n",
      "Epoch 108/250\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0118 - val_mse: 0.0118\n",
      "Epoch 109/250\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0118 - val_mse: 0.0118\n",
      "Epoch 110/250\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0118 - val_mse: 0.0118\n",
      "Epoch 111/250\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0118 - val_mse: 0.0118\n",
      "Epoch 112/250\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0118 - val_mse: 0.0118\n",
      "Epoch 113/250\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0118 - val_mse: 0.0118\n",
      "Epoch 114/250\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0118 - val_mse: 0.0118\n",
      "Epoch 115/250\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0118 - val_mse: 0.0118\n",
      "Epoch 116/250\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 117/250\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 118/250\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 119/250\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 120/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 121/250\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 122/250\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 123/250\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 124/250\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 125/250\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 126/250\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 127/250\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 128/250\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 129/250\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0117 - val_mse: 0.0117\n",
      "Epoch 130/250\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0116 - val_mse: 0.0116\n",
      "Epoch 131/250\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.0116 - val_mse: 0.0116\n",
      "Epoch 132/250\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0092 - mse: 0.0092 - val_loss: 0.0116 - val_mse: 0.0116\n",
      "Epoch 133/250\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0092 - mse: 0.0092 - val_loss: 0.0116 - val_mse: 0.0116\n",
      "Epoch 134/250\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0092 - mse: 0.0092 - val_loss: 0.0116 - val_mse: 0.0116\n",
      "Epoch 135/250\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0092 - mse: 0.0092 - val_loss: 0.0116 - val_mse: 0.0116\n",
      "Epoch 136/250\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0092 - mse: 0.0092 - val_loss: 0.0116 - val_mse: 0.0116\n",
      "Epoch 137/250\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0092 - mse: 0.0092 - val_loss: 0.0116 - val_mse: 0.0116\n",
      "Epoch 138/250\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0092 - mse: 0.0092 - val_loss: 0.0116 - val_mse: 0.0116\n",
      "Epoch 139/250\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0116 - val_mse: 0.0116\n",
      "Epoch 140/250\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0116 - val_mse: 0.0116\n",
      "Epoch 141/250\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0115 - val_mse: 0.0115\n",
      "Epoch 142/250\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0115 - val_mse: 0.0115\n",
      "Epoch 143/250\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0115 - val_mse: 0.0115\n",
      "Epoch 144/250\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0115 - val_mse: 0.0115\n",
      "Epoch 145/250\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0115 - val_mse: 0.0115\n",
      "Epoch 146/250\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0115 - val_mse: 0.0115\n",
      "Epoch 147/250\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0115 - val_mse: 0.0115\n",
      "Epoch 148/250\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0115 - val_mse: 0.0115\n",
      "Epoch 149/250\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0115 - val_mse: 0.0115\n",
      "Epoch 150/250\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0115 - val_mse: 0.0115\n",
      "Epoch 151/250\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0115 - val_mse: 0.0115\n",
      "Epoch 152/250\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0115 - val_mse: 0.0115\n",
      "Epoch 153/250\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0115 - val_mse: 0.0115\n",
      "Epoch 154/250\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0114 - val_mse: 0.0114\n",
      "Epoch 155/250\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0114 - val_mse: 0.0114\n",
      "Epoch 156/250\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0114 - val_mse: 0.0114\n",
      "Epoch 157/250\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0114 - val_mse: 0.0114\n",
      "Epoch 158/250\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0114 - val_mse: 0.0114\n",
      "Epoch 159/250\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0114 - val_mse: 0.0114\n",
      "Epoch 160/250\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0114 - val_mse: 0.0114\n",
      "Epoch 161/250\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0114 - val_mse: 0.0114\n",
      "Epoch 162/250\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0114 - val_mse: 0.0114\n",
      "Epoch 163/250\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0114 - val_mse: 0.0114\n",
      "Epoch 164/250\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0114 - val_mse: 0.0114\n",
      "Epoch 165/250\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0114 - val_mse: 0.0114\n",
      "Epoch 166/250\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0114 - val_mse: 0.0114\n",
      "Epoch 167/250\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0114 - val_mse: 0.0114\n",
      "Epoch 168/250\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0113 - val_mse: 0.0113\n",
      "Epoch 169/250\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0113 - val_mse: 0.0113\n",
      "Epoch 170/250\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.0113 - val_mse: 0.0113\n",
      "Epoch 171/250\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0113 - val_mse: 0.0113\n",
      "Epoch 172/250\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0113 - val_mse: 0.0113\n",
      "Epoch 173/250\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0113 - val_mse: 0.0113\n",
      "Epoch 174/250\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0113 - val_mse: 0.0113\n",
      "Epoch 175/250\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0113 - val_mse: 0.0113\n",
      "Epoch 176/250\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0113 - val_mse: 0.0113\n",
      "Epoch 177/250\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0113 - val_mse: 0.0113\n",
      "Epoch 178/250\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0113 - val_mse: 0.0113\n",
      "Epoch 179/250\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0113 - val_mse: 0.0113\n",
      "Epoch 180/250\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0113 - val_mse: 0.0113\n",
      "Epoch 181/250\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0113 - val_mse: 0.0113\n",
      "Epoch 182/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0113 - val_mse: 0.0113\n",
      "Epoch 183/250\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0113 - val_mse: 0.0113\n",
      "Epoch 184/250\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0113 - val_mse: 0.0113\n",
      "Epoch 185/250\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 186/250\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 187/250\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 188/250\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 189/250\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 190/250\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 191/250\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 192/250\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 193/250\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 194/250\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 195/250\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 196/250\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 197/250\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 198/250\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 199/250\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 200/250\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0086 - mse: 0.0086 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 201/250\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0086 - mse: 0.0086 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 202/250\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0086 - mse: 0.0086 - val_loss: 0.0112 - val_mse: 0.0112\n",
      "Epoch 203/250\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0086 - mse: 0.0086 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 204/250\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0086 - mse: 0.0086 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 205/250\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0086 - mse: 0.0086 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 206/250\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0086 - mse: 0.0086 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 207/250\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0086 - mse: 0.0086 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 208/250\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0086 - mse: 0.0086 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 209/250\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0086 - mse: 0.0086 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 210/250\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0086 - mse: 0.0086 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 211/250\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0086 - mse: 0.0086 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 212/250\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0086 - mse: 0.0086 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 213/250\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0086 - mse: 0.0086 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 214/250\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0086 - mse: 0.0086 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 215/250\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0086 - mse: 0.0086 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 216/250\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 217/250\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 218/250\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 219/250\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 220/250\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 221/250\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 222/250\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 223/250\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0111 - val_mse: 0.0111\n",
      "Epoch 224/250\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 225/250\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 226/250\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 227/250\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 228/250\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 229/250\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 230/250\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 231/250\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 232/250\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 233/250\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 234/250\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 235/250\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 236/250\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 237/250\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0084 - mse: 0.0084 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 238/250\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0084 - mse: 0.0084 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 239/250\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0084 - mse: 0.0084 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 240/250\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0084 - mse: 0.0084 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 241/250\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0084 - mse: 0.0084 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 242/250\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0084 - mse: 0.0084 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 243/250\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0084 - mse: 0.0084 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 244/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0084 - mse: 0.0084 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 245/250\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0084 - mse: 0.0084 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 246/250\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0084 - mse: 0.0084 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 247/250\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0084 - mse: 0.0084 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 248/250\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0084 - mse: 0.0084 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 249/250\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0084 - mse: 0.0084 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Epoch 250/250\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0084 - mse: 0.0084 - val_loss: 0.0110 - val_mse: 0.0110\n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 644)]             0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 20)                12900     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 644)               13524     \n",
      "=================================================================\n",
      "Total params: 26,424\n",
      "Trainable params: 26,424\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# calculated log returns (i.e. the log of the difference between the price x+1 and price x)\n",
    "# windows of train.shape[1] consecutive returns will be produced. \n",
    "# normalized with a MinMaxScaler to the range [0,1].\n",
    "\n",
    "epochs = 250\n",
    "batch_size = 1024\n",
    "\n",
    "class simple_autoencoder():\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def reduced_dim(self):\n",
    "        encoding_dim = 20\n",
    "        window_length = training_set_scaled.shape[1]\n",
    "        input_window = Input(shape=(window_length,))\n",
    "        # encoded representation of the input\n",
    "        encoded = Dense(encoding_dim, activation='tanh')(input_window) #tanh, linear, leakyrelu\n",
    "        # model mapping an input to its encoded representation\n",
    "        encoder = Model(input_window, encoded)\n",
    "        return pd.DataFrame(encoder.predict(test_set_scaled)).head()\n",
    "\n",
    "    def model(self,optimizer = \"Adam\", score = 'mse', loss = \"mean_squared_error\", dim = 20):\n",
    "        encoding_dim = dim\n",
    "        window_length = training_set_scaled.shape[1]\n",
    "        # input placeholder\n",
    "        input_window = Input(shape=(window_length,))\n",
    "        # encoded representation of the input\n",
    "        encoded = Dense(encoding_dim, activation='tanh')(input_window) #tanh, linear, leakyrelu\n",
    "        # lossy reconstruction of the input\n",
    "        decoded = Dense(window_length, activation='linear')(encoded) #linear\n",
    "        # model mapping an input to its reconstruction\n",
    "        simple_autoencoder = Model(input_window, decoded)\n",
    "        simple_autoencoder.summary()\n",
    "        sae = simple_autoencoder.compile(optimizer=optimizer, loss=loss, metrics=score) #MSE\n",
    "        return simple_autoencoder\n",
    "\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-5, patience=5, mode='auto', verbose = 1)\n",
    "checkpointer = ModelCheckpoint(filepath=\"best_weights.hdf5\",save_best_only=True)\n",
    "\n",
    "\n",
    "model = simple_autoencoder()\n",
    "history = model.model().fit(training_set_scaled, training_set_scaled,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_split = 0.2,\n",
    "                    callbacks = [monitor, checkpointer])       \n",
    "#                   validation_data=(test_set_scaled, test_set_scaled))\n",
    "\n",
    "decoded_stocks = simple_autoencoder().model().predict(test_set_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.860265</td>\n",
       "      <td>0.623784</td>\n",
       "      <td>-0.451812</td>\n",
       "      <td>-0.175510</td>\n",
       "      <td>0.323413</td>\n",
       "      <td>-0.732693</td>\n",
       "      <td>0.596533</td>\n",
       "      <td>-0.447595</td>\n",
       "      <td>0.150045</td>\n",
       "      <td>-0.192876</td>\n",
       "      <td>-0.170092</td>\n",
       "      <td>-0.433506</td>\n",
       "      <td>0.594197</td>\n",
       "      <td>-0.600315</td>\n",
       "      <td>-0.303716</td>\n",
       "      <td>-0.428521</td>\n",
       "      <td>-0.181546</td>\n",
       "      <td>-0.692245</td>\n",
       "      <td>-0.217343</td>\n",
       "      <td>-0.643375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.935163</td>\n",
       "      <td>0.572346</td>\n",
       "      <td>-0.571604</td>\n",
       "      <td>-0.302163</td>\n",
       "      <td>0.239265</td>\n",
       "      <td>-0.713277</td>\n",
       "      <td>0.628787</td>\n",
       "      <td>-0.414250</td>\n",
       "      <td>0.236403</td>\n",
       "      <td>-0.008389</td>\n",
       "      <td>0.230399</td>\n",
       "      <td>-0.419604</td>\n",
       "      <td>0.069427</td>\n",
       "      <td>-0.739879</td>\n",
       "      <td>-0.455660</td>\n",
       "      <td>-0.502683</td>\n",
       "      <td>-0.041132</td>\n",
       "      <td>-0.817724</td>\n",
       "      <td>0.148061</td>\n",
       "      <td>-0.769557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.945177</td>\n",
       "      <td>0.538557</td>\n",
       "      <td>-0.330623</td>\n",
       "      <td>-0.074551</td>\n",
       "      <td>0.238072</td>\n",
       "      <td>-0.806628</td>\n",
       "      <td>0.552139</td>\n",
       "      <td>-0.494771</td>\n",
       "      <td>0.005719</td>\n",
       "      <td>-0.185727</td>\n",
       "      <td>0.183970</td>\n",
       "      <td>-0.550441</td>\n",
       "      <td>0.364848</td>\n",
       "      <td>-0.613470</td>\n",
       "      <td>-0.506859</td>\n",
       "      <td>-0.448307</td>\n",
       "      <td>0.202368</td>\n",
       "      <td>-0.875680</td>\n",
       "      <td>-0.325087</td>\n",
       "      <td>-0.821959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.905469</td>\n",
       "      <td>0.532433</td>\n",
       "      <td>-0.263821</td>\n",
       "      <td>-0.330193</td>\n",
       "      <td>-0.268161</td>\n",
       "      <td>-0.696798</td>\n",
       "      <td>0.755530</td>\n",
       "      <td>-0.297464</td>\n",
       "      <td>0.017223</td>\n",
       "      <td>-0.227357</td>\n",
       "      <td>0.012475</td>\n",
       "      <td>-0.417962</td>\n",
       "      <td>0.431776</td>\n",
       "      <td>-0.671384</td>\n",
       "      <td>-0.361287</td>\n",
       "      <td>-0.294243</td>\n",
       "      <td>-0.081829</td>\n",
       "      <td>-0.639741</td>\n",
       "      <td>-0.351742</td>\n",
       "      <td>-0.748321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.912663</td>\n",
       "      <td>0.505835</td>\n",
       "      <td>-0.285272</td>\n",
       "      <td>-0.122978</td>\n",
       "      <td>0.181963</td>\n",
       "      <td>-0.664202</td>\n",
       "      <td>0.658545</td>\n",
       "      <td>-0.507145</td>\n",
       "      <td>0.003105</td>\n",
       "      <td>-0.280495</td>\n",
       "      <td>0.230701</td>\n",
       "      <td>-0.405006</td>\n",
       "      <td>0.335078</td>\n",
       "      <td>-0.532611</td>\n",
       "      <td>-0.149079</td>\n",
       "      <td>-0.355105</td>\n",
       "      <td>-0.047211</td>\n",
       "      <td>-0.705326</td>\n",
       "      <td>-0.307947</td>\n",
       "      <td>-0.757478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -0.860265  0.623784 -0.451812 -0.175510  0.323413 -0.732693  0.596533   \n",
       "1 -0.935163  0.572346 -0.571604 -0.302163  0.239265 -0.713277  0.628787   \n",
       "2 -0.945177  0.538557 -0.330623 -0.074551  0.238072 -0.806628  0.552139   \n",
       "3 -0.905469  0.532433 -0.263821 -0.330193 -0.268161 -0.696798  0.755530   \n",
       "4 -0.912663  0.505835 -0.285272 -0.122978  0.181963 -0.664202  0.658545   \n",
       "\n",
       "          7         8         9        10        11        12        13  \\\n",
       "0 -0.447595  0.150045 -0.192876 -0.170092 -0.433506  0.594197 -0.600315   \n",
       "1 -0.414250  0.236403 -0.008389  0.230399 -0.419604  0.069427 -0.739879   \n",
       "2 -0.494771  0.005719 -0.185727  0.183970 -0.550441  0.364848 -0.613470   \n",
       "3 -0.297464  0.017223 -0.227357  0.012475 -0.417962  0.431776 -0.671384   \n",
       "4 -0.507145  0.003105 -0.280495  0.230701 -0.405006  0.335078 -0.532611   \n",
       "\n",
       "         14        15        16        17        18        19  \n",
       "0 -0.303716 -0.428521 -0.181546 -0.692245 -0.217343 -0.643375  \n",
       "1 -0.455660 -0.502683 -0.041132 -0.817724  0.148061 -0.769557  \n",
       "2 -0.506859 -0.448307  0.202368 -0.875680 -0.325087 -0.821959  \n",
       "3 -0.361287 -0.294243 -0.081829 -0.639741 -0.351742 -0.748321  \n",
       "4 -0.149079 -0.355105 -0.047211 -0.705326 -0.307947 -0.757478  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = simple_autoencoder().reduced_dim()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    ax = plt.subplot(1, 4, 1)\n",
    "    plt.plot(history.history[\"loss\"])\n",
    "    plt.title(\"Train loss\")\n",
    "    ax = plt.subplot(1, 4, 2)\n",
    "    plt.plot(history.history[\"val_loss\"])\n",
    "    plt.title(\"Test loss\")\n",
    "    ax = plt.subplot(1, 4, 3)\n",
    "    plt.plot(history.history[\"val_mse\"])\n",
    "    plt.title(\"MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loss vs Epoch\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = define_y(df1, df1.columns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>916328</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-05-11</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-10</th>\n",
       "      <td>1.155878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-09</th>\n",
       "      <td>1.952862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-08</th>\n",
       "      <td>1.175268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-07</th>\n",
       "      <td>-1.028321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-24</th>\n",
       "      <td>1.843075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-23</th>\n",
       "      <td>1.632325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-20</th>\n",
       "      <td>-0.611702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-19</th>\n",
       "      <td>0.966702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-18</th>\n",
       "      <td>-1.350993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              916328\n",
       "date                \n",
       "2018-05-11       NaN\n",
       "2018-05-10  1.155878\n",
       "2018-05-09  1.952862\n",
       "2018-05-08  1.175268\n",
       "2018-05-07 -1.028321\n",
       "...              ...\n",
       "2016-05-24  1.843075\n",
       "2016-05-23  1.632325\n",
       "2016-05-20 -0.611702\n",
       "2016-05-19  0.966702\n",
       "2016-05-18 -1.350993\n",
       "\n",
       "[500 rows x 1 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = train_test(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>916328</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-05-10</th>\n",
       "      <td>1.155878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-09</th>\n",
       "      <td>1.952862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-08</th>\n",
       "      <td>1.175268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-07</th>\n",
       "      <td>-1.028321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-04</th>\n",
       "      <td>0.987402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-24</th>\n",
       "      <td>1.843075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-23</th>\n",
       "      <td>1.632325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-20</th>\n",
       "      <td>-0.611702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-19</th>\n",
       "      <td>0.966702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-18</th>\n",
       "      <td>-1.350993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>499 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              916328\n",
       "date                \n",
       "2018-05-10  1.155878\n",
       "2018-05-09  1.952862\n",
       "2018-05-08  1.175268\n",
       "2018-05-07 -1.028321\n",
       "2018-05-04  0.987402\n",
       "...              ...\n",
       "2016-05-24  1.843075\n",
       "2016-05-23  1.632325\n",
       "2016-05-20 -0.611702\n",
       "2016-05-19  0.966702\n",
       "2016-05-18 -1.350993\n",
       "\n",
       "[499 rows x 1 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>916328</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-05-11</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            916328\n",
       "date              \n",
       "2018-05-11     NaN"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit(X_train, y_train):\n",
    "    model = sklearn.linear_model.LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(model, X_test):\n",
    "    yhat = model.predict(X_test)\n",
    "    \n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_returns(t, n, k):\n",
    "    inv = get_investable(t, n)\n",
    "    X = simple_autoencoder().reduced_dim()\n",
    "    \n",
    "    returns_t = pd.DataFrame(index = inv.columns, columns = ['Pred', 'Actual'])\n",
    "    \n",
    "    for stock in inv.columns:\n",
    "        y = define_y(inv, stock)\n",
    "        X_train, y_train, X_test, y_test = train_test(X, y)\n",
    "        model = model_fit(X_train, y_train)\n",
    "        yhat = model_predict(model, X_test)[0][0]\n",
    "        returns_t['Pred'].loc[stock] = yhat\n",
    "        returns_t['Actual'].loc[stock] = y_test.values[0][0]\n",
    "    \n",
    "    return returns_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_stocks(returns, num_stocks):\n",
    "    pred_returns = returns.sort_values(by = 'Pred', ascending = False)\n",
    "    topn = pred_returns.head(num_stocks)\n",
    "    botn = pred_returns.tail(num_stocks)\n",
    "    \n",
    "    return topn, botn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def portfolio_return(topn, botn, returns):\n",
    "    return_t = topn['Actual'].mean() - botn['Actual'].mean()\n",
    "    \n",
    "    return return_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(n, k, num_stocks):\n",
    "\n",
    "    time_range = df1.loc['2007':'2021'].index[:-1]\n",
    "    \n",
    "    portfolio = pd.DataFrame(index = time_range, columns = ['Portfolio Return'])\n",
    "    \n",
    "    count = 0\n",
    "    for t in time_range:\n",
    "        pred_actual = predict_returns(t, n, k)\n",
    "        topn, botn = rank_stocks(pred_actual, num_stocks)\n",
    "        return_t = portfolio_return(topn, botn, pred_actual)\n",
    "        t_index = time_range.get_loc(t) + 1\n",
    "        portfolio['Portfolio Return'].loc[time_range[t_index]] = return_t\n",
    "        \n",
    "        count +=1\n",
    "        print(f'{(count/len(time_range))*100:.2f}% complete')\n",
    "    \n",
    "    portfolio['Portfolio Return'] = portfolio['Portfolio Return'].astype('float')\n",
    "    \n",
    "    return portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "portfolio = pipeline(500, 20, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-29 19:14:14.936898\n"
     ]
    }
   ],
   "source": [
    "#started at 3:07pm\n",
    "print(pd.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Portfolio Return</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Portfolio Return]\n",
       "Index: []"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fd8bae9cb80>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "portfolio.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Buffer has wrong number of dimensions (expected 1, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/seaborn/utils.py\u001b[0m in \u001b[0;36mcategorical_order\u001b[0;34m(values, order)\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m                 \u001b[0morder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5273\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'cat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/seaborn/utils.py\u001b[0m in \u001b[0;36mcategorical_order\u001b[0;34m(values, order)\u001b[0m\n\u001b[1;32m    546\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m                     \u001b[0morder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5273\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'unique'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-2ab55ef0c5d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbarplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mportfolio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Portfolio Return'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mportfolio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'grey'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_major_locator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmdates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mYearLocator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/seaborn/categorical.py\u001b[0m in \u001b[0;36mbarplot\u001b[0;34m(x, y, hue, data, order, hue_order, estimator, ci, n_boot, units, seed, orient, color, palette, saturation, errcolor, errwidth, capsize, dodge, ax, **kwargs)\u001b[0m\n\u001b[1;32m   3142\u001b[0m             ax=None, **kwargs):\n\u001b[1;32m   3143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3144\u001b[0;31m     plotter = _BarPlotter(x, y, hue, data, order, hue_order,\n\u001b[0m\u001b[1;32m   3145\u001b[0m                           \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mci\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_boot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3146\u001b[0m                           \u001b[0morient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaturation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/seaborn/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, hue, data, order, hue_order, estimator, ci, n_boot, units, seed, orient, color, palette, saturation, errcolor, errwidth, capsize, dodge)\u001b[0m\n\u001b[1;32m   1600\u001b[0m                  errwidth, capsize, dodge):\n\u001b[1;32m   1601\u001b[0m         \u001b[0;34m\"\"\"Initialize the plotter.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1602\u001b[0;31m         self.establish_variables(x, y, hue, data, orient,\n\u001b[0m\u001b[1;32m   1603\u001b[0m                                  order, hue_order, units)\n\u001b[1;32m   1604\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestablish_colors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaturation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/seaborn/categorical.py\u001b[0m in \u001b[0;36mestablish_variables\u001b[0;34m(self, x, y, hue, data, orient, order, hue_order, units)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                 \u001b[0;31m# Get the order on the categorical axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                 \u001b[0mgroup_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcategorical_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0;31m# Group the numeric data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/seaborn/utils.py\u001b[0m in \u001b[0;36mcategorical_order\u001b[0;34m(values, order)\u001b[0m\n\u001b[1;32m    547\u001b[0m                     \u001b[0morder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                     \u001b[0morder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36munique\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Float64HashTable.unique\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Buffer has wrong number of dimensions (expected 1, got 2)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(30,15))\n",
    "sns.barplot(x = portfolio.index, y = 'Portfolio Return', data = portfolio, color = 'grey')\n",
    "\n",
    "axes.xaxis.set_major_locator(mdates.YearLocator())\n",
    "axes.xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "\n",
    "ticklabels = [item.strftime('%Y') for item in portfolio.resample('Y').mean().index.to_period('Y')]\n",
    "\n",
    "axes.xaxis.set_major_formatter(ticker.FixedFormatter(ticklabels))\n",
    "\n",
    "plt.xticks(rotation = 'vertical')\n",
    "axes.set_title('Portfolio Returns')\n",
    "\n",
    "sns.set(font_scale=2)\n",
    "plt.axhline(0)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticklabels = [item.strftime('%Y') for item in portfolio.resample('Y').mean().loc['2007':'2021'].index.to_period('Y')]\n",
    "ticklabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Portfolio Return</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Portfolio Return]\n",
       "Index: []"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portfolio_monthly = portfolio.resample('M').mean()\n",
    "portfolio_monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average return is nan %\n"
     ]
    }
   ],
   "source": [
    "avg_return = portfolio['Portfolio Return'].mean()\n",
    "print(f'Average return is {avg_return:.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/LinearRegression'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-4a83dc3fe059>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mportfolio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'results/LinearRegression'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   3202\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3203\u001b[0m         )\n\u001b[0;32m-> 3204\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mclose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             f, handles = get_handle(\n\u001b[0m\u001b[1;32m    185\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0;31m# No explicit encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results/LinearRegression'"
     ]
    }
   ],
   "source": [
    "portfolio.to_csv('results/LinearRegression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File results/LinearRegression does not exist: 'results/LinearRegression'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-ca7e5b476855>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'results/LinearRegression'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File results/LinearRegression does not exist: 'results/LinearRegression'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('results/LinearRegression', parse_dates = True, index_col = ['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_avg = pd.DataFrame(data = df['Portfolio Return'].rolling(252).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_avg.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(figsize=(30,15))\n",
    "sns.barplot(x = df.index, y = 'Portfolio Return', data = df, color = 'grey')\n",
    "\n",
    "axes.xaxis.set_major_locator(mdates.YearLocator())\n",
    "axes.xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "\n",
    "ticklabels = [item.strftime('%Y') for item in portfolio.resample('Y').mean().index.to_period('Y')]\n",
    "\n",
    "axes.xaxis.set_major_formatter(ticker.FixedFormatter(ticklabels))\n",
    "\n",
    "plt.xticks(rotation = 'vertical')\n",
    "axes.set_title('Portfolio Returns')\n",
    "\n",
    "sns.set(font_scale=2)\n",
    "plt.axhline(0)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x = rolling_avg.index, y = 'Portfolio Return', data = rolling_avg, color = 'black')\n",
    "plt.xticks(rotation = 'vertical')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
