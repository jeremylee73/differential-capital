{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anjalichauhan/opt/anaconda3/lib/python3.8/site-packages/IPython/core/magics/pylab.py:159: UserWarning: pylab import has clobbered these variables: ['test']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  warn(\"pylab import has clobbered these variables: %s\"  % clobbered +\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Conv1D, MaxPooling1D, UpSampling1D, BatchNormalization, LSTM, RepeatVector\n",
    "from keras.models import Model\n",
    "from keras.models import model_from_json\n",
    "from keras import regularizers\n",
    "import datetime\n",
    "import time\n",
    "import requests as req\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## full log(returns)/returns dataframe\n",
    "df = pd.read_csv(\"amzn_cleaned\", index_col = 'date', parse_dates = True)\n",
    "\n",
    "df['pct_change'] = df.close.pct_change()\n",
    "df['log_ret'] = np.log(df.close) - np.log(df.close.shift(1))\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>roc</th>\n",
       "      <th>rsi</th>\n",
       "      <th>tsi</th>\n",
       "      <th>bb_bbhi</th>\n",
       "      <th>bb_bbli</th>\n",
       "      <th>aroon_down</th>\n",
       "      <th>aroon</th>\n",
       "      <th>aroon_up</th>\n",
       "      <th>...</th>\n",
       "      <th>obv</th>\n",
       "      <th>vpt</th>\n",
       "      <th>fi</th>\n",
       "      <th>nvi</th>\n",
       "      <th>day_sin</th>\n",
       "      <th>day_cos</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>pct_change</th>\n",
       "      <th>log_ret</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-07-26</th>\n",
       "      <td>36.0625</td>\n",
       "      <td>15452.1</td>\n",
       "      <td>2.852050</td>\n",
       "      <td>40.134541</td>\n",
       "      <td>-15.064725</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>-52.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-71994.0</td>\n",
       "      <td>-1447.166569</td>\n",
       "      <td>-3988.544957</td>\n",
       "      <td>632.201442</td>\n",
       "      <td>-0.743145</td>\n",
       "      <td>0.669131</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.041528</td>\n",
       "      <td>-0.042415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-07-27</th>\n",
       "      <td>31.3750</td>\n",
       "      <td>23576.7</td>\n",
       "      <td>-5.283019</td>\n",
       "      <td>32.699086</td>\n",
       "      <td>-18.375261</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-36.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-95570.7</td>\n",
       "      <td>-3706.260897</td>\n",
       "      <td>-19206.721570</td>\n",
       "      <td>632.201442</td>\n",
       "      <td>-0.587785</td>\n",
       "      <td>0.809017</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.129983</td>\n",
       "      <td>-0.139242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-07-28</th>\n",
       "      <td>30.0000</td>\n",
       "      <td>12444.2</td>\n",
       "      <td>-14.438503</td>\n",
       "      <td>30.891204</td>\n",
       "      <td>-21.659706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-40.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-108014.9</td>\n",
       "      <td>-3609.925738</td>\n",
       "      <td>-18907.300632</td>\n",
       "      <td>604.495403</td>\n",
       "      <td>-0.406737</td>\n",
       "      <td>0.913545</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.043825</td>\n",
       "      <td>-0.044814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-07-31</th>\n",
       "      <td>30.1250</td>\n",
       "      <td>9478.4</td>\n",
       "      <td>-13.928571</td>\n",
       "      <td>31.263266</td>\n",
       "      <td>-24.163073</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-40.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-98536.5</td>\n",
       "      <td>-505.870013</td>\n",
       "      <td>-16037.000541</td>\n",
       "      <td>607.014133</td>\n",
       "      <td>0.207912</td>\n",
       "      <td>0.978148</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.004158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-08-01</th>\n",
       "      <td>30.2500</td>\n",
       "      <td>8140.8</td>\n",
       "      <td>-28.823529</td>\n",
       "      <td>31.659494</td>\n",
       "      <td>-26.085806</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-40.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-90395.7</td>\n",
       "      <td>73.272586</td>\n",
       "      <td>-13600.629035</td>\n",
       "      <td>609.532864</td>\n",
       "      <td>0.207912</td>\n",
       "      <td>0.978148</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              close   volume        roc        rsi        tsi  bb_bbhi  \\\n",
       "date                                                                     \n",
       "2000-07-26  36.0625  15452.1   2.852050  40.134541 -15.064725      0.0   \n",
       "2000-07-27  31.3750  23576.7  -5.283019  32.699086 -18.375261      0.0   \n",
       "2000-07-28  30.0000  12444.2 -14.438503  30.891204 -21.659706      0.0   \n",
       "2000-07-31  30.1250   9478.4 -13.928571  31.263266 -24.163073      0.0   \n",
       "2000-08-01  30.2500   8140.8 -28.823529  31.659494 -26.085806      0.0   \n",
       "\n",
       "            bb_bbli  aroon_down  aroon  aroon_up  ...       obv          vpt  \\\n",
       "date                                              ...                          \n",
       "2000-07-26      0.0        56.0  -52.0       4.0  ...  -71994.0 -1447.166569   \n",
       "2000-07-27      1.0       100.0  -36.0      64.0  ...  -95570.7 -3706.260897   \n",
       "2000-07-28      1.0       100.0  -40.0      60.0  ... -108014.9 -3609.925738   \n",
       "2000-07-31      0.0        96.0  -40.0      56.0  ...  -98536.5  -505.870013   \n",
       "2000-08-01      0.0        92.0  -40.0      52.0  ...  -90395.7    73.272586   \n",
       "\n",
       "                      fi         nvi   day_sin   day_cos  month_sin  \\\n",
       "date                                                                  \n",
       "2000-07-26  -3988.544957  632.201442 -0.743145  0.669131  -0.500000   \n",
       "2000-07-27 -19206.721570  632.201442 -0.587785  0.809017  -0.500000   \n",
       "2000-07-28 -18907.300632  604.495403 -0.406737  0.913545  -0.500000   \n",
       "2000-07-31 -16037.000541  607.014133  0.207912  0.978148  -0.500000   \n",
       "2000-08-01 -13600.629035  609.532864  0.207912  0.978148  -0.866025   \n",
       "\n",
       "            month_cos  pct_change   log_ret  \n",
       "date                                         \n",
       "2000-07-26  -0.866025   -0.041528 -0.042415  \n",
       "2000-07-27  -0.866025   -0.129983 -0.139242  \n",
       "2000-07-28  -0.866025   -0.043825 -0.044814  \n",
       "2000-07-31  -0.866025    0.004167  0.004158  \n",
       "2000-08-01  -0.500000    0.004149  0.004141  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tts = train_test_split(df, test_size=0.2, shuffle=False)\n",
    "train = tts[0]\n",
    "test = tts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>roc</th>\n",
       "      <th>rsi</th>\n",
       "      <th>tsi</th>\n",
       "      <th>bb_bbhi</th>\n",
       "      <th>bb_bbli</th>\n",
       "      <th>aroon_down</th>\n",
       "      <th>aroon</th>\n",
       "      <th>aroon_up</th>\n",
       "      <th>...</th>\n",
       "      <th>obv</th>\n",
       "      <th>vpt</th>\n",
       "      <th>fi</th>\n",
       "      <th>nvi</th>\n",
       "      <th>day_sin</th>\n",
       "      <th>day_cos</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>pct_change</th>\n",
       "      <th>log_ret</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-07-26</th>\n",
       "      <td>36.0625</td>\n",
       "      <td>15452.1</td>\n",
       "      <td>2.852050</td>\n",
       "      <td>40.134541</td>\n",
       "      <td>-15.064725</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>-52.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-71994.0</td>\n",
       "      <td>-1447.166569</td>\n",
       "      <td>-3988.544957</td>\n",
       "      <td>632.201442</td>\n",
       "      <td>-0.743145</td>\n",
       "      <td>0.669131</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.041528</td>\n",
       "      <td>-0.042415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-07-27</th>\n",
       "      <td>31.3750</td>\n",
       "      <td>23576.7</td>\n",
       "      <td>-5.283019</td>\n",
       "      <td>32.699086</td>\n",
       "      <td>-18.375261</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-36.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-95570.7</td>\n",
       "      <td>-3706.260897</td>\n",
       "      <td>-19206.721570</td>\n",
       "      <td>632.201442</td>\n",
       "      <td>-0.587785</td>\n",
       "      <td>0.809017</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.129983</td>\n",
       "      <td>-0.139242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-07-28</th>\n",
       "      <td>30.0000</td>\n",
       "      <td>12444.2</td>\n",
       "      <td>-14.438503</td>\n",
       "      <td>30.891204</td>\n",
       "      <td>-21.659706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-40.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-108014.9</td>\n",
       "      <td>-3609.925738</td>\n",
       "      <td>-18907.300632</td>\n",
       "      <td>604.495403</td>\n",
       "      <td>-0.406737</td>\n",
       "      <td>0.913545</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.043825</td>\n",
       "      <td>-0.044814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-07-31</th>\n",
       "      <td>30.1250</td>\n",
       "      <td>9478.4</td>\n",
       "      <td>-13.928571</td>\n",
       "      <td>31.263266</td>\n",
       "      <td>-24.163073</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-40.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-98536.5</td>\n",
       "      <td>-505.870013</td>\n",
       "      <td>-16037.000541</td>\n",
       "      <td>607.014133</td>\n",
       "      <td>0.207912</td>\n",
       "      <td>0.978148</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.004158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-08-01</th>\n",
       "      <td>30.2500</td>\n",
       "      <td>8140.8</td>\n",
       "      <td>-28.823529</td>\n",
       "      <td>31.659494</td>\n",
       "      <td>-26.085806</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-40.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-90395.7</td>\n",
       "      <td>73.272586</td>\n",
       "      <td>-13600.629035</td>\n",
       "      <td>609.532864</td>\n",
       "      <td>0.207912</td>\n",
       "      <td>0.978148</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              close   volume        roc        rsi        tsi  bb_bbhi  \\\n",
       "date                                                                     \n",
       "2000-07-26  36.0625  15452.1   2.852050  40.134541 -15.064725      0.0   \n",
       "2000-07-27  31.3750  23576.7  -5.283019  32.699086 -18.375261      0.0   \n",
       "2000-07-28  30.0000  12444.2 -14.438503  30.891204 -21.659706      0.0   \n",
       "2000-07-31  30.1250   9478.4 -13.928571  31.263266 -24.163073      0.0   \n",
       "2000-08-01  30.2500   8140.8 -28.823529  31.659494 -26.085806      0.0   \n",
       "\n",
       "            bb_bbli  aroon_down  aroon  aroon_up  ...       obv          vpt  \\\n",
       "date                                              ...                          \n",
       "2000-07-26      0.0        56.0  -52.0       4.0  ...  -71994.0 -1447.166569   \n",
       "2000-07-27      1.0       100.0  -36.0      64.0  ...  -95570.7 -3706.260897   \n",
       "2000-07-28      1.0       100.0  -40.0      60.0  ... -108014.9 -3609.925738   \n",
       "2000-07-31      0.0        96.0  -40.0      56.0  ...  -98536.5  -505.870013   \n",
       "2000-08-01      0.0        92.0  -40.0      52.0  ...  -90395.7    73.272586   \n",
       "\n",
       "                      fi         nvi   day_sin   day_cos  month_sin  \\\n",
       "date                                                                  \n",
       "2000-07-26  -3988.544957  632.201442 -0.743145  0.669131  -0.500000   \n",
       "2000-07-27 -19206.721570  632.201442 -0.587785  0.809017  -0.500000   \n",
       "2000-07-28 -18907.300632  604.495403 -0.406737  0.913545  -0.500000   \n",
       "2000-07-31 -16037.000541  607.014133  0.207912  0.978148  -0.500000   \n",
       "2000-08-01 -13600.629035  609.532864  0.207912  0.978148  -0.866025   \n",
       "\n",
       "            month_cos  pct_change   log_ret  \n",
       "date                                         \n",
       "2000-07-26  -0.866025   -0.041528 -0.042415  \n",
       "2000-07-27  -0.866025   -0.129983 -0.139242  \n",
       "2000-07-28  -0.866025   -0.043825 -0.044814  \n",
       "2000-07-31  -0.866025    0.004167  0.004158  \n",
       "2000-08-01  -0.500000    0.004149  0.004141  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>roc</th>\n",
       "      <th>rsi</th>\n",
       "      <th>tsi</th>\n",
       "      <th>bb_bbhi</th>\n",
       "      <th>bb_bbli</th>\n",
       "      <th>aroon_down</th>\n",
       "      <th>aroon</th>\n",
       "      <th>aroon_up</th>\n",
       "      <th>...</th>\n",
       "      <th>obv</th>\n",
       "      <th>vpt</th>\n",
       "      <th>fi</th>\n",
       "      <th>nvi</th>\n",
       "      <th>day_sin</th>\n",
       "      <th>day_cos</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>pct_change</th>\n",
       "      <th>log_ret</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-03-30</th>\n",
       "      <td>876.3401</td>\n",
       "      <td>2762.7</td>\n",
       "      <td>2.792875</td>\n",
       "      <td>70.885407</td>\n",
       "      <td>21.777264</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1157730.7</td>\n",
       "      <td>102.387879</td>\n",
       "      <td>11984.628505</td>\n",
       "      <td>207.744064</td>\n",
       "      <td>-1.133108e-15</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.002310</td>\n",
       "      <td>0.002308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-31</th>\n",
       "      <td>886.5400</td>\n",
       "      <td>3957.6</td>\n",
       "      <td>3.935660</td>\n",
       "      <td>74.950899</td>\n",
       "      <td>26.926603</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1161688.3</td>\n",
       "      <td>52.446158</td>\n",
       "      <td>16039.270753</td>\n",
       "      <td>207.744064</td>\n",
       "      <td>2.079117e-01</td>\n",
       "      <td>0.978148</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.011639</td>\n",
       "      <td>0.011572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-04-03</th>\n",
       "      <td>891.5100</td>\n",
       "      <td>3422.3</td>\n",
       "      <td>4.463231</td>\n",
       "      <td>76.661030</td>\n",
       "      <td>31.645567</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1165110.6</td>\n",
       "      <td>65.248941</td>\n",
       "      <td>16177.779360</td>\n",
       "      <td>208.908691</td>\n",
       "      <td>5.877853e-01</td>\n",
       "      <td>0.809017</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "      <td>0.005606</td>\n",
       "      <td>0.005590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-04-04</th>\n",
       "      <td>906.8301</td>\n",
       "      <td>4984.7</td>\n",
       "      <td>6.396733</td>\n",
       "      <td>80.973174</td>\n",
       "      <td>37.548244</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1170095.3</td>\n",
       "      <td>104.844910</td>\n",
       "      <td>24776.111233</td>\n",
       "      <td>208.908691</td>\n",
       "      <td>7.431448e-01</td>\n",
       "      <td>0.669131</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "      <td>0.017184</td>\n",
       "      <td>0.017038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-04-05</th>\n",
       "      <td>909.2800</td>\n",
       "      <td>7508.4</td>\n",
       "      <td>6.104064</td>\n",
       "      <td>81.559915</td>\n",
       "      <td>42.219902</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1177603.7</td>\n",
       "      <td>105.944036</td>\n",
       "      <td>23864.499508</td>\n",
       "      <td>208.908691</td>\n",
       "      <td>8.660254e-01</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "      <td>0.002702</td>\n",
       "      <td>0.002698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               close  volume       roc        rsi        tsi  bb_bbhi  \\\n",
       "date                                                                    \n",
       "2017-03-30  876.3401  2762.7  2.792875  70.885407  21.777264      1.0   \n",
       "2017-03-31  886.5400  3957.6  3.935660  74.950899  26.926603      1.0   \n",
       "2017-04-03  891.5100  3422.3  4.463231  76.661030  31.645567      1.0   \n",
       "2017-04-04  906.8301  4984.7  6.396733  80.973174  37.548244      1.0   \n",
       "2017-04-05  909.2800  7508.4  6.104064  81.559915  42.219902      1.0   \n",
       "\n",
       "            bb_bbli  aroon_down  aroon  aroon_up  ...        obv         vpt  \\\n",
       "date                                              ...                          \n",
       "2017-03-30      0.0        72.0   28.0     100.0  ...  1157730.7  102.387879   \n",
       "2017-03-31      0.0        68.0   32.0     100.0  ...  1161688.3   52.446158   \n",
       "2017-04-03      0.0        64.0   36.0     100.0  ...  1165110.6   65.248941   \n",
       "2017-04-04      0.0        60.0   40.0     100.0  ...  1170095.3  104.844910   \n",
       "2017-04-05      0.0        56.0   44.0     100.0  ...  1177603.7  105.944036   \n",
       "\n",
       "                      fi         nvi       day_sin   day_cos  month_sin  \\\n",
       "date                                                                      \n",
       "2017-03-30  11984.628505  207.744064 -1.133108e-15  1.000000   1.000000   \n",
       "2017-03-31  16039.270753  207.744064  2.079117e-01  0.978148   1.000000   \n",
       "2017-04-03  16177.779360  208.908691  5.877853e-01  0.809017   0.866025   \n",
       "2017-04-04  24776.111233  208.908691  7.431448e-01  0.669131   0.866025   \n",
       "2017-04-05  23864.499508  208.908691  8.660254e-01  0.500000   0.866025   \n",
       "\n",
       "               month_cos  pct_change   log_ret  \n",
       "date                                            \n",
       "2017-03-30  6.123234e-17    0.002310  0.002308  \n",
       "2017-03-31  6.123234e-17    0.011639  0.011572  \n",
       "2017-04-03 -5.000000e-01    0.005606  0.005590  \n",
       "2017-04-04 -5.000000e-01    0.017184  0.017038  \n",
       "2017-04-05 -5.000000e-01    0.002702  0.002698  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.034655</td>\n",
       "      <td>0.139876</td>\n",
       "      <td>0.337145</td>\n",
       "      <td>0.292718</td>\n",
       "      <td>0.271962</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.229167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066635</td>\n",
       "      <td>0.303308</td>\n",
       "      <td>0.344254</td>\n",
       "      <td>0.811413</td>\n",
       "      <td>0.126381</td>\n",
       "      <td>0.834565</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.066987</td>\n",
       "      <td>0.347977</td>\n",
       "      <td>0.416966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.029257</td>\n",
       "      <td>0.218436</td>\n",
       "      <td>0.277233</td>\n",
       "      <td>0.195474</td>\n",
       "      <td>0.245257</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048936</td>\n",
       "      <td>0.259751</td>\n",
       "      <td>0.297396</td>\n",
       "      <td>0.811413</td>\n",
       "      <td>0.204489</td>\n",
       "      <td>0.904508</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.066987</td>\n",
       "      <td>0.198655</td>\n",
       "      <td>0.250238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.027673</td>\n",
       "      <td>0.110791</td>\n",
       "      <td>0.209806</td>\n",
       "      <td>0.171830</td>\n",
       "      <td>0.218762</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039595</td>\n",
       "      <td>0.261609</td>\n",
       "      <td>0.298318</td>\n",
       "      <td>0.770092</td>\n",
       "      <td>0.295511</td>\n",
       "      <td>0.956773</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.066987</td>\n",
       "      <td>0.344100</td>\n",
       "      <td>0.412836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.027817</td>\n",
       "      <td>0.082113</td>\n",
       "      <td>0.213562</td>\n",
       "      <td>0.176696</td>\n",
       "      <td>0.198567</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046710</td>\n",
       "      <td>0.321457</td>\n",
       "      <td>0.307156</td>\n",
       "      <td>0.773849</td>\n",
       "      <td>0.604528</td>\n",
       "      <td>0.989074</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.066987</td>\n",
       "      <td>0.425115</td>\n",
       "      <td>0.497161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.027961</td>\n",
       "      <td>0.069179</td>\n",
       "      <td>0.103866</td>\n",
       "      <td>0.181878</td>\n",
       "      <td>0.183057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052821</td>\n",
       "      <td>0.332624</td>\n",
       "      <td>0.314658</td>\n",
       "      <td>0.777605</td>\n",
       "      <td>0.604528</td>\n",
       "      <td>0.989074</td>\n",
       "      <td>0.066987</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.425086</td>\n",
       "      <td>0.497132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4    5    6         7   \\\n",
       "0  0.034655  0.139876  0.337145  0.292718  0.271962  0.0  0.0  0.541667   \n",
       "1  0.029257  0.218436  0.277233  0.195474  0.245257  0.0  1.0  1.000000   \n",
       "2  0.027673  0.110791  0.209806  0.171830  0.218762  0.0  1.0  1.000000   \n",
       "3  0.027817  0.082113  0.213562  0.176696  0.198567  0.0  0.0  0.958333   \n",
       "4  0.027961  0.069179  0.103866  0.181878  0.183057  0.0  0.0  0.916667   \n",
       "\n",
       "         8         9   ...        26        27        28        29        30  \\\n",
       "0  0.229167  0.000000  ...  0.066635  0.303308  0.344254  0.811413  0.126381   \n",
       "1  0.312500  0.625000  ...  0.048936  0.259751  0.297396  0.811413  0.204489   \n",
       "2  0.291667  0.583333  ...  0.039595  0.261609  0.298318  0.770092  0.295511   \n",
       "3  0.291667  0.541667  ...  0.046710  0.321457  0.307156  0.773849  0.604528   \n",
       "4  0.291667  0.500000  ...  0.052821  0.332624  0.314658  0.777605  0.604528   \n",
       "\n",
       "         31        32        33        34        35  \n",
       "0  0.834565  0.250000  0.066987  0.347977  0.416966  \n",
       "1  0.904508  0.250000  0.066987  0.198655  0.250238  \n",
       "2  0.956773  0.250000  0.066987  0.344100  0.412836  \n",
       "3  0.989074  0.250000  0.066987  0.425115  0.497161  \n",
       "4  0.989074  0.066987  0.250000  0.425086  0.497132  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Scaling\n",
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "training_set_scaled = sc.fit_transform(train)\n",
    "test_set_scaled = sc.fit_transform(test)\n",
    "pd.DataFrame(training_set_scaled).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 36)]              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 111       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 36)                144       \n",
      "=================================================================\n",
      "Total params: 255\n",
      "Trainable params: 255\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/250\n",
      "5/5 [==============================] - 1s 92ms/step - loss: 0.2226 - val_loss: 0.2752\n",
      "Epoch 2/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.2144 - val_loss: 0.2701\n",
      "Epoch 3/250\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2060 - val_loss: 0.2635\n",
      "Epoch 4/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.1984 - val_loss: 0.2554\n",
      "Epoch 5/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.1922 - val_loss: 0.2466\n",
      "Epoch 6/250\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.1851 - val_loss: 0.2376\n",
      "Epoch 7/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.1780 - val_loss: 0.2287\n",
      "Epoch 8/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.1708 - val_loss: 0.2202\n",
      "Epoch 9/250\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.1644 - val_loss: 0.2120\n",
      "Epoch 10/250\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.1571 - val_loss: 0.2041\n",
      "Epoch 11/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.1516 - val_loss: 0.1963\n",
      "Epoch 12/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.1456 - val_loss: 0.1887\n",
      "Epoch 13/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.1394 - val_loss: 0.1814\n",
      "Epoch 14/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.1331 - val_loss: 0.1743\n",
      "Epoch 15/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.1279 - val_loss: 0.1676\n",
      "Epoch 16/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.1225 - val_loss: 0.1611\n",
      "Epoch 17/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.1172 - val_loss: 0.1550\n",
      "Epoch 18/250\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.1123 - val_loss: 0.1491\n",
      "Epoch 19/250\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.1076 - val_loss: 0.1434\n",
      "Epoch 20/250\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.1029 - val_loss: 0.1380\n",
      "Epoch 21/250\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0984 - val_loss: 0.1330\n",
      "Epoch 22/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0946 - val_loss: 0.1282\n",
      "Epoch 23/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0908 - val_loss: 0.1237\n",
      "Epoch 24/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0869 - val_loss: 0.1194\n",
      "Epoch 25/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0836 - val_loss: 0.1154\n",
      "Epoch 26/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0799 - val_loss: 0.1116\n",
      "Epoch 27/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0771 - val_loss: 0.1082\n",
      "Epoch 28/250\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0744 - val_loss: 0.1049\n",
      "Epoch 29/250\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.0718 - val_loss: 0.1019\n",
      "Epoch 30/250\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0691 - val_loss: 0.0991\n",
      "Epoch 31/250\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0669 - val_loss: 0.0965\n",
      "Epoch 32/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0646 - val_loss: 0.0941\n",
      "Epoch 33/250\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0625 - val_loss: 0.0919\n",
      "Epoch 34/250\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0605 - val_loss: 0.0898\n",
      "Epoch 35/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0589 - val_loss: 0.0878\n",
      "Epoch 36/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0571 - val_loss: 0.0861\n",
      "Epoch 37/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0558 - val_loss: 0.0845\n",
      "Epoch 38/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0544 - val_loss: 0.0830\n",
      "Epoch 39/250\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0534 - val_loss: 0.0817\n",
      "Epoch 40/250\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0522 - val_loss: 0.0804\n",
      "Epoch 41/250\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0508 - val_loss: 0.0793\n",
      "Epoch 42/250\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0500 - val_loss: 0.0782\n",
      "Epoch 43/250\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0499 - val_loss: 0.0772\n",
      "Epoch 44/250\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0485 - val_loss: 0.0763\n",
      "Epoch 45/250\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0478 - val_loss: 0.0755\n",
      "Epoch 46/250\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0470 - val_loss: 0.0747\n",
      "Epoch 47/250\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0464 - val_loss: 0.0740\n",
      "Epoch 48/250\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0458 - val_loss: 0.0733\n",
      "Epoch 49/250\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0454 - val_loss: 0.0728\n",
      "Epoch 50/250\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0447 - val_loss: 0.0723\n",
      "Epoch 51/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0439 - val_loss: 0.0717\n",
      "Epoch 52/250\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0440 - val_loss: 0.0712\n",
      "Epoch 53/250\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0435 - val_loss: 0.0706\n",
      "Epoch 54/250\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0430 - val_loss: 0.0701\n",
      "Epoch 55/250\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0424 - val_loss: 0.0696\n",
      "Epoch 56/250\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.0426 - val_loss: 0.0691\n",
      "Epoch 57/250\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0419 - val_loss: 0.0687\n",
      "Epoch 58/250\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0420 - val_loss: 0.0683\n",
      "Epoch 59/250\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0415 - val_loss: 0.0679\n",
      "Epoch 60/250\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0412 - val_loss: 0.0675\n",
      "Epoch 61/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0407 - val_loss: 0.0672\n",
      "Epoch 62/250\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0408 - val_loss: 0.0669\n",
      "Epoch 63/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0404 - val_loss: 0.0666\n",
      "Epoch 64/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0405 - val_loss: 0.0663\n",
      "Epoch 65/250\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0400 - val_loss: 0.0660\n",
      "Epoch 66/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0396 - val_loss: 0.0656\n",
      "Epoch 67/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0397 - val_loss: 0.0653\n",
      "Epoch 68/250\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0395 - val_loss: 0.0649\n",
      "Epoch 69/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0392 - val_loss: 0.0647\n",
      "Epoch 70/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0390 - val_loss: 0.0644\n",
      "Epoch 71/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0387 - val_loss: 0.0642\n",
      "Epoch 72/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0386 - val_loss: 0.0640\n",
      "Epoch 73/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0385 - val_loss: 0.0638\n",
      "Epoch 74/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0382 - val_loss: 0.0636\n",
      "Epoch 75/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0383 - val_loss: 0.0633\n",
      "Epoch 76/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0381 - val_loss: 0.0630\n",
      "Epoch 77/250\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0382 - val_loss: 0.0626\n",
      "Epoch 78/250\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0379 - val_loss: 0.0624\n",
      "Epoch 79/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0377 - val_loss: 0.0622\n",
      "Epoch 80/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0372 - val_loss: 0.0620\n",
      "Epoch 81/250\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0373 - val_loss: 0.0618\n",
      "Epoch 82/250\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0371 - val_loss: 0.0616\n",
      "Epoch 83/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0373 - val_loss: 0.0613\n",
      "Epoch 84/250\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0369 - val_loss: 0.0610\n",
      "Epoch 85/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0367 - val_loss: 0.0608\n",
      "Epoch 86/250\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0366 - val_loss: 0.0606\n",
      "Epoch 87/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0365 - val_loss: 0.0603\n",
      "Epoch 88/250\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0362 - val_loss: 0.0601\n",
      "Epoch 89/250\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0363 - val_loss: 0.0598\n",
      "Epoch 90/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0360 - val_loss: 0.0596\n",
      "Epoch 91/250\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0358 - val_loss: 0.0593\n",
      "Epoch 92/250\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0356 - val_loss: 0.0591\n",
      "Epoch 93/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0356 - val_loss: 0.0589\n",
      "Epoch 94/250\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0356 - val_loss: 0.0587\n",
      "Epoch 95/250\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0357 - val_loss: 0.0585\n",
      "Epoch 96/250\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0356 - val_loss: 0.0584\n",
      "Epoch 97/250\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0353 - val_loss: 0.0583\n",
      "Epoch 98/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0351 - val_loss: 0.0581\n",
      "Epoch 99/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0351 - val_loss: 0.0579\n",
      "Epoch 100/250\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0347 - val_loss: 0.0577\n",
      "Epoch 101/250\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.0348 - val_loss: 0.0575\n",
      "Epoch 102/250\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0347 - val_loss: 0.0574\n",
      "Epoch 103/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0344 - val_loss: 0.0572\n",
      "Epoch 104/250\n",
      "5/5 [==============================] - 0s 52ms/step - loss: 0.0346 - val_loss: 0.0570\n",
      "Epoch 105/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0343 - val_loss: 0.0569\n",
      "Epoch 106/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0342 - val_loss: 0.0567\n",
      "Epoch 107/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0344 - val_loss: 0.0566\n",
      "Epoch 108/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0340 - val_loss: 0.0565\n",
      "Epoch 109/250\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0340 - val_loss: 0.0563\n",
      "Epoch 110/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0338 - val_loss: 0.0562\n",
      "Epoch 111/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0338 - val_loss: 0.0560\n",
      "Epoch 112/250\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0336 - val_loss: 0.0559\n",
      "Epoch 113/250\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0337 - val_loss: 0.0558\n",
      "Epoch 114/250\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0335 - val_loss: 0.0556\n",
      "Epoch 115/250\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0333 - val_loss: 0.0555\n",
      "Epoch 116/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0335 - val_loss: 0.0553\n",
      "Epoch 117/250\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0333 - val_loss: 0.0552\n",
      "Epoch 118/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0332 - val_loss: 0.0551\n",
      "Epoch 119/250\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0331 - val_loss: 0.0550\n",
      "Epoch 120/250\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0330 - val_loss: 0.0549\n",
      "Epoch 121/250\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0331 - val_loss: 0.0547\n",
      "Epoch 122/250\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0326 - val_loss: 0.0546\n",
      "Epoch 123/250\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0327 - val_loss: 0.0545\n",
      "Epoch 124/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0325 - val_loss: 0.0543\n",
      "Epoch 125/250\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0325 - val_loss: 0.0541\n",
      "Epoch 126/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0324 - val_loss: 0.0540\n",
      "Epoch 127/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0323 - val_loss: 0.0539\n",
      "Epoch 128/250\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0322 - val_loss: 0.0537\n",
      "Epoch 129/250\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0323 - val_loss: 0.0536\n",
      "Epoch 130/250\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0322 - val_loss: 0.0534\n",
      "Epoch 131/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0320 - val_loss: 0.0533\n",
      "Epoch 132/250\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0319 - val_loss: 0.0532\n",
      "Epoch 133/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0322 - val_loss: 0.0531\n",
      "Epoch 134/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0319 - val_loss: 0.0529\n",
      "Epoch 135/250\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0317 - val_loss: 0.0527\n",
      "Epoch 136/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0319 - val_loss: 0.0525\n",
      "Epoch 137/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0314 - val_loss: 0.0524\n",
      "Epoch 138/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0317 - val_loss: 0.0523\n",
      "Epoch 139/250\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0314 - val_loss: 0.0522\n",
      "Epoch 140/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0316 - val_loss: 0.0521\n",
      "Epoch 141/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0315 - val_loss: 0.0519\n",
      "Epoch 142/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0314 - val_loss: 0.0518\n",
      "Epoch 143/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0311 - val_loss: 0.0516\n",
      "Epoch 144/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0311 - val_loss: 0.0515\n",
      "Epoch 145/250\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0311 - val_loss: 0.0514\n",
      "Epoch 146/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0309 - val_loss: 0.0513\n",
      "Epoch 147/250\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0308 - val_loss: 0.0512\n",
      "Epoch 148/250\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0308 - val_loss: 0.0511\n",
      "Epoch 149/250\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0306 - val_loss: 0.0510\n",
      "Epoch 150/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0308 - val_loss: 0.0508\n",
      "Epoch 151/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0307 - val_loss: 0.0506\n",
      "Epoch 152/250\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0306 - val_loss: 0.0505\n",
      "Epoch 153/250\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0305 - val_loss: 0.0503\n",
      "Epoch 154/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0304 - val_loss: 0.0502\n",
      "Epoch 155/250\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0305 - val_loss: 0.0501\n",
      "Epoch 156/250\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0302 - val_loss: 0.0501\n",
      "Epoch 157/250\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0299 - val_loss: 0.0500\n",
      "Epoch 158/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0302 - val_loss: 0.0498\n",
      "Epoch 159/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0300 - val_loss: 0.0497\n",
      "Epoch 160/250\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0297 - val_loss: 0.0495\n",
      "Epoch 161/250\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0298 - val_loss: 0.0494\n",
      "Epoch 162/250\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0297 - val_loss: 0.0493\n",
      "Epoch 163/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0293 - val_loss: 0.0492\n",
      "Epoch 164/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0295 - val_loss: 0.0490\n",
      "Epoch 165/250\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0295 - val_loss: 0.0489\n",
      "Epoch 166/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0295 - val_loss: 0.0488\n",
      "Epoch 167/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0293 - val_loss: 0.0486\n",
      "Epoch 168/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0293 - val_loss: 0.0485\n",
      "Epoch 169/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0291 - val_loss: 0.0484\n",
      "Epoch 170/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0290 - val_loss: 0.0483\n",
      "Epoch 171/250\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0288 - val_loss: 0.0481\n",
      "Epoch 172/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0291 - val_loss: 0.0480\n",
      "Epoch 173/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0289 - val_loss: 0.0478\n",
      "Epoch 174/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0289 - val_loss: 0.0477\n",
      "Epoch 175/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0287 - val_loss: 0.0476\n",
      "Epoch 176/250\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0285 - val_loss: 0.0475\n",
      "Epoch 177/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0283 - val_loss: 0.0474\n",
      "Epoch 178/250\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0285 - val_loss: 0.0473\n",
      "Epoch 179/250\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0283 - val_loss: 0.0472\n",
      "Epoch 180/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0282 - val_loss: 0.0470\n",
      "Epoch 181/250\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0283 - val_loss: 0.0468\n",
      "Epoch 182/250\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0280 - val_loss: 0.0467\n",
      "Epoch 183/250\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0280 - val_loss: 0.0466\n",
      "Epoch 184/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0278 - val_loss: 0.0466\n",
      "Epoch 185/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0278 - val_loss: 0.0465\n",
      "Epoch 186/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0278 - val_loss: 0.0464\n",
      "Epoch 187/250\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0277 - val_loss: 0.0462\n",
      "Epoch 188/250\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0275 - val_loss: 0.0461\n",
      "Epoch 189/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0273 - val_loss: 0.0459\n",
      "Epoch 190/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0275 - val_loss: 0.0458\n",
      "Epoch 191/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0272 - val_loss: 0.0457\n",
      "Epoch 192/250\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0272 - val_loss: 0.0456\n",
      "Epoch 193/250\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0271 - val_loss: 0.0454\n",
      "Epoch 194/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0270 - val_loss: 0.0453\n",
      "Epoch 195/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0271 - val_loss: 0.0451\n",
      "Epoch 196/250\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0268 - val_loss: 0.0451\n",
      "Epoch 197/250\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0268 - val_loss: 0.0450\n",
      "Epoch 198/250\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0267 - val_loss: 0.0448\n",
      "Epoch 199/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0265 - val_loss: 0.0447\n",
      "Epoch 200/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0265 - val_loss: 0.0446\n",
      "Epoch 201/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0263 - val_loss: 0.0445\n",
      "Epoch 202/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0264 - val_loss: 0.0444\n",
      "Epoch 203/250\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0263 - val_loss: 0.0442\n",
      "Epoch 204/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0262 - val_loss: 0.0441\n",
      "Epoch 205/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0260 - val_loss: 0.0441\n",
      "Epoch 206/250\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0261 - val_loss: 0.0440\n",
      "Epoch 207/250\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0258 - val_loss: 0.0439\n",
      "Epoch 208/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0259 - val_loss: 0.0437\n",
      "Epoch 209/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0257 - val_loss: 0.0436\n",
      "Epoch 210/250\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0258 - val_loss: 0.0435\n",
      "Epoch 211/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0256 - val_loss: 0.0435\n",
      "Epoch 212/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0257 - val_loss: 0.0433\n",
      "Epoch 213/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0253 - val_loss: 0.0432\n",
      "Epoch 214/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0253 - val_loss: 0.0431\n",
      "Epoch 215/250\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0253 - val_loss: 0.0430\n",
      "Epoch 216/250\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0253 - val_loss: 0.0430\n",
      "Epoch 217/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0250 - val_loss: 0.0429\n",
      "Epoch 218/250\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0250 - val_loss: 0.0428\n",
      "Epoch 219/250\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0251 - val_loss: 0.0426\n",
      "Epoch 220/250\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0248 - val_loss: 0.0425\n",
      "Epoch 221/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0248 - val_loss: 0.0424\n",
      "Epoch 222/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0248 - val_loss: 0.0423\n",
      "Epoch 223/250\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0246 - val_loss: 0.0423\n",
      "Epoch 224/250\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0246 - val_loss: 0.0422\n",
      "Epoch 225/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0245 - val_loss: 0.0421\n",
      "Epoch 226/250\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0244 - val_loss: 0.0421\n",
      "Epoch 227/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0244 - val_loss: 0.0420\n",
      "Epoch 228/250\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0244 - val_loss: 0.0418\n",
      "Epoch 229/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0243 - val_loss: 0.0417\n",
      "Epoch 230/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0242 - val_loss: 0.0416\n",
      "Epoch 231/250\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0241 - val_loss: 0.0416\n",
      "Epoch 232/250\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0240 - val_loss: 0.0415\n",
      "Epoch 233/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0240 - val_loss: 0.0414\n",
      "Epoch 234/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0239 - val_loss: 0.0413\n",
      "Epoch 235/250\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0238 - val_loss: 0.0411\n",
      "Epoch 236/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0238 - val_loss: 0.0411\n",
      "Epoch 237/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0237 - val_loss: 0.0411\n",
      "Epoch 238/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0237 - val_loss: 0.0410\n",
      "Epoch 239/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0238 - val_loss: 0.0409\n",
      "Epoch 240/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0237 - val_loss: 0.0408\n",
      "Epoch 241/250\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0237 - val_loss: 0.0408\n",
      "Epoch 242/250\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0235 - val_loss: 0.0407\n",
      "Epoch 243/250\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0235 - val_loss: 0.0406\n",
      "Epoch 244/250\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0234 - val_loss: 0.0406\n",
      "Epoch 245/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0234 - val_loss: 0.0405\n",
      "Epoch 246/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0232 - val_loss: 0.0404\n",
      "Epoch 247/250\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0233 - val_loss: 0.0403\n",
      "Epoch 248/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0232 - val_loss: 0.0402\n",
      "Epoch 249/250\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0230 - val_loss: 0.0402\n",
      "Epoch 250/250\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0231 - val_loss: 0.0401\n"
     ]
    }
   ],
   "source": [
    "# calculated log returns (i.e. the log of the difference between the price x+1 and price x)\n",
    "# windows of train.shape[1] consecutive returns will be produced. \n",
    "# Can be normalized with a MinMaxScaler to the range [0,1]??\n",
    "\n",
    "window_length = training_set_scaled.shape[1]\n",
    "encoding_dim = 3\n",
    "epochs = 250\n",
    "\n",
    "# compress the input to a 3-dimensional latent space.\n",
    "# the input and output of the autoencoder for 10 randomly selected price return windows extracted \n",
    "# from the test dataset  \n",
    "\n",
    "# input placeholder\n",
    "input_window = Input(shape=(window_length,))\n",
    "# encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation='tanh')(input_window) #tanh, linear, leakyrelu\n",
    "# lossy reconstruction of the input\n",
    "decoded = Dense(window_length, activation='linear')(encoded) #linear\n",
    "\n",
    "# model mapping an input to its reconstruction\n",
    "autoencoder = Model(input_window, decoded)\n",
    "\n",
    "# model mapping an input to its encoded representation\n",
    "encoder = Model(input_window, encoded)\n",
    "\n",
    "\n",
    "autoencoder.summary()\n",
    "autoencoder.compile(optimizer='adam', loss='MeanSquaredError') #MSE\n",
    "history = autoencoder.fit(training_set_scaled, training_set_scaled,\n",
    "                epochs=epochs,\n",
    "                batch_size=1024,\n",
    "                shuffle=True,\n",
    "                validation_data=(test_set_scaled, test_set_scaled))\n",
    "\n",
    "decoded_stocks = autoencoder.predict(test_set_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.872192</td>\n",
       "      <td>0.037765</td>\n",
       "      <td>0.387268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.880928</td>\n",
       "      <td>0.034662</td>\n",
       "      <td>0.430423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.864796</td>\n",
       "      <td>0.143963</td>\n",
       "      <td>0.541257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.873017</td>\n",
       "      <td>0.143176</td>\n",
       "      <td>0.581687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.880993</td>\n",
       "      <td>0.139182</td>\n",
       "      <td>0.628603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>-0.310941</td>\n",
       "      <td>-0.898387</td>\n",
       "      <td>0.544246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>-0.331833</td>\n",
       "      <td>-0.909503</td>\n",
       "      <td>0.545859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>-0.286793</td>\n",
       "      <td>-0.904709</td>\n",
       "      <td>0.556094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>-0.307053</td>\n",
       "      <td>-0.910680</td>\n",
       "      <td>0.547894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>-0.294723</td>\n",
       "      <td>-0.910921</td>\n",
       "      <td>0.541966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1049 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2\n",
       "0     0.872192  0.037765  0.387268\n",
       "1     0.880928  0.034662  0.430423\n",
       "2     0.864796  0.143963  0.541257\n",
       "3     0.873017  0.143176  0.581687\n",
       "4     0.880993  0.139182  0.628603\n",
       "...        ...       ...       ...\n",
       "1044 -0.310941 -0.898387  0.544246\n",
       "1045 -0.331833 -0.909503  0.545859\n",
       "1046 -0.286793 -0.904709  0.556094\n",
       "1047 -0.307053 -0.910680  0.547894\n",
       "1048 -0.294723 -0.910921  0.541966\n",
       "\n",
       "[1049 rows x 3 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(encoder.predict(test_set_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.019723</td>\n",
       "      <td>0.062245</td>\n",
       "      <td>0.355416</td>\n",
       "      <td>0.577719</td>\n",
       "      <td>0.510872</td>\n",
       "      <td>0.187330</td>\n",
       "      <td>-0.002751</td>\n",
       "      <td>0.346451</td>\n",
       "      <td>0.777327</td>\n",
       "      <td>0.749852</td>\n",
       "      <td>...</td>\n",
       "      <td>0.531600</td>\n",
       "      <td>0.353185</td>\n",
       "      <td>0.352835</td>\n",
       "      <td>0.252397</td>\n",
       "      <td>0.629290</td>\n",
       "      <td>0.420092</td>\n",
       "      <td>0.394640</td>\n",
       "      <td>0.367849</td>\n",
       "      <td>0.464661</td>\n",
       "      <td>0.496065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.002634</td>\n",
       "      <td>0.059652</td>\n",
       "      <td>0.359804</td>\n",
       "      <td>0.596814</td>\n",
       "      <td>0.528756</td>\n",
       "      <td>0.196861</td>\n",
       "      <td>-0.007530</td>\n",
       "      <td>0.322320</td>\n",
       "      <td>0.812321</td>\n",
       "      <td>0.783954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.554616</td>\n",
       "      <td>0.355481</td>\n",
       "      <td>0.356963</td>\n",
       "      <td>0.245468</td>\n",
       "      <td>0.637120</td>\n",
       "      <td>0.421565</td>\n",
       "      <td>0.391729</td>\n",
       "      <td>0.362671</td>\n",
       "      <td>0.469070</td>\n",
       "      <td>0.501152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.037310</td>\n",
       "      <td>0.064101</td>\n",
       "      <td>0.373667</td>\n",
       "      <td>0.629782</td>\n",
       "      <td>0.562139</td>\n",
       "      <td>0.239210</td>\n",
       "      <td>-0.038004</td>\n",
       "      <td>0.223719</td>\n",
       "      <td>0.896885</td>\n",
       "      <td>0.857613</td>\n",
       "      <td>...</td>\n",
       "      <td>0.551831</td>\n",
       "      <td>0.359114</td>\n",
       "      <td>0.354398</td>\n",
       "      <td>0.247039</td>\n",
       "      <td>0.644032</td>\n",
       "      <td>0.389149</td>\n",
       "      <td>0.326096</td>\n",
       "      <td>0.259433</td>\n",
       "      <td>0.464179</td>\n",
       "      <td>0.486476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.022774</td>\n",
       "      <td>0.061810</td>\n",
       "      <td>0.377845</td>\n",
       "      <td>0.647700</td>\n",
       "      <td>0.578830</td>\n",
       "      <td>0.248529</td>\n",
       "      <td>-0.042759</td>\n",
       "      <td>0.200699</td>\n",
       "      <td>0.929954</td>\n",
       "      <td>0.889681</td>\n",
       "      <td>...</td>\n",
       "      <td>0.572524</td>\n",
       "      <td>0.361290</td>\n",
       "      <td>0.358094</td>\n",
       "      <td>0.240946</td>\n",
       "      <td>0.651514</td>\n",
       "      <td>0.389965</td>\n",
       "      <td>0.322568</td>\n",
       "      <td>0.253347</td>\n",
       "      <td>0.468251</td>\n",
       "      <td>0.490967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.003657</td>\n",
       "      <td>0.059081</td>\n",
       "      <td>0.382553</td>\n",
       "      <td>0.667795</td>\n",
       "      <td>0.597958</td>\n",
       "      <td>0.258639</td>\n",
       "      <td>-0.047978</td>\n",
       "      <td>0.174076</td>\n",
       "      <td>0.967130</td>\n",
       "      <td>0.925954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.597287</td>\n",
       "      <td>0.363642</td>\n",
       "      <td>0.362496</td>\n",
       "      <td>0.233196</td>\n",
       "      <td>0.659190</td>\n",
       "      <td>0.391544</td>\n",
       "      <td>0.319121</td>\n",
       "      <td>0.247276</td>\n",
       "      <td>0.472579</td>\n",
       "      <td>0.496111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>0.770992</td>\n",
       "      <td>0.095989</td>\n",
       "      <td>0.308178</td>\n",
       "      <td>0.113992</td>\n",
       "      <td>0.339072</td>\n",
       "      <td>-0.056956</td>\n",
       "      <td>0.018440</td>\n",
       "      <td>0.033456</td>\n",
       "      <td>0.159240</td>\n",
       "      <td>0.213841</td>\n",
       "      <td>...</td>\n",
       "      <td>0.587775</td>\n",
       "      <td>0.241865</td>\n",
       "      <td>0.334806</td>\n",
       "      <td>-0.025602</td>\n",
       "      <td>-0.035393</td>\n",
       "      <td>0.522704</td>\n",
       "      <td>0.323340</td>\n",
       "      <td>0.249970</td>\n",
       "      <td>0.123870</td>\n",
       "      <td>0.264650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>0.780782</td>\n",
       "      <td>0.096988</td>\n",
       "      <td>0.307400</td>\n",
       "      <td>0.105424</td>\n",
       "      <td>0.335428</td>\n",
       "      <td>-0.060532</td>\n",
       "      <td>0.018253</td>\n",
       "      <td>0.027585</td>\n",
       "      <td>0.148176</td>\n",
       "      <td>0.203835</td>\n",
       "      <td>...</td>\n",
       "      <td>0.586011</td>\n",
       "      <td>0.239914</td>\n",
       "      <td>0.333958</td>\n",
       "      <td>-0.029313</td>\n",
       "      <td>-0.046897</td>\n",
       "      <td>0.523077</td>\n",
       "      <td>0.320195</td>\n",
       "      <td>0.244998</td>\n",
       "      <td>0.117631</td>\n",
       "      <td>0.259779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>0.777918</td>\n",
       "      <td>0.093042</td>\n",
       "      <td>0.309823</td>\n",
       "      <td>0.128652</td>\n",
       "      <td>0.348978</td>\n",
       "      <td>-0.053360</td>\n",
       "      <td>0.019355</td>\n",
       "      <td>0.035291</td>\n",
       "      <td>0.179434</td>\n",
       "      <td>0.233964</td>\n",
       "      <td>...</td>\n",
       "      <td>0.603755</td>\n",
       "      <td>0.244407</td>\n",
       "      <td>0.338352</td>\n",
       "      <td>-0.027094</td>\n",
       "      <td>-0.022157</td>\n",
       "      <td>0.527269</td>\n",
       "      <td>0.332075</td>\n",
       "      <td>0.263361</td>\n",
       "      <td>0.132202</td>\n",
       "      <td>0.273496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>0.780555</td>\n",
       "      <td>0.094700</td>\n",
       "      <td>0.308261</td>\n",
       "      <td>0.116905</td>\n",
       "      <td>0.341693</td>\n",
       "      <td>-0.058057</td>\n",
       "      <td>0.019847</td>\n",
       "      <td>0.034900</td>\n",
       "      <td>0.162325</td>\n",
       "      <td>0.217744</td>\n",
       "      <td>...</td>\n",
       "      <td>0.595797</td>\n",
       "      <td>0.242218</td>\n",
       "      <td>0.336433</td>\n",
       "      <td>-0.028228</td>\n",
       "      <td>-0.033801</td>\n",
       "      <td>0.526442</td>\n",
       "      <td>0.328784</td>\n",
       "      <td>0.258330</td>\n",
       "      <td>0.125649</td>\n",
       "      <td>0.267677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>0.777700</td>\n",
       "      <td>0.093847</td>\n",
       "      <td>0.308018</td>\n",
       "      <td>0.120157</td>\n",
       "      <td>0.342251</td>\n",
       "      <td>-0.058258</td>\n",
       "      <td>0.021527</td>\n",
       "      <td>0.042928</td>\n",
       "      <td>0.164446</td>\n",
       "      <td>0.219884</td>\n",
       "      <td>...</td>\n",
       "      <td>0.597507</td>\n",
       "      <td>0.243120</td>\n",
       "      <td>0.337141</td>\n",
       "      <td>-0.026510</td>\n",
       "      <td>-0.027816</td>\n",
       "      <td>0.528095</td>\n",
       "      <td>0.334066</td>\n",
       "      <td>0.266636</td>\n",
       "      <td>0.129382</td>\n",
       "      <td>0.271243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1049 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0    -0.019723  0.062245  0.355416  0.577719  0.510872  0.187330 -0.002751   \n",
       "1    -0.002634  0.059652  0.359804  0.596814  0.528756  0.196861 -0.007530   \n",
       "2    -0.037310  0.064101  0.373667  0.629782  0.562139  0.239210 -0.038004   \n",
       "3    -0.022774  0.061810  0.377845  0.647700  0.578830  0.248529 -0.042759   \n",
       "4    -0.003657  0.059081  0.382553  0.667795  0.597958  0.258639 -0.047978   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1044  0.770992  0.095989  0.308178  0.113992  0.339072 -0.056956  0.018440   \n",
       "1045  0.780782  0.096988  0.307400  0.105424  0.335428 -0.060532  0.018253   \n",
       "1046  0.777918  0.093042  0.309823  0.128652  0.348978 -0.053360  0.019355   \n",
       "1047  0.780555  0.094700  0.308261  0.116905  0.341693 -0.058057  0.019847   \n",
       "1048  0.777700  0.093847  0.308018  0.120157  0.342251 -0.058258  0.021527   \n",
       "\n",
       "            7         8         9   ...        26        27        28  \\\n",
       "0     0.346451  0.777327  0.749852  ...  0.531600  0.353185  0.352835   \n",
       "1     0.322320  0.812321  0.783954  ...  0.554616  0.355481  0.356963   \n",
       "2     0.223719  0.896885  0.857613  ...  0.551831  0.359114  0.354398   \n",
       "3     0.200699  0.929954  0.889681  ...  0.572524  0.361290  0.358094   \n",
       "4     0.174076  0.967130  0.925954  ...  0.597287  0.363642  0.362496   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1044  0.033456  0.159240  0.213841  ...  0.587775  0.241865  0.334806   \n",
       "1045  0.027585  0.148176  0.203835  ...  0.586011  0.239914  0.333958   \n",
       "1046  0.035291  0.179434  0.233964  ...  0.603755  0.244407  0.338352   \n",
       "1047  0.034900  0.162325  0.217744  ...  0.595797  0.242218  0.336433   \n",
       "1048  0.042928  0.164446  0.219884  ...  0.597507  0.243120  0.337141   \n",
       "\n",
       "            29        30        31        32        33        34        35  \n",
       "0     0.252397  0.629290  0.420092  0.394640  0.367849  0.464661  0.496065  \n",
       "1     0.245468  0.637120  0.421565  0.391729  0.362671  0.469070  0.501152  \n",
       "2     0.247039  0.644032  0.389149  0.326096  0.259433  0.464179  0.486476  \n",
       "3     0.240946  0.651514  0.389965  0.322568  0.253347  0.468251  0.490967  \n",
       "4     0.233196  0.659190  0.391544  0.319121  0.247276  0.472579  0.496111  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1044 -0.025602 -0.035393  0.522704  0.323340  0.249970  0.123870  0.264650  \n",
       "1045 -0.029313 -0.046897  0.523077  0.320195  0.244998  0.117631  0.259779  \n",
       "1046 -0.027094 -0.022157  0.527269  0.332075  0.263361  0.132202  0.273496  \n",
       "1047 -0.028228 -0.033801  0.526442  0.328784  0.258330  0.125649  0.267677  \n",
       "1048 -0.026510 -0.027816  0.528095  0.334066  0.266636  0.129382  0.271243  \n",
       "\n",
       "[1049 rows x 36 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(decoded_stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    ax = plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history[\"loss\"])\n",
    "    plt.title(\"Train loss\")\n",
    "    ax = plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history[\"val_loss\"])\n",
    "    plt.title(\"Test loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loss vs Epoch\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
